\documentclass{scrreprt}
% coma version of report class:
% http://tex.stackexchange.com/questions/5948/subtitle-doesnt-work-in-article-document-class

\usepackage{fullpage}
\usepackage[utf8]{inputenc} % åäö
\usepackage{hyperref}
\usepackage{enumitem}
\usepackage[toc]{glossaries}
\usepackage{xcolor}
\usepackage{listings}



\setkomafont{disposition}{\normalfont\bfseries}


% Remove date
\date{2014}

\hypersetup{
  colorlinks = true,
  linkcolor = black,
  citecolor = red
}

\title{ A language for defining markup languages }
\subtitle{ Using regular expressions to convert annotated documents \\into hierarchical documents }
\author{ Christopher OKHRAVI \\ UPPSALA UNIVERSITY }


%
% Defining research questions
%
\newcommand\researchquestionformat[1]{\begin{quote}#1\end{quote}}
\newcommand\firstresearchquestion{\researchquestionformat{%
  \textbf{(RQ1) Research question 1} \\
  Can the verbosity of document annotation formats (e.g. \texttt{XML}) be decreased, by allowing authors themselves to design annotation formats?%
}}


\newcommand\secondresearchquestion{\researchquestionformat{%
  \textbf{(RQ2) Research question 2} \\
  Can an annotation language
  For an author to be able to design an annotation language, this author must design a domain-specific-language (DSL), a parser, a compiler and a transformation engine. Consequently we come to the point where we instead ask ourselves the following question:


  Can a \texttt{Domain Specific Language (DSL)} be defined, such that a semi-technical author can employ it to design a document annotation format?

  Requiring (A) the combined complexity of the DSL and the annotated document is less than that of a document annotated in a traditional markup language with fixed syntax (e.g. \texttt{XML}).

  Without (B) sacrificing any flexibility of the original markup language.
}}

\newcommand\thirdresearchquestion{\researchquestionformat{%
  \textbf{(RQ3) Research question 3} \\
  Can a process be defined, such that any document can be converted into \texttt{XML} format, using \texttt{Regular Expressions} as rules for document hierarchy?
}}

\newcommand\fourthresearchquestion{\researchquestionformat{%
  \textbf{(RQ4) Research question 4} \\
  Can a language be defined, such that it is a subset of \texttt{Regular Expressions}, where most control characters are replaced by assumptions? Where the intent of the subset is to express annotations of document hierarchy.
}}


\newcommand{\tab}{\hspace*{6pt}}
\newcommand{\tabb}{\tab\tab}



\newenvironment{example}
{ \hrulefill \vspace{12pt} \\ }
{ \\\\ \vspace{12pt} \hrulefill }


\lstset{
  language=XML,
  basicstyle=\color[rgb]{0.3,0.3,0.3}\ttfamily,
  backgroundcolor=\color[rgb]{0.98,0.98,0.98},
  showstringspaces=false,
  breaklines,
  breakatwhitespace,
}



\newglossaryentry{document authoring}
{
  name={Document Authoring},
  description={The act of writing literature.}
}
\makeglossaries






\begin{document}

\maketitle
\tableofcontents
\pagebreak



\glsaddall
\printglossary





\chapter{Introduction}

- A brief history of writing from pens, to the printing press, to computers and word processors, to markup languages.
 


\section{Exploring annotation formats}

- What does it mean to annotate? What are common document annotation formats?

- Examples of markup languages and showcase of some syntax examples.

- A brief overview of markup languages and how they can be used for (1) Document authoring, and (2) Data transport.

- The relevance of markup for fiction and non-fiction.



\section{Markup is not just data transportation}
An explanation as to why object notation is a subset of annotations. Meaning that formats like \texttt{JSON} are too data-centric and can consequently not, in any sensible manner, be used for manual document authoring. \texttt{XML} for example can be used for data transport, but in this thesis we'll focus on its markup properties. Consequently also ignore formats like \texttt{JSON}, \texttt{YAML} etc.



\section{The problem of verbosity}
The problem discussed in this thesis is that markup languages easily become verbose. Thus, documents expressed in a certain markup language must be wrapped in this verbosity. Thus, authors must not only concern, but learn to understand and work with this verbosity. Let's look at what we mean with verbosity.

Assume we are writing a cookbook and what to use markup to express the following recipe:

\begin{example}
\tab\tab\tab \textbf{PANCAKES}\\
\tab\tab\tab A secret from the ancient world.\\
\tab\tab\tab \emph{8 servings}.\\

\begin{tabular}{l l l}
Flour & 1 1/2 & cups \\
Baking powder & 3 1/2 & tbps \\
Salt & 1 & tbps \\
Sugar & 1 & tbps \\
Milk & 1 & cups \\
Egg & 1 & pcs\\
Butter, melted & 3 & tbps \\
\end{tabular}
\end{example}

Notice how we can distinguish different parts of the ``document'' merely by looking at it. The header, the preamble, the servings information. Also note how the ingredients seem to be organized in some kind of a table. We can of course achieve the visual effect of the above by simply making use of the \texttt{tab} character. This would however not maintain any semantic information about the columns and rows in the table. In fact, looking closer at the above example, it becomes obvious that the columns depict ingredient name, amount and measurement -- in corresponding order. Whereas each row depicts an ingredient. 

\paragraph{Describing the recipe using XML}
Remember that we are attempting to model the contents of the document, not necessarily it's visuals. Let's look at how we could depict this using XML.
\begin{lstlisting}
<recipe name="Pancakes">
  <description>A secret from the ancient world.</description>
  <ingredients servings="8">
    <ingredient amount="1 1/2" measurement="cups">
      Flour
    </ingredient>
    <ingredient amount="3 1/2" measurement="tbps">
      Baking powder
    </ingredient>
    <ingredient amount="1" measurement="tbps">
      Salt
    </ingredient>
    <ingredient amount="1" measurement="tbps">
      Sugar
    </ingredient>
    <ingredient amount="1" measurement="cups">
      Milk
    </ingredient>
    <ingredient amount="1" measurement="pcs">
      Egg
    </ingredient>
    <ingredient amount="3" measurement="tbps">
      Butter, melted
    </ingredient>
  </ingredients>
</recipe>
\end{lstlisting}
Any person with elemental knowledge of XML, I argue, would have no problem reading and interpreting the above document. However, consider for a moment, the number of meta characters in relation to the number of content characters. With the term \emph{meta characters}, we refer to \emph{control characters}. Any character that was not in the original document. Meaning, information that was implicit in the original document.

\paragraph{Describing the recipe using SGML}
XML was actually derived from a language called SGML (Standard Generalized Markup Language) (\#REF!). A language that usually becomes less verbose than XML, as it does not require the author to repeat the name of an element upon closing it. This of course help us get rid of a number of characters, and consequently lower the verbosity. Consider the following example.

\begin{lstlisting}
<recipe name="Pancakes">
  <description>A secret from the ancient world.</>
  <ingredients servings="8">
    <ingredient amount="1 1/2" measurement="cups">Flour</>
    <ingredient amount="3 1/2" measurement="tbps">Baking powder</>
    <ingredient amount="1" measurement="tbps">Salt</>
    <ingredient amount="1" measurement="tbps">Sugar</>
    <ingredient amount="1" measurement="cups">Milk</>
    <ingredient amount="1" measurement="pcs">Egg</>
    <ingredient amount="3" measurement="tbps">Butter, melted</>
  </>
</>
\end{lstlisting}



\paragraph{Describing the recipe using Jade}
Naturally there are of course those that realized that constantly typing out the ``less-than'', ``greater than'', and ``slash'' characters quite quickly becomes annoying for document authors. Consequently languages such \texttt{Jade}. and \texttt{YAML} (YAML Ain't Markup Language) have been developed. Both of these languages are what is commonly known as white-space sensitive. A concept also used in the programming language \texttt{Python}. In the case of \texttt{Jade} this means carriage returns and indentation is used as a means of detecting delimitations between elements.

Let's look at an example:

\begin{lstlisting}
recipe(name="Pancakes")
  description A secret from the ancient world
  ingredients(servings="8")
    ingredient(amount="1 1/2", measurement="cups") Flour
    ingredient(amount="3 1/2", measurement="tbps") Baking powder
    ingredient(amount="1", measurement="tbps") Salt
    ingredient(amount="1", measurement="tbps") Sugar
    ingredient(amount="1", measurement="cups") Milk
    ingredient(amount="1", measurement="pcs") Egg
    ingredient(amount="3", measurement="tbps") Butter, melted
\end{lstlisting}



\paragraph{Describing the recipe using Markdown}
If we leave the realm of languages free from semantics we encounter other useful creations such as \texttt{Markdown}. Being bound by semantics, a lot more can be ``implicit'' rather than ``explicit''. \texttt{Markdown} is a format intended to be converted to \texttt{HTML} (\#REF!).

To explore the possibilities of the semantically coupled formats, let us instead attempt to model the given document from a visual point of view. Since the format is semantically coupled it is impossible to model the document from a ``data-focused'' point of view. Consider the example below.
% TODO: If we use the term data-focused here then I probably need to revisit the idea of saying that markup for writers is not data transportation oriented. I was wrong.
\begin{lstlisting}
# PANCAKES
A secret from the ancient world.
**8 servings**
| Flour          | 1   | tbps  |
| Baking powder  | 1   | tbps  |
| Salt           | 1   | tbps  |
| Sugar          | 1   | tbps  |
| Milk           | 1   | tbps  |
| Egg            | 1   | tbps  |
| Butter, melted | 1   | tbps  |
\end{lstlisting}

The above code would roughly translate to the following HTML.
\begin{lstlisting}
<h1>PANCAKES</h1>
<p>A secret from the ancient world.</p>
<p><em>8 servings</em></p>
<table>
  <tr>
    <td>Flour</td>
    <td>1</td>
    <td>tbps</td>
  </tr>
  <tr>
    <td>Baking</td>
    <td>1</td>
    <td>tbps</td>
  </tr>
  <tr>
    <td>Salt</td>
    <td>1</td>
    <td>tbps</td>
  </tr>
  <tr>
    <td>Sugar</td>
    <td>1</td>
    <td>tbps</td>
  </tr>
  <tr>
    <td>Milk</td>
    <td>1</td>
    <td>tbps</td>
  </tr>
  <tr>
    <td>Egg</td>
    <td>1</td>
    <td>tbps</td>
  </tr>
  <tr>
    <td>Butter melted</td>
    <td>1</td>
    <td>tbps</td>
  </tr>
</table>
\end{lstlisting}




\paragraph{Describing a recipe using LaTeX}
Continuing on the path of semantically coupled languages we could of course also express this document in LaTeX.

Write about Turing Completeness.

Contrast it to Markdown.

Could it be so that when power increase then complexity also increase?
\begin{lstlisting}
\textbf{PANCAKES}\\
A secret from the ancient world.\\
\emph{8 servings}.\\

\begin{tabular}{l l l}
Flour          & 1 1/2 & cups \\
Baking powder  & 3 1/2 & tbps \\
Salt           & 1     & tbps \\
Sugar          & 1     & tbps \\
Milk           & 1     & cups \\
Egg            & 1     & pcs\\
Butter, melted & 3     & tbps \\
\end{tabular}
\end{lstlisting}



\paragraph{Meta character analysis of the different languages}
Let us reflect and run some quick character counts.

\vspace{12pt}
\begin{tabular}{l c c c c}
Language
& Semantically coupled
& Tot. chars.
& Meta chars.
& Increase (approx.)
\\
\hline
Original & -   & 164 & 0   & 0\\
XML      & NO  & 701 & 537 & 4.27\\
Markdown & YES & 289 & 125 & 1.76
\end{tabular}
\vspace{12pt}






\section{TODO}
- Exploring the problem of verbosity in markup languages

- Briefly exploring how verbose markup disrupts ``flow''.

- Also looking at how minimization cause other issues such as where \texttt{Markdown} is too simple. And where \texttt{Jade} causes forced line-break problems.

- Giving utterly verbose examples from e.g. LaTeX (at seemingly simple tasks such as centering a text-box). Looking at the same solution from an XML perspective. Then briefly suggesting how we instead could do this with much less markup. Leading the reader towards the area of research.







\chapter{Research questions}
Let us formulate this problem as a research question.

\firstresearchquestion

Let's break down what it means to be able to work with an arbitrarily annotated document. Initially, the author would of course have to decide upon an annotation syntax. If the syntax doesn't look like any existing annotation syntax we immediately run into a problem. To be able to convert our arbitrary syntax into any other structured format (such as a \texttt{PDF} or a \texttt{HTML} document) we would need a parser that can ``read and treat'' the document as a tree\footnote{ We'll return to discuss why free text documents (regardless of annotation syntax) can be treated as a trees. }. %TODO: This is not true, think of the underline/highlight example - i.e. overlapping tags

Assume we've created a parser that can ``read and treat'' an arbitrary document written in our syntax. Assume we've written this parser in any programming language of your choice. Assume the parser, written in your programming language of choice, builds up the document tree in memory. Now obviously the next problem arise. A document tree in memory does not do us any good if we cannot export this tree from memory into any other format of our wishing. This means we've only covered the first subproblem of two. We'll express this second subproblem as -- converting the tree representation of the document from memory into any given concrete format of our choosing, that in turn can be written to disk.

In summary -- the first subproblem is an ``interpretation problem'', whereas the second a ``transformation problem''. Without making any assumptions about whether the tools exist or not -- let us formalize these two as requirements for tools: For an author to be able to work with an arbitrary annotation syntax one would need:

\begin{enumerate}
\item A parser (that ``reads the annotated document as a tree''),
\item A transformation engine (that ``converts the read document into a tree'').
\end{enumerate}

In fact, the problem stated above is not actually that complicated. In fact, given a moderately skilled programmer, the problem can easily be solved today -- without introducing any new tools. Let's look at an example.

Assume a document author is approaching this problem. Assume that the author in question, after designing a custom annotation syntax, constructs a set of \texttt{Regular Expressions} that utilizes captures to distinguish data from annotations. Assume then that the author would apply these regular expressions to the document (written in the custom annotation syntax)  using one of the few engines that convert \texttt{regex captures} to \texttt{XML}. Perhaps the author might even write a custom \texttt{regex} to \texttt{XML} converter -- given the lack of existence\footnote{ Search strategies are declared in the method section of the thesis.} of a of a ``de facto'' standardized conversion strategy\footnote{ The conversion is not trivial, and thus many different solutions may be considered ``correct''. Further investigation is found in the empirical sections of this thesis.}.

Given that \texttt{regex} is a widely used (\#REF!), and perhaps the most common, way of searching plain-text (\#REF!), it is a fitting technique for identifying annotations in semi-structured text. Further, \texttt{XML} is a widely used (\#REF!), and perhaps the most common, way of representing documents as hierarchies of information, where annotations are easily separated from data.

Further, \texttt{XML} is a suitable output format because of it being free from semantics. Put this in contrast to for example \texttt{HTML} or \texttt{LaTeX} which both have semantic properties embedded into their specifications. Consider for example the paragraph element in \texttt{HTML} (\texttt{<p>..</p>}). The element denotes semantic significance to the contents inside it. It denotes a paragraph. The same problem arise when using, previously mentioned, \texttt{LaTeX}. According to the the project documentation it is a ``typesetting system [...] for producing [...] documents'' \footnote{ http://www.techscribe.co.uk/ta/latex-introduction.pdf}. Semantically, it is tightly bound to the ``idea'' of the physical document (\#REF!). 

Given that \texttt{XML} lives in an eco-system with other useful languages and tools -- such as \texttt{XSL}, \texttt{XSLT}, \texttt{XPath}, \texttt{DOM} etc. -- it has also proven to be a useful data transportation format (\#REF!). Meaning that if two given systems need to share data, \texttt{XML} can serve as a neutral intermediate format.

Consequently, the idea is to use \texttt{regex}:es to parse an arbitrarily (but known) annotated document, and convert the parsed result into \texttt{XML}. For the purpose of allowing an author to turn an arbitrary annotation syntax into virtually any other structured format. Using \texttt{XML} as an intermediate format, and \texttt{regex} as a means of parsing.

While this all seems reasonable, we have yet not to address the question of why this approach would need any further research. Given that the two above discussed techniques are both well documented and well used -- it might seem this technique is highly employable today.

\begin{enumerate}
\item design/use a parser (that ``reads the annotated document''),
\item design/use a compiler (that ``converts the read document into a tree''),
\item design/use a transformation engine.
\end{enumerate}


Streamlining,   argue  Consequently we come to the point where we instead ask ourselves the following question:

\secondresearchquestion

To explore the possibility of the above question the author has chosen to evaluate whether \texttt{Regular Expressions} can be used to define annotation rules that enables an annotated document to be converted into \texttt{XML}. As expressed by the question below:

\thirdresearchquestion

Given that \texttt{Regular Expressions} may cause the level of complexity to increase (which previously stated as explicitly unwanted) the author has chosen to suggest the use of a simplified subset of the language. As expressed in the question below:

\fourthresearchquestion


\section{Delimitations}
Given that markup languages are prolifically used in areas other than document authoring (such as data transportation), it is naïve to not recognize that an effort in one may allow (or force) change in the other(s). While the author do admit this multifaceted nature of markup languages, \emph{all facets of markup languages expect document authoring are considered outside the scope of this thesis}. In the literature review these other use cases will be touched upon, only as to provide the reader with context. Any effects that may ripple out of the results and into other usage areas of markup languages will within this thesis be ignored. 




\chapter{Method}
Lorem ipsum.

This thesis employs design research, as outlined by Oates (\#REF!), as a means of producing \emph{Constructs} and an \emph{Instantiation}. The first is the result of a literature review, whereas the second a result of artifact development. These are explained in further depth below, where the whole research process is outlined.


\section{Phase I -- Literature review of markup languages}
The first undertaking of the thesis is a literature review aiming to summarize and classify the larger bulk of current and previous markup languages.

The outcome of this literature review is a large table that classifies and compares markup languages. Markup languages are listed on the y-axis, whereas identified aspects or properties are listed on the x-axis.

To delimit the subset of markup languages to a set that seemingly have provided value to the public, the author has decided to use the crowd-sourced, online encyclopedia Wikipedia as starting point. If a certain thing have been mentioned on Wikipedia, the author argue, it must at the very least be known by a few people.

From an article titled ``List of markup languages''\footnote{http://en.wikipedia.org/wiki/List\_of\_markup\_languages} (\#REF!), the author have followed links (into any number of depths) whenever the links implies articles that might introduce the author to yet another markup language.

To, however, minimize the risk of accidentally looking past any splendid markup language that might (e.g.) have been more narrowly applied by researchers, the following search matrix has been constructed. The matrix served as a base for query construction when searching articles via the academically oriented search engine Google Scholar.

\vspace{6pt}
\begin{tabular}{ |l|l|l|l| }
  \hline
  list            & of & markup     & languages \\\hline
  review          & ..   & annotation & languages \\\hline
  summary         & ..   & .. & .. \\\hline
  classification  & ..   & .. & .. \\\hline
  comparison      & ..   & .. & .. \\\hline
  categorization  & ..   & .. & .. \\\hline
\end{tabular}
\vspace{6pt}

Grammatical inflections (such as the infliction listings of list) of all words were of course used, even though not explicitly mentioned in the search matrix.

The total number of markup languages found and analyzed were approximately 50 (\#TODO!).


\subsection{Data analysis}
Whenever a new markup language was encountered, the following strategy was applied:

\begin{enumerate}
\item Add another row to the resulting table for this particular markup language.
\item Consult more literature about this markup language, using the following questions as guidelines: (a) When was it created? (b) By whom? (c) With what intent? (d) How was it actually used? (e) What characteristics make it different from other markup languages? (f) How did it contribute to the future of markup languages?
\item During the above mentioned literature review, when unique aspects of the markup language where found, a new column was introduced to the resulting table. Marking the presence of that aspect in the cell corresponding to the language and that aspect. Further, all the already existing markup languages were revisited and re-evaluated according to the newly introduced aspect.
\item During the above mentioned literature review, not only answers to the questions outlined in step 2 were sought. Step 3 shows how the classification of aspects were conducted. Consequently, when reading about a given markup language, all the previous classified aspects were 
\end{enumerate}

[INSERT FLOWCHART DIAGRAM OF THE PROCESS HERE]


\subsection{Deliverables}
The literature review was intended to produce two deliverables. First, a classification system for markup languages. This classification system is part of the thesis's contribution in terms of what Oates (\#REF!) defines as a \emph{Construct}.

The second outcome of the literature review is the table consisting of classified markup languages. The intent is to show which of the markup languages of today that are free from semantic coupling. This thesis is only concerned with languages free from semantic coupling, and with a list of markup languages that fit this criteria we consequently have something to measure the artifact (which is explained in the following section) against.



\section{Phase II -- Designing a new markup language}
During the second major phase of this thesis -- or rather -- during the empirical phase of this thesis the intent was to produce an artifact. In essence the intention (as more thoroughly described in the research questions section) was to design a new markup language, free from semantics.

However, two requirements were posed upon this new language. It must be (1) \emph{equally powerful} to any existing markup language. Secondly, it must have a (2) \emph{lower syntactical complexity} than any existing markup language. This of course raises a number of questions.



\begin{enumerate}
\item How can we measure \emph{power}?
\item How can we measure \emph{complexity}?
\end{enumerate}

\paragraph{Matching power by converting to another language}
The first question is the easier one, as it will not actually be answered. By constructing a language that is intended to be converted into the most powerful language identified in the literature review (phase I). Consequently the effective power of the developed language (the artifact) will be equal to the language it is converted into. Of course, the real power of the artifact language will probably be much lower, as it is merely a transitional language. Measuring the level of power of the language in itself, has been considered outside the scope of this thesis.

\paragraph{Measuring syntactical complexity through a quantitative formula}
To answer the second question a complexity measure will be introduced. Complexity will be measured by applying the following formula:\\\\
\texttt{ASSUME THAT}\\
\texttt{\tabb C = (number of concepts needed to markup document X) }\\
\texttt{\tabb M = (number of meta characters needed to markup document X) }\\
\texttt{FOR A GIVEN MARKUP DOCUMENT X, }\\
\texttt{WE EXPRESS IT'S SYNTACTICAL COMPLEXITY AS: }\\
\texttt{\tabb complexity = C * M }
% TODO: Needs to be converted to some nicer fig-box


\subsection{Requirements gathering}
Finding data sample. Books from Amazon.
Grouping different concepts in books.
Constructing a sample document.

\subsection{Evaluation}
Constructing the sample document in all the other semantically free languages identified in the literature review -- and then comparing complexity using the quantitative measure previously discussed.






\chapter{Literature review}
Lorem ipsum.






\chapter{Empirics}
Lorem ipsum.


\section{SimEx -- A control structure-light subset of regex}
...


\section{Flexup -- A DSL for expressing regex to XML conversions}
...

\subsection{The annotated file (\texttt{.fup})}
...

\subsection{The annotation definitions file (\texttt{.fupd})}
...

\subsection{The binary}
...





\chapter{Analysis}
Lorem ipsum.





\chapter{Conclusion}
Lorem ipsum.





\chapter{Discussion}
Lorem ipsum.

\section{Future research}
Lorem ipsum.






\end{document}
\documentclass{scrreprt}
% coma version of report class:
% http://tex.stackexchange.com/questions/5948/subtitle-doesnt-work-in-article-document-class

\usepackage{fullpage}
\usepackage[utf8]{inputenc} % åäö
\usepackage{hyperref}
\usepackage{enumitem}
\usepackage[toc]{glossaries}
\usepackage{xcolor}
\usepackage{listings}
\usepackage{tikz}
\usepackage[T1]{fontenc}
\usepackage{ulem}

% Info about natbib:
% https://www.sharelatex.com/learn/Bibliography_management_with_natbib
% http://en.wikibooks.org/wiki/LaTeX/Bibliography_Management#Natbib
\usepackage[round]{natbib}
\bibliographystyle{plainnat}



\setkomafont{disposition}{\normalfont\bfseries}


% Remove date
\date{2014}

\hypersetup{
  colorlinks = true,
  linkcolor = black,
  citecolor = red
}

\title{ A language for expressing \\ abstract markup languages }
\subtitle{A holistic response to constant \\ change in contemporary authorship.}
\author{ Christopher OKHRAVI \\ UPPSALA UNIVERSITY }


%
% Defining research questions
%
\newcommand\researchquestionformat[1]{\begin{quote}#1\end{quote}}
\newcommand\firstresearchquestion{\researchquestionformat{%
  \textbf{(RQ1) Research question 1} \\
  Can the verbosity of document annotation formats (e.g. \texttt{XML}) be decreased, by allowing authors themselves to design annotation formats?%
}}


\newcommand\secondresearchquestion{\researchquestionformat{%
  \textbf{(RQ2) Research question 2} \\
  Can an annotation language
  For an author to be able to design an annotation language, this author must design a domain-specific-language (DSL), a parser, a compiler and a transformation engine. Consequently we come to the point where we instead ask ourselves the following question:


  Can a \texttt{Domain Specific Language (DSL)} be defined, such that a semi-technical author can employ it to design a document annotation format?

  Requiring (A) the combined complexity of the DSL and the annotated document is less than that of a document annotated in a traditional markup language with fixed syntax (e.g. \texttt{XML}).

  Without (B) sacrificing any flexibility of the original markup language.
}}

\newcommand\thirdresearchquestion{\researchquestionformat{%
  \textbf{(RQ3) Research question 3} \\
  Can a process be defined, such that any document can be converted into \texttt{XML} format, using \texttt{Regular Expressions} as rules for document hierarchy?
}}

\newcommand\fourthresearchquestion{\researchquestionformat{%
  \textbf{(RQ4) Research question 4} \\
  Can a language be defined, such that it is a subset of \texttt{Regular Expressions}, where most control characters are replaced by assumptions? Where the intent of the subset is to express annotations of document hierarchy.
}}


\newcommand{\tab}{\hspace*{6pt}}
\newcommand{\tabb}{\tab\tab}



\newenvironment{example}
{ \hrulefill \vspace{12pt} \\ }
{ \\\\ \vspace{12pt} \hrulefill }


\lstset{
  language=XML,
  basicstyle=\color[rgb]{0.3,0.3,0.3}\ttfamily\scriptsize,
  backgroundcolor=\color[rgb]{0.98,0.98,0.98},
  showstringspaces=false,
  breaklines,
  breakatwhitespace,
}






%
%
% GLOSSARY
%
%

\newglossaryentry{document authoring}{
  name={Document Authoring},
  description={The act of writing literature.}
}
\newglossaryentry{EBNF}{
  name={EBNF},
  description={Extended Backus-Naur Form}
}
\makeglossaries






\begin{document}

\maketitle
\tableofcontents
\pagebreak





% % % % % % % % % % % % % % % % % % % % % % 
%
%
%
%    The linguistic note
%
%
%
% % % % % % % % % % % % % % % % % % % % % %
\chapter*{A linguistic note}
A linguistic note in the spirit of Scribe (1980).
 






% % % % % % % % % % % % % % % % % % % % % % 
%
%
%
%    Glossary
%
%
%
% % % % % % % % % % % % % % % % % % % % % %
\glsaddall
\printglossary






% % % % % % % % % % % % % % % % % % % % % % 
%
%
%
%     Introduction
%
%
%
% % % % % % % % % % % % % % % % % % % % % %

\chapter{Introduction}

Since the invention of the writing on the wall, mankind has struggled with an interesting problem of change. Be it change in medium or means (e.g. the process of production or the means of storage). Every generation bear the burden of passing the collective body of knowledge on to the next. Every generation that cause medium or means of the written word to significantly change --  face a somewhat trivial dilemma. To somehow transcend (transform) all existing written content to fit the new ways of today, or leave the currently collected body of human knowledge to slowly perish with time.

\paragraph{TODO}
- A brief history of writing from pens, to the printing press, to computers and word processors, to markup languages.
 


\section{A brief history of Markup}

\begin{tabular}{ l | l | l | l }
  \textbf{Year} &
  \textbf{Language} &
  \textbf{Semantic-free} &
  \textbf{Mixed-content}
  \\ \hline

  1967 &
  \href{http://en.wikipedia.org/wiki/RUNOFF}{RUNOFF, ROFF}

  \\ \hline
  1969 &
  \href{http://en.wikipedia.org/wiki/IBM_Generalized_Markup_Language}{GML}

  \\ \hline
  1973 &
  \href{http://en.wikipedia.org/wiki/Nroff}{NROFF}

  \\ \hline
  1978 &
  TeX

  \\ \hline
  1980 &
  \href{http://www.dtic.mil/docs/citations/ADA125287}{Scribe}

  \\ \hline
  1986 &
  \href{http://en.wikipedia.org/wiki/SGML}{SGML}

  \\ \hline
  1979 &
  \href{http://www.troff.org/history.html}{TROFF}

  \\ \hline
  1990 &
  \href{http://en.wikipedia.org/wiki/Groff_(software)}{GROFF}

  \\ \hline
  1991 &
  \href{http://alistapart.com/article/a-brief-history-of-markup}{HTML}

  \\ \hline
  1992 &
  \href{http://en.wikipedia.org/wiki/Setext}{Setext}


  \\ \hline
  2000
  & \href{http://public.ccsds.org/publications/archive/641x0b2.pdf}{Parameter Value Language}
  & ??

  \\ \hline
  2002 &
  \href{http://en.wikipedia.org/wiki/Asciidoc}{AsciiDoc}

  \\ \hline
  2003 &
  \href{http://en.wikipedia.org/wiki/Org-mode}{OrgMode}

  \\ \hline
  2004 &
  \href{http://en.wikipedia.org/wiki/Markdown}{Markdown}+


  \\ \hline
  &
  \href{http://en.wikipedia.org/wiki/Textile_(markup_language)}{Textile 2} (TODO: 1?)
\end{tabular}


- What does it mean to annotate? What are common document annotation formats?

- Examples of markup languages and showcase of some syntax examples.

- A brief overview of markup languages and how they can be used for (1) Document authoring, and (2) Data transport.

- The relevance of markup for fiction and non-fiction.



\section{Mixed content}
When discussing markup languages -- I often find the need to make a distinction between intentions of (1) \emph{data modeling} and (2) \emph{document authoring}. One could of course argue that any imaginable document (in the document authoring sense) could be encoded in a particular data structure. That is however not the argument I am opposing here. I argue that some languages are more suited for intermingling non-marked up text with marked up text.

% TODO: Use and refer to the terms used by H. Schöning (2001) -- Tamino a DBMS for XML.

The first official W3C working draft of XML (1996)\footnote{http:\/\/www.w3.org/TR/1998/REC-xml-19980210\#sec-mixed-content} already contained the idea of what was referred to as ``mixed content''. Simplified, the idea was that an element may either contain a string of text, or another element, or any combination of the two. In (1998) W3C published a recommended specification of XML 1.0, where the concept of mixed content was expressed as a grammar in Extended Backus-Naur Form (subsequently referred to as EBNF) as follows\footnote{Part of the production for the non-terminal ``content'' has been omitted in favor of readability.}.

\begin{lstlisting}
element ::= EmptyElemTag | STag content ETag 
content ::= (element | CharData)*
\end{lstlisting}

In essence -- any non-terminal \texttt{element} can either produce an \texttt{EmptyElemTag} (i.e. self-closing tag), or the set of a \texttt{STag} (i.e. opening tag) followed by \texttt{content}, followed by an \texttt{ETag} (i.e. closing tag). The non-terminals \texttt{start} (e.g. \texttt{<foo>}) and \texttt{end} (e.g. \texttt{</foo>}) will terminate without any risk of indirect recursion back to the non-terminal \texttt{element}. The non-terminal \texttt{content} is the one interesting to this example.

In the XML 1.0 (1998) specification the following is true\footnote{This is of course merely a partial extraction of the syntax used in the specification.} for the EBNF syntax. The character ``pipe'' (|) represent an \texttt{XOR} choice such that \texttt{A | B} match ``A or B but not both''. The character ``star'' (*) is used as a ``Kleene Star'' such that \texttt{A*} matches ``zero or more occurrences of A''. Parentheses is used to group expressions such that the two mentioned operators can be used on an expression group.

This essentially mean that the non-terminal \texttt{content} can produce either a new \texttt{element} (causing indirect recursion) or \texttt{CharData} (i.e. a string), any number of times. Subsequently allowing for any combination of any length of the two.



\subsection{Exploring an example of mixed content}
I argue, that Mixed content is a fundamental, and necessary, property of markup languages. The following example is partially intended to help enforce this. Assume we have a formatted paragraph of text\footnote{In this example we will disregard whitespace between tokens and assume that whitespace between words in a string remains, but whitespace between strings is omitted and left implicit.}, as outlined in Figure \ref{fig:mixed-content-paragraph}.


\begin{figure}[h]
\centering
\fbox{
The color \underline{is \textbf{now}} \sout{red}.
}
\caption{An example paragraph with formatting.}
\label{fig:mixed-content-paragraph}
\end{figure}


Assume that we want to mark up some aspects of the formatting. The important\footnote{...and admittedly fabricated...} characteristics are that the paragraph, from a markup perspective, can be argued to consist of four unique ``token types''. More precisely (1) plain-text, (2) underline, (3) bold and underline, and finally (4) strike-through.


More importantly, the fact that two words are underlined, whereas one of those words are both underlined and drawn in bold. When working with markup, this situation is usually expressed through inheritance. This is visualized in Figure \ref{fig:mixed-content-tree}. Without inheritance, the number of tokens needed in our lexer would have to be the number of permutations of \texttt{n} (i.e. \texttt{n!}) where \texttt{n} is the number of unique tokens.




Assume we had a lexical analyzer (onwards referred to as ``lexer'') that could read this visual markup. Obviously this is a purely hypothetical lexer as it will be able to identify beginnings and endings of visual markup. Consider for example the human vision and mind. Or an OCR (Optical Character Recognition) engine. A sensible tokenization from this lexer's point of view might go along the following lines (where tokens prepended with \texttt{S-} represents ``starting of'' and \texttt{E-} ``ending of''),




\begin{lstlisting}
  TXT S-UNDERLINE TXT S-BOLD TXT E-BOLD S-STRIKETHROUGH TXT E-STRIKETHROUGH TXT
\end{lstlisting}



Figure \ref{fig:mixed-content-tree} visualizes a reasonable parse-tree in pre-order. In which ending of a particular property is simply denoted by not being a child of that parent. In the figure, the nesting of properties, becomes obvious.
  



\begin{figure}[h]
  \centering
  \begin{tikzpicture}[
    tlabel/.style={pos=1,right=2pt,font=\footnotesize\color{red!70!black}},
    sibling distance=4cm,
  ]
  \node {<root>}
  child {node {"The color"}}
  child {node {<underline>}
    child {node {"is"}}
    child {node {<bold>}
      child {node {"now"}}
    }
  }
  child {
    node {<strikethrough>}{
      child {node {"red"}}
    }
  }
  child {node {"."}}
  ;
  \end{tikzpicture}

  \caption{The example paragraph from \ref{fig:mixed-content-paragraph} represented as a pre-order tree.}
  \label{fig:mixed-content-tree}
\end{figure}



Now that we have established an example input, a model tokenization and a model parse-tree, let us attempt to express this in two different languages.



\subsubsection{Suitable formats for Mixed Content}
Figure \ref{fig:mixed-content-paragraph} in XML.


\begin{figure}[h]
\begin{lstlisting}
The color <underline>is <bold>now</bold> <strikethrough>red</strikethrough>.
\end{lstlisting}
\caption{Expressing Figure \ref{fig:mixed-content-paragraph} in XML.}
\label{fig:mixed-content-xml}
\end{figure}



\subsubsection{Unsuitable formats for Mixed Content}
Mixed content becomes utterly difficult to express in languages that lack a canonical way of expressing it. Attempting to express Figure \ref{fig:mixed-content-paragraph} in JSON, one quickly realizes there are multiple ways and that it is not obvious which was is the one to prefer. No approach I have found\footnote{I have merely conducted non-scientific experiments.} is trivial to parse without telling the parser a particular piece of information.

\begin{figure}[h]
\begin{lstlisting}
{
  "root": [
    "The color ",
    { "emph": ["is", { "keyword": "red" }], },
    "now."
  ]
};
\end{lstlisting}
\caption{Attempting to express Figure \ref{fig:mixed-content-paragraph} in JSON.}
\label{fig:mixed-content-json}
\end{figure}


Consider Figure \ref{fig:mixed-content-json}. At first glance it seems like a perfectly reasonable representation of the discussed paragraph. Using Arrays to sequence, and single-key objects to name elements. However, the limitations become painfully apparent if we ask ourselves the question -- what does an object with two keys represent?

One might argue that the counter example is breaking the rules. We did say ``single-key objects'' and not ``n-key objects''. This is a syntactical requirement that can easily be encoded into the parser (or probably even the lexer). This is however to misunderstand the point. The point is that the JSON specification does not actually enforce this behavior today. Meaning, there is (to my knowledge\footnote{TODO: Do I need a reference or something?}) no way of representing mixed content with labeled elements using JSON, without introducing a requirement which isn't already encoded in the lexer/parser. Meaning, all valid JSON strings, cannot (applying a certain strategy, such as e.g. Figure \ref{fig:mixed-content-json}) be considered valid strings of mixed content. Ergo, JSON is an unsuitable format for mixed content.

Within this thesis, it is this line of reasoning that is referred to when discussing whether a particular format is suitable for representing mixed content or not.




\subsection{Enforcing the importance of mixed content}

Mignet et al. (2003) analyzed different aspects of what they referred to as ``The XML web'' (i.e. "the subset of the Web made of XML documents only"). During this study it was found that 72\% of all documents utilize mixed content. Almost three out of four documents use mixed content. Mignet et al. (2003) conclude that they invalidate the folklore which underestimate the importance of mixed content in XML. Given that XML (in contrast to HTML -- the de facto language of the web) is commonly used a data transportation format, one can reasonably assume that the number of HTML documents making use of mixed content is even higher.



Further, anyone who has ever authored a free-text-oriented document in a markup language of the XML-family (counting improper HTML subsets such as HTML5) I expect have some appreciation of how absurd document creation would be without mixed content.





\subsection{Old stuff TODO}
Markup is not just data transportation. An explanation as to why object notation is a subset of annotations. Meaning that formats like \texttt{JSON} are too data-centric and can consequently not, in any sensible manner, be used for manual document authoring. \texttt{XML} for example can be used for data transport, but in this thesis we'll focus on its markup properties. Consequently also ignore formats like \texttt{JSON}, \texttt{YAML} etc.

Maybe we can even use parse-tree's to symbolize the problem. To show that it is not possible to unambiguously represent mixed content. One needs to perform computation on the parse tree to derive the data.






\section{A Taxonomy of Markup Languages}
In \citeyearpar{coombs} \citeauthor*{coombs} published an article dubbed \emph{Markup Systems and the Future of Scholarly Text Processing}. While the intent of the article, presumably, was to speculate on the future markup -- it also gave us a a solid taxonomy of markup languages. This thesis utilize the categorizations of ``types of markup'' identified by \citet{coombs}, under the term ``markup theory''. \citet{coombs} divide markup types into five categories. (1) Punctuational, (2) Presentational, (3) Procedural, (4) Descriptive, and (5) Abstract. We will now take a closer look at each of these.


\subsection{Punctuational}
Lorem ipsum.


\subsection{Presentational}
Outlining problems.


\subsection{Procedural}
Outlining problems and benefits.


\subsection{Descriptive}
Outlining problems and benefits.


\subsection{Abstract}
Outlining problems and benefits.




\section{Situational analysis}
Why are so many documents still procedural?
Model information flows of contemporary publishing. Figures describing the current state in contrast to the thesis's subjective idea of the ideal state. Refer to Reid (1980).

\subsection{Presentational}
i.e. Microsoft Word etc.
[FIG. ]
Explicit outline of the problems so that we can attack them in the hypothetical ideal.

\subsection{Procedural}
i.e. \LaTeX etc.
[FIG. ]
Explicit outline of the problems so that we can attack them in the hypothetical ideal.

\subsection{Descriptive}
i.e. DocBook, EPUB, Markdown etc.
[FIG. ]
Explicit outline of the problems so that we can attack them in the hypothetical ideal.

\subsection{A hypothetical ideal addressing the problems}
[FIG.]

\subsubsection{Composition \& Abstraction}
The minimal denominator.



















\section{Problem outline}
\subsection{Coupling to compilers}
\subsection{Coupling to concepts}
\subsection{Verbosity \& cognitive overhead}

\section{Suggested solutions}
\subsection{Composition}
Symbiosis and packages
\subsection{Declarative at the highest level of abstraction}
We should strive to express things: Declaratively at the highest level of abstraction. At the intersection of all imaginable formats.
\subsection{Syntax-free languages}
DSL's for grammars.










\chapter{Research questions}
Let us formulate this problem as a research question.

TODO: Write about how the efforts in this thesis relates to the bigger picture discussed in the introduction. How this is merely one effort. It is almost useless without the other efforts.

\firstresearchquestion

Let's break down what it means to be able to work with an arbitrarily annotated document. Initially, the author would of course have to decide upon an annotation syntax. If the syntax doesn't look like any existing annotation syntax we immediately run into a problem. To be able to convert our arbitrary syntax into any other structured format (such as a \texttt{PDF} or a \texttt{HTML} document) we would need a parser that can ``read and treat'' the document as a tree\footnote{ We'll return to discuss why free text documents (regardless of annotation syntax) can be treated as a trees. }. %TODO: This is not true, think of the underline/highlight example - i.e. overlapping tags

Assume we've created a parser that can ``read and treat'' an arbitrary document written in our syntax. Assume we've written this parser in any programming language of your choice. Assume the parser, written in your programming language of choice, builds up the document tree in memory. Now obviously the next problem arise. A document tree in memory does not do us any good if we cannot export this tree from memory into any other format of our wishing. This means we've only covered the first subproblem of two. We'll express this second subproblem as -- converting the tree representation of the document from memory into any given concrete format of our choosing, that in turn can be written to disk.

In summary -- the first subproblem is an ``interpretation problem'', whereas the second a ``transformation problem''. Without making any assumptions about whether the tools exist or not -- let us formalize these two as requirements for tools: For an author to be able to work with an arbitrary annotation syntax one would need:

\begin{enumerate}
\item A parser (that ``reads the annotated document as a tree''),
\item A transformation engine (that ``converts the read document into a tree'').
\end{enumerate}

In fact, the problem stated above is not actually that complicated. In fact, given a moderately skilled programmer, the problem can easily be solved today -- without introducing any new tools. Let's look at an example.

Assume a document author is approaching this problem. Assume that the author in question, after designing a custom annotation syntax, constructs a set of \texttt{Regular Expressions} that utilizes captures to distinguish data from annotations. Assume then that the author would apply these regular expressions to the document (written in the custom annotation syntax)  using one of the few engines that convert \texttt{regex captures} to \texttt{XML}. Perhaps the author might even write a custom \texttt{regex} to \texttt{XML} converter -- given the lack of existence\footnote{ Search strategies are declared in the method section of the thesis.} of a of a ``de facto'' standardized conversion strategy\footnote{ The conversion is not trivial, and thus many different solutions may be considered ``correct''. Further investigation is found in the empirical sections of this thesis.}.

Given that \texttt{regex} is a widely used (\#REF!), and perhaps the most common, way of searching plain-text (\#REF!), it is a fitting technique for identifying annotations in semi-structured text. Further, \texttt{XML} is a widely used (\#REF!), and perhaps the most common, way of representing documents as hierarchies of information, where annotations are easily separated from data.

Further, \texttt{XML} is a suitable output format because of it being free from semantics. Put this in contrast to for example \texttt{HTML} or \texttt{LaTeX} which both have semantic properties embedded into their specifications. Consider for example the paragraph element in \texttt{HTML} (\texttt{<p>..</p>}). The element denotes semantic significance to the contents inside it. It denotes a paragraph. The same problem arise when using, previously mentioned, \texttt{LaTeX}. According to the the project documentation it is a ``typesetting system [...] for producing [...] documents'' \footnote{ http://www.techscribe.co.uk/ta/latex-introduction.pdf}. Semantically, it is tightly bound to the ``idea'' of the physical document (\#REF!). 

Given that \texttt{XML} lives in an eco-system with other useful languages and tools -- such as \texttt{XSL}, \texttt{XSLT}, \texttt{XPath}, \texttt{DOM} etc. -- it has also proven to be a useful data transportation format (\#REF!). Meaning that if two given systems need to share data, \texttt{XML} can serve as a neutral intermediate format.

Consequently, the idea is to use \texttt{regex}:es to parse an arbitrarily (but known) annotated document, and convert the parsed result into \texttt{XML}. For the purpose of allowing an author to turn an arbitrary annotation syntax into virtually any other structured format. Using \texttt{XML} as an intermediate format, and \texttt{regex} as a means of parsing.

While this all seems reasonable, we have yet not to address the question of why this approach would need any further research. Given that the two above discussed techniques are both well documented and well used -- it might seem this technique is highly employable today.

\begin{enumerate}
\item design/use a parser (that ``reads the annotated document''),
\item design/use a compiler (that ``converts the read document into a tree''),
\item design/use a transformation engine.
\end{enumerate}


Streamlining,   argue  Consequently we come to the point where we instead ask ourselves the following question:

\secondresearchquestion

To explore the possibility of the above question the author has chosen to evaluate whether \texttt{Regular Expressions} can be used to define annotation rules that enables an annotated document to be converted into \texttt{XML}. As expressed by the question below:

\thirdresearchquestion

Given that \texttt{Regular Expressions} may cause the level of complexity to increase (which previously stated as explicitly unwanted) the author has chosen to suggest the use of a simplified subset of the language. As expressed in the question below:

\fourthresearchquestion


\section{Delimitations}
Given that markup languages are prolifically used in areas other than document authoring (such as data transportation), it is naïve to not recognize that an effort in one may allow (or force) change in the other(s). While the author do admit this multifaceted nature of markup languages, \emph{all facets of markup languages expect document authoring are considered outside the scope of this thesis}. In the literature review these other use cases will be touched upon, only as to provide the reader with context. Any effects that may ripple out of the results and into other usage areas of markup languages will within this thesis be ignored. 




\chapter{Method}
Lorem ipsum.

This thesis employs design research, as outlined by Oates (\#REF!), as a means of producing \emph{Constructs} and an \emph{Instantiation}. The first is the result of a literature review, whereas the second a result of artifact development. These are explained in further depth below, where the whole research process is outlined.


\section{Phase I -- Literature review of markup languages}
The first undertaking of the thesis is a literature review aiming to summarize and classify the larger bulk of current and previous markup languages.

The outcome of this literature review is a large table that classifies and compares markup languages. Markup languages are listed on the y-axis, whereas identified aspects or properties are listed on the x-axis.

To delimit the subset of markup languages to a set that seemingly have provided value to the public, the author has decided to use the crowd-sourced, online encyclopedia Wikipedia as starting point. If a certain thing have been mentioned on Wikipedia, the author argue, it must at the very least be known by a few people.

From an article titled ``List of markup languages''\footnote{http://en.wikipedia.org/wiki/List\_of\_markup\_languages} (\#REF!), the author have followed links (into any number of depths) whenever the links implies articles that might introduce the author to yet another markup language.

To, however, minimize the risk of accidentally looking past any splendid markup language that might (e.g.) have been more narrowly applied by researchers, the following search matrix has been constructed. The matrix served as a base for query construction when searching articles via the academically oriented search engine Google Scholar.

\vspace{6pt}
\begin{tabular}{ |l|l|l|l| }
  \hline
  list            & of & markup     & languages \\\hline
  review          & ..   & annotation & languages \\\hline
  summary         & ..   & .. & .. \\\hline
  classification  & ..   & .. & .. \\\hline
  comparison      & ..   & .. & .. \\\hline
  categorization  & ..   & .. & .. \\\hline
\end{tabular}
\vspace{6pt}

Grammatical inflections (such as the infliction listings of list) of all words were of course used, even though not explicitly mentioned in the search matrix.

The total number of markup languages found and analyzed were approximately 50 (\#TODO!).


\subsection{Data analysis}
Whenever a new markup language was encountered, the following strategy was applied:

\begin{enumerate}
\item Add another row to the resulting table for this particular markup language.
\item Consult more literature about this markup language, using the following questions as guidelines: (a) When was it created? (b) By whom? (c) With what intent? (d) How was it actually used? (e) What characteristics make it different from other markup languages? (f) How did it contribute to the future of markup languages?
\item During the above mentioned literature review, when unique aspects of the markup language where found, a new column was introduced to the resulting table. Marking the presence of that aspect in the cell corresponding to the language and that aspect. Further, all the already existing markup languages were revisited and re-evaluated according to the newly introduced aspect.
\item During the above mentioned literature review, not only answers to the questions outlined in step 2 were sought. Step 3 shows how the classification of aspects were conducted. Consequently, when reading about a given markup language, all the previous classified aspects were 
\end{enumerate}

[INSERT FLOWCHART DIAGRAM OF THE PROCESS HERE]


\subsection{Deliverables}
The literature review was intended to produce two deliverables. First, a classification system for markup languages. This classification system is part of the thesis's contribution in terms of what Oates (\#REF!) defines as a \emph{Construct}.

The second outcome of the literature review is the table consisting of classified markup languages. The intent is to show which of the markup languages of today that are free from semantic coupling. This thesis is only concerned with languages free from semantic coupling, and with a list of markup languages that fit this criteria we consequently have something to measure the artifact (which is explained in the following section) against.



\section{Phase II -- Designing a new markup language}
During the second major phase of this thesis -- or rather -- during the empirical phase of this thesis the intent was to produce an artifact. In essence the intention (as more thoroughly described in the research questions section) was to design a new markup language, free from semantics.

However, two requirements were posed upon this new language. It must be (1) \emph{equally powerful} to any existing markup language. Secondly, it must have a (2) \emph{lower syntactical complexity} than any existing markup language. This of course raises a number of questions.



\begin{enumerate}
\item How can we measure \emph{power}?
\item How can we measure \emph{complexity}?
\end{enumerate}

\paragraph{Matching power by converting to another language}
The first question is the easier one, as it will not actually be answered. By constructing a language that is intended to be converted into the most powerful language identified in the literature review (phase I). Consequently the effective power of the developed language (the artifact) will be equal to the language it is converted into. Of course, the real power of the artifact language will probably be much lower, as it is merely a transitional language. Measuring the level of power of the language in itself, has been considered outside the scope of this thesis.

\paragraph{Measuring syntactical complexity through a quantitative formula}
To answer the second question a complexity measure will be introduced. Complexity will be measured by applying the following formula:\\\\
\texttt{ASSUME THAT}\\
\texttt{\tabb C = (number of concepts needed to markup document X) }\\
\texttt{\tabb M = (number of meta characters needed to markup document X) }\\
\texttt{FOR A GIVEN MARKUP DOCUMENT X, }\\
\texttt{WE EXPRESS IT'S SYNTACTICAL COMPLEXITY AS: }\\
\texttt{\tabb complexity = C * M }
% TODO: Needs to be converted to some nicer fig-box


\subsection{Requirements gathering}
Finding data sample. Books from Amazon.
Grouping different concepts in books.
Constructing a sample document.

\subsection{Evaluation}
Constructing the sample document in all the other semantically free languages identified in the literature review -- and then comparing complexity using the quantitative measure previously discussed.






\chapter{Literature review}
Lorem ipsum.






\chapter{Empirics}
Lorem ipsum.


\section{SimEx -- A control structure-light subset of regex}
...


\section{Flexup -- A DSL for expressing markup-oriented CFG's}
...

\subsection{The annotated file (\texttt{.fup})}
...

\subsection{The annotation definitions file (\texttt{.fupd})}
...

\subsection{The binary}
...





\chapter{Analysis}
Lorem ipsum.





\chapter{Conclusion}
Lorem ipsum.





\chapter{Discussion}
Lorem ipsum.

\section{Future research}
Lorem ipsum.













\chapter{OLD STUFF BELOW}


\subsection{Verbosity and cognitive overhead}
The problem discussed in this thesis is that markup languages easily become verbose. Thus, documents expressed in a certain markup language must be wrapped in this verbosity. Thus, authors must not only concern, but learn to understand and work with this verbosity. Let's look at what we mean with verbosity.

Assume we are writing a cookbook and what to use markup to express the following recipe:

\begin{example}
\tab\tab\tab \textbf{PANCAKES}\\
\tab\tab\tab A secret from the ancient world.\\
\tab\tab\tab \emph{8 servings}.\\

\begin{tabular}{l l l}
Flour & 1 1/2 & cups \\
Baking powder & 3 1/2 & tbps \\
Salt & 1 & tbps \\
Sugar & 1 & tbps \\
Milk & 1 & cups \\
Egg & 1 & pcs\\
Butter, melted & 3 & tbps \\
\end{tabular}
\end{example}

Notice how we can distinguish different parts of the ``document'' merely by looking at it. The header, the preamble, the servings information. Also note how the ingredients seem to be organized in some kind of a table. We can of course achieve the visual effect of the above by simply making use of the \texttt{tab} character. This would however not maintain any semantic information about the columns and rows in the table. In fact, looking closer at the above example, it becomes obvious that the columns depict ingredient name, amount and measurement -- in corresponding order. Whereas each row depicts an ingredient. 

\paragraph{Describing the recipe using XML}
Remember that we are attempting to model the contents of the document, not necessarily it's visuals. Let's look at how we could depict this using XML.
\begin{lstlisting}
<recipe name="Pancakes">
  <description>A secret from the ancient world.</description>
  <ingredients servings="8">
    <ingredient amount="1 1/2" measurement="cups">
      Flour
    </ingredient>
    <ingredient amount="3 1/2" measurement="tbps">
      Baking powder
    </ingredient>
    <ingredient amount="1" measurement="tbps">
      Salt
    </ingredient>
    <ingredient amount="1" measurement="tbps">
      Sugar
    </ingredient>
    <ingredient amount="1" measurement="cups">
      Milk
    </ingredient>
    <ingredient amount="1" measurement="pcs">
      Egg
    </ingredient>
    <ingredient amount="3" measurement="tbps">
      Butter, melted
    </ingredient>
  </ingredients>
</recipe>
\end{lstlisting}
Any person with elemental knowledge of XML, I argue, would have no problem reading and interpreting the above document. However, consider for a moment, the number of meta characters in relation to the number of content characters. With the term \emph{meta characters}, we refer to \emph{control characters}. Any character that was not in the original document. Meaning, information that was implicit in the original document.

\paragraph{Describing the recipe using SGML}
XML was actually derived from a language called SGML (Standard Generalized Markup Language) (\#REF!). A language that usually becomes less verbose than XML, as it does not require the author to repeat the name of an element upon closing it. This of course help us get rid of a number of characters, and consequently lower the verbosity. Consider the following example.

\begin{lstlisting}
<recipe name="Pancakes">
  <description>A secret from the ancient world.</>
  <ingredients servings="8">
    <ingredient amount="1 1/2" measurement="cups">Flour</>
    <ingredient amount="3 1/2" measurement="tbps">Baking powder</>
    <ingredient amount="1" measurement="tbps">Salt</>
    <ingredient amount="1" measurement="tbps">Sugar</>
    <ingredient amount="1" measurement="cups">Milk</>
    <ingredient amount="1" measurement="pcs">Egg</>
    <ingredient amount="3" measurement="tbps">Butter, melted</>
  </>
</>
\end{lstlisting}



\paragraph{Describing the recipe using Jade}
Naturally there are of course those that realized that constantly typing out the ``less-than'', ``greater than'', and ``slash'' characters quite quickly becomes annoying for document authors. Consequently languages such \texttt{Jade}. and \texttt{YAML} (YAML Ain't Markup Language) have been developed. Both of these languages are what is commonly known as white-space sensitive. A concept also used in the programming language \texttt{Python}. In the case of \texttt{Jade} this means carriage returns and indentation is used as a means of detecting delimitations between elements.

Let's look at an example:

\begin{lstlisting}
recipe(name="Pancakes")
  description A secret from the ancient world
  ingredients(servings="8")
    ingredient(amount="1 1/2", measurement="cups") Flour
    ingredient(amount="3 1/2", measurement="tbps") Baking powder
    ingredient(amount="1", measurement="tbps") Salt
    ingredient(amount="1", measurement="tbps") Sugar
    ingredient(amount="1", measurement="cups") Milk
    ingredient(amount="1", measurement="pcs") Egg
    ingredient(amount="3", measurement="tbps") Butter, melted
\end{lstlisting}



\paragraph{Describing the recipe using Markdown}
If we leave the realm of languages free from semantics we encounter other useful creations such as \texttt{Markdown}. Being bound by semantics, a lot more can be ``implicit'' rather than ``explicit''. \texttt{Markdown} is a format intended to be converted to \texttt{HTML} (\#REF!).

To explore the possibilities of the semantically coupled formats, let us instead attempt to model the given document from a visual point of view. Since the format is semantically coupled it is impossible to model the document from a ``data-focused'' point of view. Consider the example below.
% TODO: If we use the term data-focused here then I probably need to revisit the idea of saying that markup for writers is not data transportation oriented. I was wrong.
\begin{lstlisting}
# PANCAKES
A secret from the ancient world.
**8 servings**
| Flour          | 1   | tbps  |
| Baking powder  | 1   | tbps  |
| Salt           | 1   | tbps  |
| Sugar          | 1   | tbps  |
| Milk           | 1   | tbps  |
| Egg            | 1   | tbps  |
| Butter, melted | 1   | tbps  |
\end{lstlisting}

The above code would roughly translate to the following HTML.
\begin{lstlisting}
<h1>PANCAKES</h1>
<p>A secret from the ancient world.</p>
<p><em>8 servings</em></p>
<table>
  <tr>
    <td>Flour</td>
    <td>1</td>
    <td>tbps</td>
  </tr>
  <tr>
    <td>Baking</td>
    <td>1</td>
    <td>tbps</td>
  </tr>
  <tr>
    <td>Salt</td>
    <td>1</td>
    <td>tbps</td>
  </tr>
  <tr>
    <td>Sugar</td>
    <td>1</td>
    <td>tbps</td>
  </tr>
  <tr>
    <td>Milk</td>
    <td>1</td>
    <td>tbps</td>
  </tr>
  <tr>
    <td>Egg</td>
    <td>1</td>
    <td>tbps</td>
  </tr>
  <tr>
    <td>Butter melted</td>
    <td>1</td>
    <td>tbps</td>
  </tr>
</table>
\end{lstlisting}




\paragraph{Describing a recipe using LaTeX}
Continuing on the path of semantically coupled languages we could of course also express this document in LaTeX.

Write about Turing Completeness.

Contrast it to Markdown.

Could it be so that when power increase then complexity also increase?
\begin{lstlisting}
\textbf{PANCAKES}\\
A secret from the ancient world.\\
\emph{8 servings}.\\

\begin{tabular}{l l l}
Flour          & 1 1/2 & cups \\
Baking powder  & 3 1/2 & tbps \\
Salt           & 1     & tbps \\
Sugar          & 1     & tbps \\
Milk           & 1     & cups \\
Egg            & 1     & pcs\\
Butter, melted & 3     & tbps \\
\end{tabular}
\end{lstlisting}



\paragraph{Meta character analysis of the different languages}
Let us reflect and run some quick character counts.

\vspace{12pt}
\begin{tabular}{l c c c c}
Language
& Semantically coupled
& Tot. chars.
& Meta chars.
& Increase (approx.)
\\
\hline
Original & -   & 164 & 0   & 0\\
XML      & NO  & 701 & 537 & 4.27\\
Markdown & YES & 289 & 125 & 1.76
\end{tabular}
\vspace{12pt}






\section{TODO}
- Exploring the problem of verbosity in markup languages

- Briefly exploring how verbose markup disrupts ``flow''.

- Also looking at how minimization cause other issues such as where \texttt{Markdown} is too simple. And where \texttt{Jade} causes forced line-break problems.

- Giving utterly verbose examples from e.g. LaTeX (at seemingly simple tasks such as centering a text-box). Looking at the same solution from an XML perspective. Then briefly suggesting how we instead could do this with much less markup. Leading the reader towards the area of research.

A need for headings numbered according to context is expressed





\section{Aspects of languages}
\subsection{Can we see it as a matrix?}

\begin{tabular}{ l | c | c | c }
    & Presentation-bound (\(P\)) & Presentation-free (\(\neg P \))
    \\ \hline

    Domain-bound (\(D\)) 
    \\ \hline

    Domain-free (\(\neg D\))
\end{tabular}

\begin{tabular}{ l | l }
    \(\neg P \wedge \neg D\) &
    \(P \wedge \neg D\) \\
    \hline
    \(\neg P \wedge D\) &
    \(P \wedge D\)
\end{tabular}




\subsection{Abstraction from presentation}

\begin{tabular}{ l | l | l }
  \textbf{Category} &
  \textbf{Examples} &
  \textbf{Description}
  \\ \hline

  Presentational
  & Older WYSIG word processors
  & Embedding binary codes
  \\


  Procedural
  & \LaTeX, PostScript, troff
  & Instruction-oriented markup
  \\


  Descriptive
  & XML, Word ML
  & Conceptual descriptions rather than visual
  \\
\end{tabular}


\subsection{Abstraction from domain}

\begin{tabular}{ l | l | l }
  \textbf{Category} &
  \textbf{Examples} &
  \textbf{Description}
  \\ \hline

  Fixed
  & Older WYSIG word processors
  & Embedding binary codes
  \\


  Free
  & \LaTeX, PostScript, troff
  & Instruction-oriented markup
  \\

\end{tabular}







%
%
% BIBLIOGRAPHY
%
%
\bibliography{bibliography}







\end{document}

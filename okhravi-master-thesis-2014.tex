\documentclass{scrreprt}
% coma version of report class:
% http://tex.stackexchange.com/questions/5948/subtitle-doesnt-work-in-article-document-class

\usepackage{fullpage}
\usepackage{caption}
\usepackage[margin=1cm]{subcaption}
\usepackage[utf8]{inputenc} % åäö
\usepackage{hyperref}
\usepackage{enumitem}
\usepackage[toc]{glossaries}
\usepackage[table]{xcolor}
\usepackage{listings}
\usepackage{tikz}
\usetikzlibrary{calc, fit, shapes.geometric, arrows}
\pgfdeclarelayer{background}
\pgfdeclarelayer{foreground}
\pgfsetlayers{background,main,foreground}
\usepackage[T1]{fontenc}
\usepackage{ulem}
\usepackage{fancyvrb}
\usepackage{shorttoc}
\usepackage{epigraph}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{relsize}
\usepackage{fixfoot}


% NATBIB:
% https://www.sharelatex.com/learn/Bibliography_management_with_natbib
% http://en.wikibooks.org/wiki/LaTeX/Bibliography_Management#Natbib
\usepackage[numbers,square,longnamesfirst]{natbib}
\bibliographystyle{plainnat}


% FONT
\setkomafont{disposition}{\normalfont\bfseries}


% DATE
\date{2014 \smaller(v.0.2 please do not distribute)}


% LINKS
\hypersetup{
  colorlinks = false,
  linkcolor = black,
  citecolor = black
}


% COVER
\title{ A metalanguage for expressing \\ descriptive markup languages }
\subtitle{Transform into anything. \\ -- \\ A holistic response to constant \\ change in contemporary authorship.}
\author{ Christopher OKHRAVI \\ UPPSALA UNIVERSITY }


% DETAILED TOC
\renewcommand*\contentsname{Contents (detailed)}


% TAB COMMANDS
\newcommand{\tab}{\hspace*{6pt}}
\newcommand{\tabb}{\tab\tab}


% CODE
\lstset{
  language=XML,
  basicstyle=\color[rgb]{0.3,0.3,0.3}\ttfamily\scriptsize,
  backgroundcolor=\color[rgb]{0.98,0.98,0.98},
  showstringspaces=false,
  breaklines,
  breakatwhitespace,
}


% FANCY CHARACTERS
\newcommand*{\prim}{\ensuremath{\prime}} % Prime



% GLOSSARY
\newglossaryentry{document authoring}{
  name={Document Authoring},
  description={The act of writing literature.}
}
\newglossaryentry{EBNF}{
  name={EBNF},
  description={Extended Backus-Naur Form}
}
\makeglossaries



% FIXED FOOTNOTES
\DeclareFixedFootnote{\footnotePandoc}{http://johnmacfarlane.net/pandoc/README.html}



% TiKZ Figures
\tikzstyle{dot}       = [rectangle, minimum width=.2cm, minimum height=.2cm,text centered, draw=black, fill=white]
\tikzstyle{startstop} = [rectangle, rounded corners, minimum width=3cm, minimum height=1cm,text centered, draw=black, fill=red!30]
\tikzstyle{io}        = [trapezium, trapezium left angle=75, trapezium right angle=105, minimum width=1cm, minimum height=1cm, text centered, draw=black, fill=blue!30]
\tikzstyle{ds}        = [rectangle, minimum width=1cm, minimum height=1cm, text centered, draw=black, fill=black!15]
\tikzstyle{process}   = [circle, minimum width=1cm, minimum height=1cm, text centered, draw=black, fill=orange!30]
\tikzstyle{decision}  = [diamond,   minimum width=2cm, minimum height=2cm, text centered, draw=black, fill=green!30]
\tikzstyle{arrow}     = [thick,->,>=stealth]
\tikzstyle{dotbox}    = [draw, dotted, rectangle, rounded corners, draw=black]
%TODO: I don't need all of these













% % % % % % % % % % % % % % % % % % % % % % 
%
%
%
%             TITLE
%
%
%
% % % % % % % % % % % % % % % % % % % % % %

\begin{document}
\maketitle






% % % % % % % % % % % % % % % % % % % % % % 
%
%
%
%             ABSTRACT
%
%
%
% % % % % % % % % % % % % % % % % % % % % %


\begin{abstract}
\section*{Abstract}
Every day the number of markup languages increase. Every day the number of documents increase. The notion of ``write once, publish everywhere'' most often feels like a sarcastic joke. The two most prominent approaches to document conversion is the $n-to-n$ approach and the $1-to-n$ approach. The first attempting to convert from all conceivable formats to all conceivable formats. The second attempting to standardize the input format.

This thesis suggest an approach less complex than the first, yet more flexible than the other. An $M-to-N$ approach. Where the language constructs of $M$ exist in all languages in $N$. This thesis argue the benefit of converting ``down the abstraction chain'', from more abstract formats to less abstract formats. Or in other words, from less complex formats to increasingly complex formats.

The thesis reach two concrete contributions: (1) A suggestion for how to parse a syntax-less language that encapsulate many of the languages in $M$, and (2) a model of an ideal transformation process for markup, that allow for transformations from $M$ to $N$. Positive side-effects of such an approach is composability, package sharing, and declarative configurability. \\


Keywords: TODO  \\ \\ \\ \\ \\ \\ \\ \\ \\ \\


\smaller
\paragraph{}
\noindent Thank you Fredrik Bengtsson (for being a supervising supervisor), 
Steve McKeever (for down-to-earth technical advice),
and my significant other (for being awesome).
\end{abstract}










% % % % % % % % % % % % % % % % % % % % % % 
%
%
%
%          TABLE OF CONTENTS
%
%
%
% % % % % % % % % % % % % % % % % % % % % %


\shorttoc{Contents}{1} % Only sections
\tableofcontents
\pagebreak











% % % % % % % % % % % % % % % % % % % % % % 
%
%
%
%    Glossary
%
%
%
% % % % % % % % % % % % % % % % % % % % % %
% TODO: Reintroduce glossary!!
%\glsaddall
%\printglossary










% % % % % % % % % % % % % % % % % % % % % % 
%
%
%
%     Background
%
%
%
% % % % % % % % % % % % % % % % % % % % % %

\chapter{Background}
The problem approached in this thesis, is perhaps best summarized by the quote raised in a paper by \citet{krijnen}. A paper that approach the same problem from a different angle. More precisely an $n-to-n$ approach, whereas this thesis explore an $m-to-n$ approach. The meaning of these terms will be further explored in this thesis.

\begin{quote}
``The nice thing about standards is that there are so many to choose from.''\\
\textit{-- Andy Tanenbaum, as quoted by \citet{krijnen}}
\end{quote}

For many good reasons, different formats exist. Simple things such as taste, cause different languages to express the same things through different syntactical constructs. More complex things, such as different levels of needed granularity and control, cause languages to express things at different levels of abstraction. The hypothesis of this thesis is that conscious categorization of languages, according to such properties, can spawn a set of conversion strategies superior to $n-to-n$, according to aspects detailed later in this thesis.

\section{Languages exhibit properties}
\paragraph{Intentions of design} vary from language to language. One, may have been designed with the intent of enabling document authors to (more easily) manually author documents (i.e. eliminating the need for a rich GUI). Another may have been designed with intents of compactness and high precision, at the expense of giving up manual authoring. Another factor one can consider is whether a format is presentation oriented or not. Whether it is designed to directly produce visual output or not. Analyzing the different hypothetical combinations of categorizations makes it apparent that lots of formats exist (and can exist). Merely combining the two categories above, the following category sets, and thus formats, emerge.

\begin{enumerate}
\item Languages intended to support manual document authoring, but not intended to directly produce visual output (e.g. XML).
\item Languages intended to support manual document authoring, and also intended to directly produce visual output (e.g. HTML).
\item Languages not intended to support manual document authoring, but intended to directly produce visual output (e.g. PDF).
\item Languages neither intended to support manual document authoring, nor directly produce visual output (e.g. transportation formats).
\end{enumerate}

Considering the above four different format categories, the need for format conversion programs become apparent. One may for example want to express a document in XML but publish it in PDF. Or express and publish a document in HTML, while however also converting it to some proprietary format of the fourth category in order to feed it as input to some other system.

Exhaustively identifying all possible categorizations of languages is of course all but a trivial task.
% TODO: Thus this is not the goal of this thesis, but rather.... x 

\paragraph{Naive approaches consider conversions between languages, rather than between categories of languages. } Aspiring to be able to convert any input language to any output language is what this thesis refer to as the $n-to-n$ approach. The most trivial approach to this problem is to write a single unique conversion scheme for every input format, mapping it to every output format. Evidently, such an approach would, at the very least, require the number of unique  conversion schemes to be equal to the number of k-permutations of n, where $k=2$. So converting between 10 languages would require, at least, 90 unique conversion strategies, as outlined in Equation \ref{eq:permutations-between-10-languages}, and generally visualized in Figure \ref{fig:n-to-n-conversion-boxes-without-intermediate-format}.

An example of such a naive approach to document conversion may be writing ad-hoc conversion schemes in, e.g., XSLT 2.0.

\begin{equation}
\frac{n!}{(n-k)!} = \frac{10!}{(10-2)!} = 90
\label{eq:permutations-between-10-languages}
\end{equation}

A less naive approach to $n-to-n$ conversion is Pandoc, which make use of an intermediate format. This essentially mean that merely two conversion schemes has to be written per language. One that converts from the language to the intermediate format, and one that converts from the intermediate format to the language. This essentially decrease the (otherwise exponentially increasing) need for unique conversion schemes to merely $2n$. Thus 10 formats would require 20 conversion schemes. Generally visualized in Figure \ref{fig:n-to-n-conversion-boxes-with-intermediate-format}.



\begin{figure}[h]

  \begin{subfigure}{.5\textwidth}
    \centering

    \begin{tikzpicture}[node distance=1cm]
      \node (0A)  [dot]              {$N_1$};
      \node (0B)  [dot, below of=0A] {$N_2$};
      \node (0C)  [dot, below of=0B] {$..$};
      \node (0D)  [dot, below of=0C] {$N_n$};

      \node (1A)  [dot, right of=0A, xshift=1cm] {$N_1$};
      \node (1B)  [dot, right of=0B, xshift=1cm] {$N_2$};
      \node (1C)  [dot, right of=0C, xshift=1cm] {$..$};
      \node (1D)  [dot, right of=0D, xshift=1cm] {$N_n$};

      \draw [arrow] (0A) -- (1A);
      \draw [arrow] (0A) -- (1B);
      \draw [arrow] (0A) -- (1C);
      \draw [arrow] (0A) -- (1D);

      \draw [arrow] (0B) -- (1A);
      \draw [arrow] (0B) -- (1B);
      \draw [arrow] (0B) -- (1C);
      \draw [arrow] (0B) -- (1D);

      \draw [arrow] (0C) -- (1A);
      \draw [arrow] (0C) -- (1B);
      \draw [arrow] (0C) -- (1C);
      \draw [arrow] (0C) -- (1D);

      \draw [arrow] (0D) -- (1A);
      \draw [arrow] (0D) -- (1B);
      \draw [arrow] (0D) -- (1C);
      \draw [arrow] (0D) -- (1D);

    \end{tikzpicture}

    \caption{Naive $n-to-n$ conversion.}
    \label{fig:n-to-n-conversion-boxes-without-intermediate-format}
  \end{subfigure}%
  \begin{subfigure}{.5\textwidth}
    \centering

    \begin{tikzpicture}[node distance=1cm]
      \node (0A)  [dot]              {$N_1$};
      \node (0B)  [dot, below of=0A] {$N_2$};
      \node (0C)  [dot, below of=0B] {$..$};
      \node (0D)  [dot, below of=0C] {$N_n$};

      \node (mid) [dot, right of=0C, xshift=.5cm] {AST};


      \node (1C)  [dot, right of=mid, xshift=.5cm] {$..$};
      \node (1B)  [dot, above of=1C] {$N_2$};
      \node (1A)  [dot, above of=1B] {$N_1$};
      \node (1D)  [dot, below of=1C] {$N_n$};

      \draw [arrow] (0A) -- (mid);
      \draw [arrow] (0B) -- (mid);
      \draw [arrow] (0C) -- (mid);
      \draw [arrow] (0D) -- (mid);

      \draw [arrow] (mid) -- (1A);
      \draw [arrow] (mid) -- (1B);
      \draw [arrow] (mid) -- (1C);
      \draw [arrow] (mid) -- (1D);

    \end{tikzpicture}

    \caption{$n-to-n$ conversion through intermediate format.}
    \label{fig:n-to-n-conversion-boxes-with-intermediate-format}
  \end{subfigure}
  
  \label{fig:n-to-n-conversion-boxes}
\end{figure}




\section{Conversions are approximations}
\paragraph{A single document have multiple potential interpretations, and can thus be expressed differently within the same output language.} The previous sections may have indicated, that given an input document, there exist a single, unanimous, output document, for each output language. While one could argue the truth of such a statement, this thesis considers it untrue.  The opposition may argue, that if there exist, at least one, semantic difference between two output documents, and these documents are claimed to be expressed in the same language, then these two documents cannot possibly be considered to have derived from the same input document. Subsequently, one would reach the conclusion that these output documents, in fact, are either (a) expressed in two different languages, or (b) derived from two different input documents. This thesis argues the opinion of (b), but claim that the difference stem from differences in the interpretation of the same input document, rather than differences caused by different input documents.

In order to clarify the use of the word ``interpretation'', consider the following example. Assume there exist an abstract idea in the mind of a document author. Assume then that the author encodes this idea using some input language. This thesis employ the philosophical standpoint that the concrete input document (i.e. the encoded idea) is always a compromise (i.e. an approximation) of the actual idea. More precisely put, the input document is always an approximation, attempted to, as precisely as possible, encode the envisioned idea. The intent of the conversion tool is then to translate this approximation of the abstract idea, from the input language, to some output language.  However, since the input document is an approximation of the abstract idea, ambiguity arise, and thus multiple different interpretations of the input document are reasonable. A visualization of the concept is given in Figure \ref{fig:diagram-explaining-interpretations}.

\paragraph{From a more pragmatic perspective, one could argue that allowing multiple interpretations of a single input document, simply, is a necessary prerequisite if one is to achieve the goal of ``writing once, and publishing everywhere''.} The very nature of such a goal implies that the sought documents one wishes to publish (i.e. the ``everywhere''), are different manifestations of some abstract idea (i.e. the ``once''). The statement of the goal, inherently seem to imply, that what is sought is manifestations (i.e. variations) and not mere translations. Consequently, this is the view employed in this thesis.



\begin{figure}[h]
    \centering

    \begin{tikzpicture}[node distance=1.7cm]
      \node (idea)  [dot]            {$Idea$};

      \node (E2)  [dot, right of=idea, xshift=1cm] {$Encoding_{(...)}$};
      \node (E1)  [dot, above of=E2]               {$Encoding_{(1)}$};
      \node (E3)  [dot, below of=E2]               {$Encoding_{(i)}$};

      \node (I3) [dot, right of=E2, xshift=3.8cm] {$Interpretation_{(...)}$};
      \node (I1) [dot, above of=I3]               {$Interpretation_{(1)}$};
      \node (I2) [dot, above of=I1]               {$Interpretation_{(2)}$};
      \node (I4) [dot, below of=I3]               {$Interpretation_{(4)}$};
      \node (I5) [dot, below of=I4]               {$Interpretation_{(5)}$};


      \node (L2)  [dot, right of=I3, xshift=3.8cm] {$Language_{(...)}$};
      \node (L1)  [dot, above of=L2]               {$Language_{(1)}$};
      \node (L3)  [dot, below of=L2]               {$Language_{(j)}$};


      \draw [arrow] (idea) -- (E1);
      \draw [arrow] (idea) -- (E2);
      \draw [arrow] (idea) -- (E3);

      % Encodings to interpretations
      \draw [arrow] (E1) -- (I1);
      \draw [arrow] (E1) -- (I2);
      \draw [arrow] (E1) -- (I3);
      \draw [arrow] (E1) -- (I4);
      \draw [arrow] (E1) -- (I5);

      \draw [arrow] (E2) -- (I1);
      \draw [arrow] (E2) -- (I2);
      \draw [arrow] (E2) -- (I3);
      \draw [arrow] (E2) -- (I4);
      \draw [arrow] (E2) -- (I5);

      \draw [arrow] (E3) -- (I1);
      \draw [arrow] (E3) -- (I2);
      \draw [arrow] (E3) -- (I3);
      \draw [arrow] (E3) -- (I4);
      \draw [arrow] (E3) -- (I5);

      % Interpretations to languages
      \draw [arrow] (I1) -- (L1);
      \draw [arrow] (I1) -- (L2);
      \draw [arrow] (I1) -- (L3);

      \draw [arrow] (I2) -- (L1);
      \draw [arrow] (I2) -- (L2);
      \draw [arrow] (I2) -- (L3);

      \draw [arrow] (I3) -- (L1);
      \draw [arrow] (I3) -- (L2);
      \draw [arrow] (I3) -- (L3);

      \draw [arrow] (I4) -- (L1);
      \draw [arrow] (I4) -- (L2);
      \draw [arrow] (I4) -- (L3);

      \draw [arrow] (I5) -- (L1);
      \draw [arrow] (I5) -- (L2);
      \draw [arrow] (I5) -- (L3);
    \end{tikzpicture}

    \caption{An abstract idea in the mind of the author may be encoded in multiple ways. Any encoding may be interpreted in multiple ways. Any interpretation may be manifested in multiple languages.}
    \label{fig:diagram-explaining-interpretations}
  \end{figure}


\paragraph{In order to exemplify the pragmatic usefulness of accepting ambiguous interpretations of input documents, consider an example of a document consisting of a set of paragraphs, intended to be converted to HTML.} One could encode this list of paragraphs, as either a list of \texttt{<li>} elements (i.e. list items), or simply as a set of subsequent \texttt{<p>} elements (i.e. paragraph elements). It is not obvious which one is to be considered the more reasonable, as it depends on how one interprets the subtleties of the input document.

\paragraph{As another example, consider a book, written in some markup language, but intended to be published in HTML.} Perhaps the author wish to publish two versions, where in one, all chapters sequentially follow each other on the same page, and in the other, all chapters are divided into separate pages, hyperlinked to from a table of contents. While both of the output documents are expressed in language HTML, they are indeed examples variations of the original document. In other words, they exemplify different stations of The abstract idea.




\paragraph{Pandoc handles the ability to produce different output documents from the same input document through two facilities.} One facility is templating, and the other is what the documentation refer to as ``scripting''\footnote{TODO: Reference}. The first essentially refer to the idea of constructing a template into which the input document is fed. Naturally, a unique template as needed for every unique output document. The second facility, scripting, is more general as it operates on a concrete JSON representation of the abstract syntax tree, i.e. what has previously been referred to as the intermediate format.

A visual interpretation of these facilities of Pandoc is depicted in Figure \ref{fig:interpretation-of-pandoc}.

The main problem of the scripting approach of Pandoc, is that it is, as outlined by \citet{krijnen}, coupled to this intermediate format. As \citet{krijnen} suggest, it is ``unrealistic to expect that it can represent all document elements which are introduced by any of the existing or future standards''.

\citet{krijnen} suggest a less naive approach, that demonstrate how conversion schemes can be shared across different input and output formats, through working with grammars and a set of Haskell libraries. The main benefit of such an approach is that it enables flexible package sharing, in ways is similar to the eco-system of \LaTeX{} \citep{krijnen}.



\begin{figure}[h]
    \centering

    \begin{tikzpicture}[node distance=1cm]
      \node (0A)  [dot]              {$N_1$};
      \node (0B)  [dot, below of=0A] {$N_2$};
      \node (0C)  [dot, below of=0B] {$..$};
      \node (0D)  [dot, below of=0C] {$N_n$};

      \node (mid) [dot, right of=0C,  xshift=1cm] {AST};
      \node (ext) [dot, above of=mid, yshift=1cm] {$S_i$};

      \node (1C)  [dot, right of=mid, xshift=1cm] {$..$};
      \node (1B)  [dot, above of=1C] {$T_2$};
      \node (1A)  [dot, above of=1B] {$T_1$};
      \node (1D)  [dot, below of=1C] {$T_n$};

      \node (2C)  [dot, right of=1C, xshift=1cm] {$..$};
      \node (2B)  [dot, above of=2C] {$N_2$};
      \node (2A)  [dot, above of=2B] {$N_1$};
      \node (2D)  [dot, below of=2C] {$N_n$};

      \draw [arrow] (0A) -- (mid);
      \draw [arrow] (0B) -- (mid);
      \draw [arrow] (0C) -- (mid);
      \draw [arrow] (0D) -- (mid);

      \draw [arrow] (mid) to[out=120, in=-110] (ext);
      \draw [arrow] (ext) to[out=-70, in=60] (mid);

      \draw [arrow] (mid) -- (1A);
      \draw [arrow] (mid) -- (1B);
      \draw [arrow] (mid) -- (1C);
      \draw [arrow] (mid) -- (1D);

      \draw [arrow] (1A) -- (2A);
      \draw [arrow] (1B) -- (2B);
      \draw [arrow] (1C) -- (2C);
      \draw [arrow] (1D) -- (2D);

    \end{tikzpicture}

    \caption{Interpretation of the Pandoc workflow when making use of the templating ($T_n$) and scripting ($S$) facilities.}
    \label{fig:interpretation-of-pandoc}
  \end{figure}



\paragraph{To summarize, this thesis will concern itself with three types of conversions -- conversions from input format (i.e. encoding) to interpretation, conversions from interpretations to interpretations, and conversions from interpretation to output format.} In other words, this thesis will concern itself only very briefly with conversions from the abstract idea to encodings (i.e. input formats). 





\section{Markup has resolution}

\paragraph{This thesis is based upon the hypothesis that there exist a hierarchy of abstraction, in which conversions are trivial in one direction but significantly more complex in the other.} Call the complex direction ``up'', and the trivial ``down''. If true, then it must be considered futile to attempt to convert ``up'' in the chain, and if converting ``up'' is futile, then it must be considered a priority to express all documents, in potential need of conversion, in a language as ``low'' in the abstraction chain as possible. 

\paragraph{It has already been argued that markup languages can be categorized according to properties, and it is such properties must determine whether a given language is more abstract than another, i.e. weather one language can be converted to another or not.}
An example of such a categorization is whether a language is presentation-oriented or not. One could argue, that the idea of presentation is a specialization, where the idea of describing a thing outside of its domain of presentation is an abstraction. This would mean, that languages concerned with presentation are less abstract (i.e. more specialized) than those not concerned with presentation.

The classical notion of ``separation of concerns'' emphasizes the known benefits of such a strategy. Thus, the separation of presentation and content not only makes intuitive sense, but have also been successfully applied in the real-world. Consider for example how CSS has successfully separated presentation from content in HTML\footnote{http://www.w3.org/standards/webdesign/htmlcss\#whatcss}.

Converting from a non-presentational format to a presentational format is a conversion ``down'' the abstraction chain, if ``level of presentational coupling'' is used as the categorizing property when differentiating between two different languages. Vice versa, the task of converting from a presentational format to a non-presentational format is a conversion ``up'' the abstraction chain. Thus, the latter must be considered a significantly more difficult task than the former, assuming the hypothesis of this thesis is true.

\paragraph{The hypothesis is not merely that presentation and content are different,} but again, that all languages exhibit properties, and that all of these properties can be grouped into categories, where these categories can be ordered hierarchically according to their level of abstraction.

\paragraph{The chain of abstractions is perhaps better explained by analogy of a tree,} rather than a chain. Consider Figure \ref{fig:fictive-specialization-tree}, in which the language $L$ is the most abstract, and the languages ``below'' are specializations of that language. It is important to understand that, like in any hierarchy, branchings may exist. Which becomes apparent through considering the siblings $L1$, $L2$ and their relationship. However, it is also important to remember that the hypothesis suggest to only allow conversions from parent to child (i.e. ``down'') and never from child to parent (i.e. ``up''), nor from child to child (i.e. ``sideways'').

\paragraph{Consider the following concrete example of the three languages HTML, \LaTeX, and Markdown.} Figure \ref{fig:hierarchy-of-abstraction-example-tree} depicts one potential interpretation of the relationship between the three languages. The word \emph{one} is used instead of \emph{the} to emphasize that this particular relationship might not hold when considering all properties of these languages. The figure is to be considered an oversimplified example.

HTML is (in Figure \ref{fig:hierarchy-of-abstraction-example-tree}) depicted as the parent language (i.e. the most abstract), while \LaTeX and Markdown, are depicted as children (i.e. derivations) of that language (and are thus more specialized). The hypothesis thus suggests that converting from HTML to either \LaTeX{} or Markdown should be considered a ``safe'' task, while all other conversions (i.e. ``up'' or ``sideways'') should be considered ``unsafe''.

Consider then the concrete sentence (or rather: document fragment) expressed in all of these languages (still Figure \ref{fig:hierarchy-of-abstraction-example-tree}. The fragment, expressed in HTML, employ the use of the two distinct elements \texttt{em} (emphasis) and \texttt{i} (italics). While these two, in web browsers, render the same, the following quote, from the HTML5 specification,\footnote{http://www.w3.org/TR/html5/text-level-semantics.html\#the-em-element} explain the existence of a semantic difference.

\begin{quote}
``The \emph{em} element isn't a generic \emph{italics} element. Sometimes, text is intended to stand out from the rest of the paragraph, as if it was in a different mood or voice. For this, the \emph{i} element is more appropriate.''
\begin{flushright}
% TODO Make sure all quotes are formatted alike
\textit{-- HTML5 Recommended specification 2014}
%TODO: Ref
\end{flushright}
\end{quote}

The semantic encapsulation of the word ``Cats'' (in the document fragment depicted in Figure \ref{fig:hierarchy-of-abstraction-example-tree}) is thus intended to be different from the semantic encapsulation of the word ``Dogs'' (as they are surrounded by tags with different semantic meaning). The existence of such a difference is the key to understanding the relationship between the abstraction of two languages.

\begin{figure}[h]
  \centering
  \begin{tikzpicture}[
    tlabel/.style={pos=1,right=2pt,font=\footnotesize\color{red!70!black}},
      sibling distance=6.5cm,
    ]
    \node {
      \texttt{<i>Cats</i> and <em>Dogs</em>}
    }
    child {node {
      \texttt{\textbackslash textit\{Cats\} and \textbackslash emph\{Dogs\}}
    }}
    child {node {
      \texttt{*Cats* and *Dogs*}
    }}
    ;
  \end{tikzpicture}

  \caption{Hierarchy of abstraction of the three languages HTML, \LaTeX, and Markdown (top-to-bottom, left-to-right).}
  \label{fig:hierarchy-of-abstraction-example-tree}
\end{figure}

\paragraph{Preservation of the ability to distinguish all elements of different semantic types, should determine whether a string in a language can or cannot be converted to another language.}

Converting the fragment (of Figure \ref{fig:hierarchy-of-abstraction-example-tree}) expressed in HTML to \LaTeX{} poses no immediate problem, since the distinction can be preserved. Converting the fragment expressed in HTML to Markdown, however, pose problems since the distinction cannot possibly be preserved, due to the less rich expressiveness of Markdown. Thus, HTML must (in this simplified example) be considered the more abstract language, and Markdown the more concrete (i.e. less abstract).

To generalize: If a distinction cannot be preserved when converting from language $A$ to language $B$, then $A$ must be considered the relatively superior (i.e. the more abstract) language.

\paragraph{Preserving difference is more fundamental than equivalence.} While the semantic meaning of the markup keywords (actually macros) \texttt{textit} and \texttt{emph} in the language \LaTeX{} might not refer to the exact same semantic meaning as \texttt{i} and \texttt{emph} of HTML, the importance lies in the preservation of the distinction. If one were to only convert between languages that preserved the exact same semantic meaning, constructing an abstraction tree would be a much more difficult task. Simply because semantic equivalence is a philosophically more difficult question, than syntactic difference. Thus, this thesis explore the concept of preservation of semantic difference (through preservation of syntactic difference), and leave semantic equivalence open for future research.

%TODO: The conversion is the semantically closest, and distinction is preserved.

\paragraph{Conversions ``up'' and ``sideways'' are impossible because distinction may have been lost.} Expanding on the idea that a conversion that cause loss in semantic distinction must be considered a conversion ``down'' the abstraction chain, it becomes apparent that conversions in the opposite direction mean converting from a language with ``less information'' to one with more. Analogy to \emph{resolution} may spawn good understanding. An image of 200x200 pixels may very well be scaled down to 20x20 pixels. However a 20x20 pixels cannot possibly be intelligently scaled up to 200x200 pixels. Simply because the 20x20 pixel image contain ``less information'' than needed to express the original image in 200x200 pixels.

The number of semantically different syntactical constructs of a language, perfectly map to the analogy of number of pixels carrying information. As pixels carrying information are destroyed, there is no way to recreate them.

There are of course instances where converting from a low-resolution format to a high-resolution pose no problem at all. Consider for example a 200x200 pixels perfectly black image. Scaling this image down to a resolution of 20x20 pixels, and then scaling it back up to a resolution of 200x200 pose no problem at all. The final image is exactly the same as the original. Consider then on the other hand a 200x200 pixel portrait photograph. Scaling this image down to a resolution of 20x20, and then scaling it back up to 200x200 cause serious information loss. The final image is now merely an approximation of the original.

\paragraph{Retainment of distinction must be determined on the basis of languages, not instances of languages.} In the language of pixels, it is trivial to imagine cases where conversions from low resolution to high resolution indeed cause no loss of information (consider the black square). However, this is, again, a fortunate effect of the specific case. It is equally trivial to imagine a counter-example, where in the same language, such a conversion will cause loss of information (consider the portrait photograph).

The same apply to markup languages. It is trivial to imagine an example where conversion from a low-resolution format (e.g. Markdown) to a high-resolution format (e.g. HTML) will cause no information loss. Consider for example how all information is retained in Figure \ref{fig:low-to-high-resolution-non-destructive}. However, as with pixel resolution, it is equally trivial to imagine an example where information (i.e. distinction) is in fact lost. Consider for example how information is lost in Figure \ref{fig:low-to-high-resolution-destructive}.


\begin{figure}[h]

\begin{subfigure}{.5\textwidth}
  \centering
  \begin{tikzpicture}[
    tlabel/.style={pos=1,right=2pt,font=\footnotesize\color{red!70!black}},
      sibling distance=6.5cm,
    ]
    \node {
      \texttt{<em>Cats</em> and <i>Dogs</i>}
    }
    child {node {
      \texttt{*Cats* and *Dogs*}
    }}
    ;
  \end{tikzpicture}

  \caption{Example of an instance where converting ``up'' the abstraction chain do cause loss of information.}
  \label{fig:low-to-high-resolution-destructive}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
  \centering
  \begin{tikzpicture}[
    tlabel/.style={pos=1,right=2pt,font=\footnotesize\color{red!70!black}},
      sibling distance=6.5cm,
    ]
    \node {
      \texttt{<i>Cats</i> and <i>Dogs</i>}
    }
    child {node {
      \texttt{*Cats* and *Dogs*}
    }}
    ;
  \end{tikzpicture}

  \caption{Example of an instance where converting ``up'' the abstraction chain does not cause loss of information.}
  \label{fig:low-to-high-resolution-non-destructive}
\end{subfigure}

\caption{Example of how conversions from low resolution to high resolution can not be guaranteed to not cause loss of information.}
\end{figure}



TODO. Perhaps draw pixels on grids to exemplify.





\section{An analogy to natural language}
TODO. "Fika".





\section{Destructive and non-destructive conversions}
TODO. Scaling down vs. simply transforming the image within the same resolution.






\section{(Deprecated)}

% TODO: Change to red to mark beginning of potentially deprecated section
\color{red}


\paragraph{Consider a concrete example} where $L$ is a trivial language, such as e.g. the language of equal 0's and 1's (i.e. $0^n1^n$). Let then $L1$ be the language of equal number of 0's and 1's with an extra 0 in the middle (i.e. $0^n0^11^n$). Let $L2$ be the language of any number of repetitions of equal numbers of 0's and 1's (i.e. ($0^n1^n)^*$). The languages $L2$ and $L1$ both exhibit the property of $L$ -- the property of containing a balanced set of 0's and 1's. However either with some addition, or with some variation. Meaning, that $L1$ is more specialized than $L$, and $L$ is more generalized (abstract) than $L1$.





\begin{figure}[h]
  \centering
  \begin{tikzpicture}[
    tlabel/.style={pos=1,right=2pt,font=\footnotesize\color{red!70!black}},
    sibling distance=2.5cm,
  ]
  \node {L}
  child {node {L1}
    child {node {L1.1}
      child {node {...}}
    }
    child {node {...}}
  }
  child {node {L2}
    child {node {...}}
  }
  child {node {...}}
  ;
  \end{tikzpicture}

  \caption{Fictive languages ordered according to specialization of language properties}
  \label{fig:fictive-specialization-tree}
\end{figure}




\begin{figure}[h]
\centering

  \begin{subfigure}{.33\textwidth}
    \centering

    \begin{tikzpicture}[
      tlabel/.style={pos=1,right=2pt,font=\footnotesize\color{red!70!black}},
      sibling distance=2.5cm,
    ]
    \node {$L$}
    child {node {$L1$}}
    child {node {$L2$}}
    ;
    \end{tikzpicture}

    \caption{Ref. names.}
    \label{fig:fictive-specialization-tree-example-ref}
  \end{subfigure}%
  \begin{subfigure}{.33\textwidth}
    \centering
    \begin{tikzpicture}[
      tlabel/.style={pos=1,right=2pt,font=\footnotesize\color{red!70!black}},
      sibling distance=2.5cm,
    ]
    \node {$1^n0^n$}
    child {node {$1^{2n}0^{2n}$}}
    child {node {$1^{n-1}0^{n-1}$}}
    ;
    \end{tikzpicture}

    \caption{Languages.}
    \label{fig:fictive-specialization-tree-example-languages}

  \end{subfigure}%
  \begin{subfigure}{.33\textwidth}
    \centering

    \begin{tikzpicture}[
      tlabel/.style={pos=1,right=2pt,font=\footnotesize\color{red!70!black}},
      sibling distance=2.5cm,
    ]
    \node {$1100$}
    child {node {$11110000$}}
    child {node {$10$}}
    ;
    \end{tikzpicture}

    \caption{Strings.}
    \label{fig:fictive-specialization-tree-example-run}
  \end{subfigure}

  \caption{Concrete example of specialization of some set of languages over the characters $0$ and $1$. TODO: This is probably the best way to illustrate the problem. By discussing the conflicts when converting from node L1 to L2. Even the question of what it \emph{means} to convert from L1 to L2 is hard to answer.}
  \label{fig:fictive-specialization-tree-example}
\end{figure}

Expressed in terms of Object Oriented Programming inheritance -- $L1$ is a specialized kind of $L$. All instances of $L1$, can be specialized into are not necessarily equal  neither necessarily equivalent to each other, nor to the same instance(s) of $L$. Furthermore, all instances of $L$ can be converted into $L1$'s.

Consider for example the Chomsky Hierarchy\footnote{TODO: REFERENCE}. A regular language can be expressed in with context-free grammar, but a context-free language cannot necessarily be expressed with a regular grammar. In other words, a language of a simpler grammar can be expressed using a more complex grammar, but a language expressed in a more complex grammar cannot necessarily be expressed in the simpler grammar.


\begin{figure}[h]
\centering

  \begin{subfigure}{.5\textwidth}
    \centering

    \begin{tikzpicture}[
      tlabel/.style={pos=1,right=2pt,font=\footnotesize\color{red!70!black}},
      sibling distance=2.5cm,
    ]
    \node {$Markdown$}
      child {node {\LaTeX}
        child {node {$HTML$}}
        child {node {$PDF$}}
      }
    ;
    \end{tikzpicture}

    \caption{Strings.}
    \label{fig:concrete-specialization-tree-example-run}

  \end{subfigure}%
  \begin{subfigure}{.5\textwidth}

    \centering

    \begin{tikzpicture}[
      tlabel/.style={pos=1,right=2pt,font=\footnotesize\color{red!70!black}},
      sibling distance=2.5cm,
    ]
    \node {$Markdown$}
      child {node {$+Numbers$}
        child {node {$+TOC$}
          child {node {$PDF$}}
          child {node {$HTML$}}
          child {node {\LaTeX}}
        }
      }
    ;
    \end{tikzpicture}

    \caption{Strings.}
    \label{fig:concrete-specialization-tree-example-run}
  \end{subfigure}


  \caption{TODO}
  \label{fig:concrete-specialization-tree-example}
\end{figure}


\paragraph{Conversions outside or inside the boundary of a language.} One way to measure the complexity of a string conversion is to consider whether the output string of the conversion in question lies within the language boundaries of the input string. In other words: assume two strings $S1$, $S2$, and assume one language $L$. Assume that $S1$ is expressed in language $L$. Consider then the process where $S1$ is converted into $S2$. If the new string $S2$ also is expressed in language $L$ then the conversion must be said to have been done within the boundary of the language. If $S2$ is not expressed in language $L$, then the conversion must be said to have been done outside the boundary of the language. In other words a conversion between languages.

Whether all the concepts of $S1$ was transferable to $S2$ is another interesting dimension, and will be the topic of discussion next.

%TODO: Decide whether to use the word transform or convert

\paragraph{Destructive transformations and non-destructive transformations} must, according to this thesis, be considered different, and another measure of conversion. A non-destructive change usually refer to a reversible change. Whereas a destructive change refer to an irreversible change. This thesis adhere to this definition, and consider a conversion to have been destructive if semantics have been lost, and non-destructive if semantics remain intact.

Even with this definition. Determining whether a string conversion is destructive or non-destructive is not trivial. Conversions within a language could be destructive, and conversions out from a language could be non-destructive.

As a means of determining destructiveness of a conversion, this thesis suggest counting the number of distinctly identifiable semantic delimitations. In other words, consider all the denoted substrings of a string within the formalism of the current language. If the number of substrings of different classes are less in the output language than the input language, then the conversion must be considered destructive. Else non-destructive. Figure TODO depicts a trivial example within the boundaries of a language, and Figure TODO outside the boundaries of a language.


\begin{figure}[h]
  \centering
  
  \begin{subfigure}{.5\textwidth}
    \begin{lstlisting}
      <p>Someone <b>must</b> have</p>
      <b>Someone <p>must</p> have</b>
    \end{lstlisting}

    \caption{Non-destructive}

  \end{subfigure}%
  \begin{subfigure}{.5\textwidth}

    \begin{lstlisting}
      <p>Someone <b>must</b> have</p>
      <p>Someone must have</p>
    \end{lstlisting}

    \caption{Destructive}
  \end{subfigure}

  \caption{Within language boundaries.}
\end{figure}


\begin{figure}[h]
  \centering
  
  \begin{subfigure}{.5\textwidth}
    \begin{lstlisting}
      <p>Someone <b>must</b> have</p>
      Someone **must** have
    \end{lstlisting}

    \caption{Non-destructive}

  \end{subfigure}%
  \begin{subfigure}{.5\textwidth}

    \begin{lstlisting}
      <p>Someone <b>must</b> have</p>
      **Someone must have**
    \end{lstlisting}

    \caption{Destructive}
  \end{subfigure}

  \caption{Outside of language boundaries}
\end{figure}


This thesis suggest an analogy to Object Oriented Programming, and the application of Liskov's Substitution principle. The principle state that it is only allowed to subclass a superclass if any instance of the superclass can be replaced by any instance of the subclass at any point in the code\footnote{TODO: REFERENCE}. In other words, a subclass may only add, or gracefully alter functionality but never remove.

(1)
Liskov's substitution principle is used as means of determining destructiveness of string conversions. However, instead of considering functionality one must consider semantics. Assume two strings $S1$ and $S2$. Then consider the conversion from $S1$ to $S2$

(2) Number of distinctions.


Consider some set of  successfully applies to the two languages transformed between. Given two formats A and B, where A is converted into B. The output format should be considered the subclass, and the input format the superclass. If and only if Liskov's Substitution principle applies, can it be considered a non-destructive transformation.

TODO: Maybe I should argue the whole thing about mentally equivalent or something like that. I.e. that HTML potentially could be considered a subclass of \LaTeX.


The argument in this thesis is thus, that when performing document conversion, if complexity is always introduced and never removed one moves from simpler languages to more complex languages. While this example discuss conversion between two different levels of the Chomsky Hierarchy, the same reasoning apply to languages within one level.

Consider two grammars, $G1$ and $G2$. Let $G1$ be the language of strings matching $(0^n 1^n)$. Let $G2$ be the language of all strings matching $(0^n 1^n)*$. Consequently, $G1$ describes the simpler language of equal number of 0's and 1's (such as $01$ or $00001111$). Whereas the second, more complex, grammar denote the language of any repetition of equal number of 0's and 1's (such as $0101$ or $000111000111$). What's interesting about these two grammars is that $G1$ is contained within $G2$. In other words that all strings of the language defined by $G1$ can also be expressed in the language $G2$.

It is this property of inclusion that this thesis refer to as level of abstraction. Given all this information one might spontaneously approach the problem of markup format conversion with the idea that the ideal would be to have a grammar with such a high level of complexity that it can express any other language. In other words this would mean converting up in the abstraction chain. Considering Figure \ref{fig:fictive-specialization-tree}, a problem with such an approach can be viewed as such. Consider $L$ to be the most specialized language. The language in which all other languages are expressible. As soon as one manages to imagine something that is not expressible in $L$, $L$ need to be rewritten. The severity of the change in the language $L$ will likely depend on how ``deep down'' in the abstraction chain the diversion was discovered. If discovered at a more shallow level the changes will likely be more significant (because the change might have ``ripple effects'' down the chain).

% TODO: Abstraction chain concept still not clear

This thesis however take the opposite approach and suggest that documents should be expressed in the simplest possible language, in an attempt to avoid these unfortunate discoveries of languages outside of the set. Thus the suggestion is that complexity should be layered on top of the simpler language. In other words, if one wants to express a string in the language defined by the grammar $G2$, this thesis suggest that one should express a reasonable simplification in the language defined by $G1$, and then define a process for converting that simpler string into the more complex. Using these two concrete grammars, the transformation would amount to repeating the string some number of times.







\paragraph{TODO -- Old stuff about Pandoc}

Thus, the issues related to Pandoc are two-faceted. On one hand (P1) it may be difficult for semi-programmers to significantly modify the structure of a output document, but on the other -- even (P2) assuming programmers would collectively build an extensive library of transformation scripts for Pandoc, these ``packages'' operate on AST's that represent already complex formats. This facility and it's drawbacks is made clear in the research of \citet{krijnen}, who propose a solution for essentially the same problem as this thesis. The main difference between the approach of this thesis and that of \citet{krijnen} is that they operate on a ``lower level''. Seemingly \citet{krijnen} operate with intents of compilation efficiency through the use of lazy-evaluation, and criticize the Pandoc scripting system for requiring multiple (thus costly) runs over the AST. While the work of \citet{krijnen} is splendid, I unfortunately argue that formal grammars are too complex for such a tool to be used by laymen, or semi-programmers.
%TODO: Make the P's and their problems more clear. Why are they never referred to?

To put this thesis in relation to Pandoc's Scripting facilities, the argument goes along the lines of readability for the masses. I am not arguing that the current approach of Pandoc is absurd -- in fact, argue the approach of \citet{krijnen} is beautiful from a composition point of view, and I argue that the approach of Pandoc is effective in getting around to solve the problem without too much fuss. In this thesis, we will however look at an approach I argue is even more fuss-free than that of Pandoc Scripting. This is why we talk about a holistic approach to markup languages. Subjectively I would argue that both Pandoc and the work of \citet{krijnen} are \emph{reactive} instead of \emph{proactive}. In this thesis we look at the intents of markup languages from a more abstract point-of-view, revisit the actual intents and needs and not simply argue that the problem is the need to convert all formats to all formats. Since the naive approach of converting all formats to all formats, I argue, is a too massive undertaking to provide significant value\footnote{Which of course may change in the future, so I do not intend to be discouraging towards anyone within that field.}.

\begin{quote}
``If I had asked people what they wanted, they would have said faster horses.''
\begin{flushright}
% TODO Make sure all quotes are formatted alike
\textit{-- Commonly attributed to Henry Ford}
\end{flushright}
\end{quote}


\paragraph{Consider instead a ``1-to-n'' problem}. Expressing our documents in a single format but converting into any imaginable format. This approach has been utterly popularized and democratized through the family of languages under XML. If one considers the scenario where an author choose to always express documents in the one format and never in any of the family of n -- then one could argue that solving this problem also solves much of the intent of the ``n-to-n-problem''.

An author can express her document in an arbitrary XML-format (perhaps imagined ``on the fly''), and then use a combination of XSLT and XPath to convert this document into any other imaginable format. Some problems with this approach is (1) the verbosity of the XML languages\footnote{http://www.ibm.com/developerworks/xml/library/x-sbxml/index.html} \footnote{http://blog.codinghorror.com/xml-the-angle-bracket-tax/} \footnote{http://blog.codinghorror.com/revisiting-the-xml-angle-bracket-tax/}, and (2) the fact that because of the lack of any kind of centralization or standardization of format or process, reuse of common transformations become difficult. The composition problem becomes apparent again. Wouldn't you rather use an XML to PDF transformer that the community had curated rather than rewriting one from scratch? Rhetorical question. The problem is again that since XML allows for an infinite number of syntaxes it is non-trivial to encompass all these syntaxes in the transformer.

%TODO: Does it really look like the Pandoc situation?
\paragraph{Of course there are XML subsets like DocBook} that are more commonly accepted than others. Thus, community curated packages exist that can convert between the commonly used format and some other common presentational format e.g. PDF\footnote{http://docbook.sourceforge.net/}. This makes the XML situation look very much like the Pandoc situation but instead of considering completely different syntaxes as input, one considers the XML-based languages. The main problem with this approach is that the input format is fixed. If (e.g.) the Docbook format is not sufficiently rich to express some concept that the author want to express then the community curated converter will be of limited use. Thus, that is a problem with diverging input. In the other end, we can look at diverging output. If working with standards, it is reasonable to assume that there is a standardized (or set of standardized options) for output. What if the author wishes to diverge from the standard output style? How much is flexible and configurable?


\paragraph{Instead, consider the following idea.} Let $N$ be all the languages an author may want to convert to. Let all the concrete languages of $N$ be represented by $n_1, n_2, .., n_n$. Let $M$ be some subset of the languages in $N$ (i.e. $M \subseteq N$). Such that the languages of $M$ exhibit only language properties also exhibited by all languages of $N$. In other words, any string in $M$ must be expressible in any of the languages $n_1, n_2, .., n_n$. In other words, $M$ is a subset of the intersection of all languages in $N$, i.e: $M \subseteq (n_1 \cap n_2 \cap .. \cap n_n)$.

% TODO:
% However, we are only informally talking about sets and subsets here. Since most languages that we want to convert to (e.g. XML, LaTeX) are definitely not regular they are thus at least context-free. In the Chomsky Hierarchy. And since context-free grammars are not closed under language inclusion, this mean we cannot actually certainly say that some language actually is a subset of another language.

\paragraph{In layman terms} the above formulas encapsulate the idea of only allow string conversions from simpler languages to more complex languages, but never the other way around. Assume the two languages $I$ and $O$. If $O$ is more complex than $I$. Expressing a string from $I$ in language $O$ is likely trivial. However, expressing a string from $O$ in language $I$ is not always trivial, and many times not even possible. Since the language $O$ allow constructs that are not expressible in $I$ information must inevitably be lost in the conversion.

\paragraph{In this thesis I argue} that there is a difference between converting a document when the language is within M, from M to N, or within N. I will also argue that employing this division will spawn much more pragmatic solutions than many-to-many-document conversions.

\paragraph{The aim} of this thesis, is to suggest a software eco-system in which a document author can ``write once, [and] publish everywhere''. But of course, that is an oversimplification. More specifically this thesis suggest a way in which we could write our (1) actual document contents once, (2) the transformation process in which we create, what \citet{reid} refers to as, ``derived text'' once, and finally (3) the mapping onto an output format once (per format of course). Note that the two second parts are generalized, and could be published as packages and reused. Note also that the manually authored files at each step are merely configuration files. Between each above step lives a compiler, and these compilers are \emph{not rewritten} on a per-conversion-basis. What also is not obvious in the above explanation is that this thesis suggest an approach that can be likened by a pipeline of templates (similar to a UNIX pipe\footnote{http://en.wikipedia.org/wiki/Pipeline\_(Unix)}) and through this achieve a high level of composability. Instead of a one-pass-transform, transformations are split into composable chunks at different levels of abstraction. But all this will be discussed further on in the thesis.






% TODO: Use or remove?
%\paragraph{Existing solutions}
%\begin{itemize}
%\item No proper pipeline for reusing common things, such as the creation of a table of contents.
%\item Could be solved using e.g XSLT but too verbose. No standard way of sharing libs.
%\item Pandoc is too difficult to configure. Syntax. Haskell/Python. Lacks an approach to share libs. 
%\end{itemize}


\color{black}
% TODO: Returning to black, to mark end of potentially deprecated section.
















\chapter{Problem and hypothesis}
To summarize, the problem focus of this thesis is three-fold.

\begin{itemize}
\item Many languages exist, and converting documents between these languages is non-trivial.
\item Sharing packages/modules for commonly performed conversions (across) language-boundaries is non-trivial.
\item Intercepting conversions in order to achieve a high level of control of the output format is non-trivial.
\end{itemize}




\paragraph{The argument is that these problems stem from a lack of respect for the semantic resolution of markup languages.} Thus, it is the aim of this thesis to prove that it is possible to construct an eco-system of tools that respect resolution. As the proof will be deduced from example, the aim is only to demonstrate the possibility of such an approach. Suitability of the suggested approach will not be verified.

\paragraph{The hypothesis is, thus, that is is possible to construct an eco-system of tools, that through respecting semantic resolution, allow authors to:}

\begin{enumerate}
\item write once and publish everywhere (i.e. convert to any format), and
\item share common conversions as packages across output language boundaries (e.g. generation of tables of contents), while
\item giving authors much control over the shape of the final output format.
\end{enumerate}



\subsection{Deliverables}
The following research goals is this thesis's attempt to mitigate the above problems.

% \paragraph{Deliverable 1:} A suggested list of layers required of high-level categories 

\paragraph{Deliverable 1:} An ideal conversion workflow will be identified through analyzing foundational characteristics of common, current, and previous, markup languages, as well as strengths and fallacies of existing conversion techniques. Ideal is defined as a workflow that never risks loss of markup resolution, where resolution is needed. Further, as enabling package sharing, and high levels of control over conversions.

\paragraph{Deliverable 2:} A high-resolution markup language that fit such an ideal conversion workflow will be suggested.








\section{Thesis outline}
This thesis is centered around two major contributions. A new markup language, and a model for markup transformation.

The high-level goals already introduced in this background chapter is aimed to guide these two contributions, and will constantly be returned to, and reflected upon, throughout the thesis.

The chapters following the initial Background chapter will come in pairs, where the first is aimed toward the first contribution and the second toward the second.

The first set of pairs is a look into different important theoretical concepts of the two. Thus characteristics of markup languages, and characteristics of document processing.

The second set of pairs derive requirements from this first pair, in other words from the existing body of theory. These requirements are posed on the two major contributions.

The third and last set of pairs engineer the two contributions using both the requirements and previously discussed theory as a basis for argument.

Finally the thesis is finished off with the usual conclusions, analysis, and discussions.


























% % % % % % % % % % % % % % % % % % % % % % 
%
%
%
%   Theory: Characteristics of markup languages
%
%
%
% % % % % % % % % % % % % % % % % % % % % %


\chapter{Theory part I -- Characteristics of markup languages}
\label{sec:theory}
This chapter is concerned with exploring, discovering and understanding some of the most important characteristics of markup languages.


\section{Nested markup}
\label{sec:nesting}
A fundamental feature of (many) descriptive markup languages is the ability to nest elements such that a larger set of semantics is expressable than with merely the isolated elements themselves. In most descriptive markup languages, this is implemented as the ability to nest elements in order to create hierarchies. As \citet*{durand} point out, describing documents as ordered hierarchical structures, is the view that has been most prominent. Perhaps because there exist many conflicting suggestions on how to approach non-hierarchical documents. Some of these are outlined by \citet{durand} but an extensive amount of papers have been written on the subject matter\footnote{http://xml.coverpages.org/hierarchies.html}. Even today, hierarchical document structures enjoy the most widespread use. Consider, for example, the large family of XML-based languages such as HTML.



To enforce the difference between nesting and not, consider the following example. Assume we have a formatted paragraph of text\footnote{In this example we will disregard whitespace between tokens and assume that whitespace between words in a string remains, but whitespace between strings is omitted and left implicit.}, as outlined in Figure \ref{fig:mixed-content-paragraph}.


\begin{figure}[h]
\centering
\fbox{
The color \underline{is \textbf{now}} \sout{red}.
}
\caption{An example paragraph with formatting.}
\label{fig:mixed-content-paragraph}
\end{figure}


Assume that we want to mark up some aspects of the formatting. The important\footnote{...and admittedly fabricated...} characteristics are that the paragraph, from a markup perspective, can be argued to consist of four unique ``token types''. More precisely (1) plain-text, (2) underline, (3) bold-underline, and finally (4) strikethrough.





\begin{figure}[h]
  \centering
  \begin{tikzpicture}[
    tlabel/.style={pos=1,right=2pt,font=\footnotesize\color{red!70!black}},
    sibling distance=3.5cm,
  ]
  \node {<root>}
  child {node {"The color"}}
  child {node {<underline>}
    child {node {"is"}}
  }
  child {node {<bold-underline>}
    child {node {"now"}}
  }
  child {
    node {<strikethrough>}{
      child {node {"red"}}
    }
  }
  child {node {"."}}
  ;
  \end{tikzpicture}

  \caption{Pre-order tree representation of Fig. \ref{fig:mixed-content-paragraph}.}
  \label{fig:mixed-content-flat-tree}
\end{figure}



More importantly, the fact that two words are underlined, whereas one of the words that are underlined is actually both underlined and bold. This is an example of a situation where nesting may help us express a string more efficiently. Consider the difference between the non-nested tree representation in Figure \ref{fig:mixed-content-flat-tree} and the nested tree representation in Figure \ref{fig:mixed-content-tree}. In the example that employ nesting, we are actually re-using the semantics of one string to partially describe another.


\begin{figure}[h]
  \centering
  \begin{tikzpicture}[
    tlabel/.style={pos=1,right=2pt,font=\footnotesize\color{red!70!black}},
    sibling distance=3cm,
  ]
  \node {<root>}
  child {node {"The color"}}
  child {node {<underline>}
    child {node {"is"}}
    child {node {<bold>}
      child {node {"now"}}
    }
  }
  child {
    node {<strikethrough>}{
      child {node {"red"}}
    }
  }
  child {node {"."}}
  ;
  \end{tikzpicture}

  \caption{Pre-order tree representation of Fig. \ref{fig:mixed-content-paragraph} employing nesting.}
  \label{fig:mixed-content-tree}
\end{figure}



To understand why nesting is a sensible approach to such situations consider the lexing step of a compiler that read the paragraph in question. Without nesting, the number of terminal tokens needed in the lexer would have to be a number that exponentially increase with every introduced new terminal token, because tokens can be combined with tokens.






\subsection{Arguing the case of nesting}
I argue that nested markup is a fundamental, and necessary property of any pragmatically useful markup language. I will make four arguments in order to support the use of nesting. First I will argue that (1) there is no semantic difference between an element implied by the use of nesting rather than it's explicit counterpart. Secondly, I argue that (2) no significantly complex cognitive leap is required for humans in order to be able to think and discuss nesting. The two remaining arguments enforce the power of nesting. (3) Nesting can reduce the number of required terminal tokens in a markup compiler exponentially. Finally, (4) nesting allow languages to contain an infinite set of semantic tokens while expressing only a finite set of tokens in the compiler.

The arguments will be explored in some detail below.



\paragraph{(1) No semantic difference.}
Other than explicitness, whether a particular string (s) belong to a particular semantic domain (D) is of course constant regardless of what syntax is used to express its belonging. Consider the classic saying ``syntax is not semantics''. Nesting is simply a form of context-sensitive syntax. A form of implicit syntax rather than explicit.

Consider for example chapters and subchapters. One explicit way of distinguishing between the two would be to introduce one syntactic keword denoting a chapter, and another denoting a subchapter. This is an explicit approach. An implicit approach however would be to argue that any chapter ``inside'' of another chapter should be considered a subchapter. In this latter case, only the syntactic keywords for denoting a chapter are required.

One may of course argue that there is a subtle difference between a chapter in a chapter, and a subchapter. That would however be to misinterpret the point. Think not in terms of defining a one, true, global language that encompass all envisionable semantics. Think instead in terms of one author defining her own syntax denoting some semantics envisioned by her. If the author claim the implicit syntax is the same as the explicit syntax then there is no point in arguing differences in specific cases. The author has only claimed that the implicit syntax and the explicit syntax are the same in her particular case.

Thus, I argue, that there is no reason write off nesting with arguments of subtle semantic differences. Simply because the author of a language have all the right to couple any kind of semantic to any kind of syntactic construct. Regardless of whether the syntactic construct is context-free or context-sensitive.




%\paragraph{(2) No significantly complex cognitive leap.}
%TODO: This is actually a really poor argument. It's probably the other way around. Put this as %critique instead. Think of the 32-nested stars-example.





\paragraph{(2) Nesting reduce the number of terminal tokens.}

Assume two unique terminal tokens -- \(A\) and \(B\). Assume a language where a string can belong to either the class A, or the class B, or simultaneously to the class of both A and B. Without the power of nesting, this requires the introduction of a third explicit token -- call it \(C\). Understand that we started out with two initial unique tokens, and that through allowing these tokens to be combined, we were forced to introduce yet another token in order to be able to encode this ``combined'' token.

Now, understand that the above example only require the introduction of one more token, because the starting point consist of a set of (merely) two unique terminal tokens. You may have realized, that introducing single token, is not sufficient if the starting point is a set of three unique terminal tokens. Subsequently far from sufficient if the starting point is a set of 100 unique terminal tokens.


\begin{figure}[h]
\centering
\fbox{
Lexing \underline{without \textbf{\sout{parsing} often}} \sout{make} \textbf{life \sout{oh so} \underline{cumbersome}}.
}
\caption{Exhibiting all combinations of \{bold, underline, strikethrough\}.}
\label{fig:mixed-content-paragraph-complex}
\end{figure}


Analyzing the paragraph outlined in Figure \ref{fig:mixed-content-paragraph}, one may suspect that the underlying language can be described by a grammar consisting of the three tokens outlined in Equation \ref{eq:tokens-as-subset} where plain-text is disregarded. Mixing and matching these tokens, one can trivially create a string, such as Figure \ref{fig:mixed-content-paragraph-complex}, that indeed exhibit the need of more than merely one more terminal token.



\begin{equation}
\{B, U, S\} = \{bold, underline, strikethrough\}
\label{eq:tokens-as-subset}
\end{equation}



Mathematically, the total number of terminals in a grammar that allow combinations of unique terminal tokens, correspond to the number of k-combinations for all k, of \(n\), where \(n\) is the number of unique terminal tokens before combining. In other words: the total number of tokens is the number of possible subsets of the set of unique terminal tokens. Expressed in Equation \ref{eq:formula-all-subsets}.

\begin{equation}
\sum_{0\leq{k}\leq{n}} {n \choose k} = 2^n
\label{eq:formula-all-subsets}
\end{equation}

Since the Equation \ref{eq:formula-all-subsets} also include the empty set we would need to modify the formula such that it subtracts one. However, remember that what we are trying to calculate, is the number of terminal tokens needed to express some grammar of a language where the initial terminal tokens can be combined arbitrarily. Consider the fact that one terminal token of markup languages is plain-text. Consider also the fact that depending on the philosophical orientation of the reader, plain-text can either be considered to be combined with all combinations or none of them. It does not make sense to distinguish between a token that is plain-text-and-bold, from a token that is merely bold. They are essentially the same. In the example, plain-text will be the token that represent the unsuitability of all other tokens -- in other words: if no token match, then the token must be plain-text. Consequently, there will only ever be one combination including the plain-text token regardless of the size of \(n\) and \(k\). Thus, the empty set, in the formula above, will represent the plain-text, and thus there will be no need to subtract one.

Applying this formula to Figure \ref{fig:mixed-content-paragraph} would yield the same number of subsets as depicted in Formula \ref{eq:all-subsets-of-example}. The set used as a starting-point is depicted in Formula \ref{eq:tokens-as-subset}.

\begin{equation}
|\{\{\};\{B\};\{U\};\{S\};\{BU\};\{BS\};\{US\};\{BUS\}\}|
\label{eq:all-subsets-of-example}
\end{equation}


Following this line of thinking an interesting question may be posed. Why merely combinations and not permutations? In other words: many markup languages allow authors to permute the base tokens and not merely combine them. To emphasize the difference consider the subtle distinction between an \textit{\textbf{italicized bold}} string and a \textit{\textbf{bolded italic}} string.

Whether such a distinction would matter in a language is of course dependent on that particular language, and might even vary on a per-token-basis within that particular language. While the distinction might be irrelevant in the example above (combining bold and italics), consider instead the semantic distinction between a \emph{paragraph in a figure} and a \emph{figure in a paragraph}.

Assuming a language where all tokens may be permuted and not simply combined, the total number of tokens required in the grammar would be significantly higher than in the combinatorial case. In mathematical terms, the number corresponds to the number of k-permutations of n, for all k, where n is the number of unique terminal tokens before combining (as depicted in Formula \ref{eq:formula-permutations}).


\begin{equation}
\sum_{0\leq{k}\leq{n}} P(n,k) = 
\sum_{0\leq{k}\leq{n}} \frac{n!}{(n-k)!}
\label{eq:formula-permutations}
\end{equation}

Assuming that the language of the previously discussed paragraph (Figure \ref{fig:mixed-content-paragraph}) allow permutation of tokens. Treating the language as the set of tokens depicted in Equation \ref{eq:tokens-as-subset}, and applying the formula to that set (as depicted in Equation \ref{eq:all-tuples-calculation}), yield 16 tuples, where the tuples in question are depicted in Equation \ref{eq:all-tuples}.



\begin{equation}
\begin{split}
\frac{3!}{(3-3)!} + \frac{3!}{(3-2)!} + \frac{3!}{(3-1)!} + \frac{3!}{(3-0)!} = 
16
\label{eq:all-tuples-calculation}
\end{split}
\end{equation}



Consequently, if one has a language consisting of three original terminal tokens, and wish to construct a language where each possible k-permutation of the three tokens correspond to unique token, then the language must actually contain 16 tokens. In a language that starts with four terminal tokens, the number of actually required terminal tokens is 65. Obviously this is absurd. And thus, this is a reason as to why nesting is such an important corner stone of markup languages. Because it allow languages to omit the explicit use of terminals in favor of implicit hierarchies.




\begin{equation}
\begin{split}
\{
(); \\
(B);
(U);
(S); \\
(B,U);
(B,S);
(U,B);
(U,S);
(S,B);
(S,U); \\
(B,U,S);
(B,S,U);
(U,B,S);
(U,S,B);
(S,U,B);
(S,B,U)
\}
\label{eq:all-tuples}
\end{split}
\end{equation}


% TODO: Call these ``derived'' tokens????




\paragraph{(3) Nesting enable infinite semantics.}
This last argument can easily be understood by contemplating the language of infinitely nestable chapter tokens. In other words, a markup language in which every chapter nested inside of another chapter denotes a subchapter of the chapter level of the previous chapter. In other words the first time a chapter appears, it is a top-level chapter. When a chapter appears within that other chapter it is a sub-chapter. When a chapter appears within that sub-chapter, it denotes a sub-sub-chapter. And so forth into infinity.

Such a language can be described by allowing elements to nest, and decide upon the semantic rule, that any chapter within a chapter is a sub-chapter of that chapter. This is obviously a powerful feature, it is obviously very common in markup languages, and obviously not (trivially) expressible without nesting, in a finite grammar.

Another, less interesting, language is the family of Dyck\footnote{http://en.wikipedia.org/wiki/Dyck\_language}-like languages of infinite, balanced parentheses, as shown in Figure \ref{fig:mixed-content-nesting}.


\begin{figure}[h]
\centering
\fbox{
Consider (the (infinite (nesting) of (parentheses))).
}
\caption{A Dyck language with balanced parentheses.}
\label{fig:mixed-content-nesting}
\end{figure}







\subsection{Non-hierarchical nesting}
\label{sec:non-hierarchical-nesting}
It is important to understand that this thesis employ and endorse the hierarchical model of nesting for delimitational and pragmatic reasons. The necessary property, I argue, is nesting -- not necessarily hierarchical nesting. However, since there is disagreement in the community (as discussed previously), a number of conflicting papers\footnote{http://xml.coverpages.org/hierarchies.html}, with no consensus, on how to approach non-hierarchical nesting, it shall be left as a request for further research.











\section{Mixed content}
\label{sec:mixed-content}
When discussing markup languages -- it is important to identify the sometimes subtle distinction between languages suitable for (1) \emph{data modeling} and languages suitable for (2) \emph{document authoring}. The prior is concerned with structured data, whereas the second is concerned with structured data that fit within the metaphor  of a document. As the first can be described as referring to datastructures -- one could of course argue that any imaginable document (in the document authoring sense) could be encoded in a particular data structure. Thus that there consequently could not exist any clear distinction between data oriented formats, and document oriented formats. That is however not the argument being opposed here. In order to make unambigious use of these terms in the thesis, below an explenation of one separation is explained. Essentially it encompass the idea that some languages are more suited for intermingling non-marked up text with marked up text.

% TODO: Use and refer to the terms used by H. Schöning (2001) -- Tamino a DBMS for XML.

The first official W3C working draft of XML (1996)\footnote{http:\/\/www.w3.org/TR/1998/REC-xml-19980210\#sec-mixed-content} already contained the idea of what was referred to as ``mixed content''. Simplified, the idea was that an element may either contain a string of text, or another element, or any combination of the two. In (1998) W3C published a recommended specification of XML 1.0, where the concept of mixed content was expressed as a grammar in Extended Backus-Naur Form (subsequently referred to as EBNF) as follows\footnote{Part of the production for the non-terminal ``content'' has been omitted in favor of readability.}.

\begin{lstlisting}
element ::= EmptyElemTag | STag content ETag 
content ::= (element | CharData)*
\end{lstlisting}

In essence -- any non-terminal \texttt{element} can either produce an \texttt{EmptyElemTag} (i.e. self-closing tag), or the set of a \texttt{STag} (i.e. opening tag) followed by \texttt{content}, followed by an \texttt{ETag} (i.e. closing tag). The non-terminals \texttt{start} (e.g. \texttt{<foo>}) and \texttt{end} (e.g. \texttt{</foo>}) will terminate without any risk of indirect recursion back to the non-terminal \texttt{element}. The non-terminal \texttt{content} is the one interesting to this example.

The XML 1.0 (1998) specification use the following flavored syntax\footnote{This is of course merely a partial extraction of the syntax used in the specification.} of EBNF. The character ``pipe'' (|) represent an \texttt{XOR} choice such that \texttt{A | B} match ``A or B but not both''. The character ``star'' (*) is used as a ``Kleene Star'' such that \texttt{A*} matches ``zero or more occurrences of A''. Parentheses is used to group expressions such that the two mentioned operators can be used on an expression group.

This essentially mean that the non-terminal \texttt{content} can produce either a new \texttt{element} (causing indirect recursion) or \texttt{CharData} (i.e. a string), any number of times. Subsequently allowing for any combination of any length of the two.



\subsection{Unsuitable formats for Mixed Content}
Mixed content becomes utterly difficult to express in languages that lack a canonical way of expressing it. Attempting to express Figure \ref{fig:mixed-content-paragraph} in JSON, one quickly realizes there are multiple ways and that it is not obvious which was is the one to prefer. No approach I have found\footnote{I have merely conducted non-scientific experiments.} is trivial to parse without telling the parser a particular piece of information.

\begin{figure}[h]
\begin{lstlisting}
{
  "root": [
    "The color ",
    { "emph": ["is", { "keyword": "red" }], },
    "now."
  ]
};
\end{lstlisting}
\caption{Attempting to express Figure \ref{fig:mixed-content-paragraph} in JSON.}
\label{fig:mixed-content-json}
\end{figure}


Consider Figure \ref{fig:mixed-content-json}. At first glance it seems like a perfectly reasonable representation of the discussed paragraph. Using Arrays to sequence, and single-key objects to name elements. However, the limitations become painfully apparent if we ask ourselves the question -- what does an object with two keys represent?

One might argue that the counter example is breaking the rules. We did say ``single-key objects'' and not ``n-key objects''. This is a syntactical requirement that can easily be encoded into the parser (or probably even the lexer). This is however to misunderstand the point. The point is that the JSON specification does not actually enforce this behavior today. Meaning, there is (to my knowledge) no way of representing mixed content with labeled elements using JSON, without introducing a requirement that isn't already encoded in the lexer/parser. Meaning, all valid JSON strings, cannot (applying a certain strategy, such as e.g. Figure \ref{fig:mixed-content-json}) be considered valid strings of mixed content. This is visualized in Figure \ref{fig:mixed-content-venn-json}. Ergo, JSON is an unsuitable format for mixed content.

Within this thesis, it is this line of reasoning that is referred to when discussing whether a particular format is suitable for representing mixed content or not.



\begin{figure}
\centering
\begin{subfigure}{.4\textwidth}
  \centering

  \begin{tikzpicture}[fill=gray]
    \draw (-.75,0) circle (1.25) (1,0)  node [text=black] {$V$}
          ( .75,0) circle (1.25) (-1,0)  node [text=black] {$M$}
          (-2.5,-2.5) rectangle (2.5,2.5) node [text=black] {};
  \end{tikzpicture}
  \caption{Not all valid JSON ($V$) is valid mixed content ($M$).}
  \label{fig:mixed-content-venn-json}
  
\end{subfigure}%
\begin{subfigure}{.4\textwidth}
  \centering

   \begin{tikzpicture}[fill=gray]
    \draw (0,0) circle (2) (0,1.5)  node [text=black] {$M$}
          (0,0) circle (1) (0,0)     node [text=black] {$V$}
          (-2.5,-2.5) rectangle (2.5,2.5) node [text=black] {};
  \end{tikzpicture}
  \caption{All valid XML ($V$) is valid mixed content ($M$).}
  \label{fig:mixed-content-venn-json}
  
\end{subfigure}
\caption{The relationship between valid strings in a language and valid mixed content strings.}
\label{fig:mixed-content-venn}
\end{figure}






%\paragraph{Ambiguity in JSON mixed content}
%TODO: Show examples of how lexing and parsing JSON mixed content may create different parse trees. %Ambiguity.



\subsection{Arguing the case of mixed content}
\citet*{mignet} analyzed different aspects of what they referred to as ``The XML web'' (i.e. "the subset of the Web made of XML documents only"). During this study it was found that 72\% of all documents utilize mixed content. Almost three out of four documents use mixed content. \citet{mignet} conclude that they've invalidated the folklore of underestimating the importance of mixed content in XML. Given that XML (in contrast to HTML -- the de facto language of the web) is commonly used as a data transportation format, one can reasonably assume that the number of HTML documents making use of mixed content is even higher.



Further, anyone who has ever authored a free-text-oriented document in a markup language of the XML-family (counting improper HTML subsets such as HTML5) I expect will have some appreciation of how absurd document creation would be without mixed content.





% TODO: Should I exemplify more? Do we need to show JSON problems using parse trees? Why else are we showing parse trees?
% \subsection{Old stuff TODO}
% Markup is not just data transportation. An explanation as to why object notation is a subset of annotations. Meaning that formats % like \texttt{JSON} are too data-centric and can consequently not, in any sensible manner, be used for manual document authoring. \% texttt{XML} for example can be used for data transport, but in this thesis we'll focus on its markup properties. Consequently also%  % ignore formats like \texttt{JSON}, \texttt{YAML} etc.% 
% 
% Maybe we can even use parse-tree's to symbolize the problem. To show that it is not possible to unambiguously represent mixed content. One needs to perform computation on the parse tree to derive the data.






\section{A Taxonomy of Markup Languages}
\label{sec:taxonomy}
In \citeyear{coombs}, \citeauthor*{coombs} published an article dubbed \textit{Markup Systems and the Future of Scholarly Text Processing}. While the intent of the article, presumably, was to speculate on the future of markup -- it also provided a solid taxonomy of markup languages. This thesis utilize the categorizations of ``types of markup'' identified by \citet{coombs}, under the term ``markup theory''. \citet{coombs} divide markup languages into five categories.The categories are (1) Punctuational, (2) Presentational, (3) Procedural, (4) Descriptive, and (5) Abstract. We will now take a closer look at each of these.

Intermingled in these explanations are also opinions from \citet{bray} -- co-founder of the Open Text Corporation, and co-author of the first XML 1.0 draft specification. The opinions of \citeauthor{bray} was expressed in a blog post exploring the categorizations of \citet{coombs}. It is \citeauthor{bray} that described the work of \citeauthor{coombs} as a ``taxonomy''. The opinions of \citeauthor{bray} are intermingled, both because they contemporize the work of \citeauthor{coombs}, but also because there are some conflicts of opinion between the two. While the statements of \citeauthor{bray} are informal in nature, of course, I argue that his history in the field bestow him reliability.


\subsection{Punctuational}
Punctuational markup refer to the markup we pay little attention to in our everyday life. Spaces, commas, periods, words, sentences etc. As \citet{coombs} points out, punctuational markup has been studied by mankind for hundreds of years. The example \citet{coombs} use to underline that punctuational markup should, indeed, be considered markup and not merely a part of our writing system -- is the following. Consider the all the conflicting opinions on how punctuational markup should be used. You argue it ought to be a  semicolon, I argue it should be a colon. I argue space-delimited dash, you argue dash with no space. Consider the author contemplating whether a certain domain of sentences/words should be presented as \textbf{bold} or as \textit{italics}. This is the same kind of choice, as the mentioned choices between semicolon, colon and so forth. However, today, \textbf{bold} and \textit{italics} can clearly be considered markup, if you consider languages such as HTML.

Understand, that the term punctuational markup, according to \citet{coombs}, does not refer to the idea of encoding punctuation in some other language (i.e. ``\texttt{\&mdash;}'') but rather the actual punctuation itself (i.e. ``\texttt{--}''). In other words, the use of for example character entities in HTML5\footnote{http://dev.w3.org/html5/html-author/charref} is not to be considered punctuational markup.




\subsection{Presentational}
Assume we are talking about a document written on a very old, mechanical typewriter. Presentational markup, according to \citet{coombs} refer to the practice where the author of a document (e.g.) hit the space key multiple times to center text on the page. Or hitting enter multiple times to create paragraphs, or line-breaks.

Now, consider a physical paper document written with a ball pen by hand. With a little bit of effort the author can easily distinguish two parts of the text by applying the technique of writing in italics, or even in cursive. So the reader of the document would understand that author is attempting to communicate some semantic difference between the non-italic and the italic parts.

To understand presentational markup, consider the two above cases -- the typewriter document and the handwritten document. In both cases the presentational elements is embedded within the language of the document. The semantics intended by the author (e.g. a paragraph break) is achieved through presentational means.

\citet{bray} makes the concept of presentational markup, more clear by referring to older What You See Is What You Get (WYSIWYG) word processors. Since modern word processors work with (e.g.) descriptive markup ``under the hood'', this analogy generally no longer holds. But for the sake of the argument, imagine a really old version of e.g. Microsoft Word. But instead, consider word processors of the older model. By surrounding parts of a document with code specific to a particular word processor that particular word processor would know to (e.g.) display that piece of text centered, in bold, in italics or so forth.

One immediate problem with this, the quick reader probably have noticed, can be extracted from one particular sentence -- ``a specific word processor''. Presentational markup requires standardization of what codes to use to mark things such as bold, breaks, sizes and so forth. As you can probably imagine, with the creativity of programmers this quickly gets out of scale.

\subsubsection{Implicitness}
Another more subtle issue, not mentioned by neither \citet{coombs} nor \citet{bray}, is that of implicitness. As the presentational codes of WYSIWYG word processors are not actually visible to the user of the interface -- there exist a mental disconnect. Consider for example the concept of a paragraph break, and the concept of a hard line-break. Now, assume that, in a particular editor, a paragraph break has the same visual appearance as two consecutive hard line-breaks. Assume I type up a document, and hand it to you. How would you then possible know whether I have used paragraph breaks or hard line-breaks? Perhaps, this is the problem \citet{bray} is referring to, when arguing that What You See Is What You Get essentially is a false claim.
%TODO: This claim seems to be false! I have misunderstood and this need to be rethought.



\subsection{Procedural}
The notion of Procedural markup is perhaps most easily communicated through making an analogy to procedural programming languages. Procedural markup refers to the idea of embedding instruction style code directly in the document. Much like as in procedural languages, code duplication, or painful repetition, \citet{bray} argue, can be reduced through elaborate macros or subroutines.

This is the first category that we have discussed, that actually show the user an abstraction of a particular concept rather than the actual concept. Assume an author is to write a particular sentence in a red font. If done through presentational markup, the user would actually see the sentence in red. If however done through procedural markup the user might something similar to Figure \ref{fig:procedural-markup-red-sentence}.


\begin{figure}[h]
\centering
\fbox{
\texttt{
  \textbackslash begin\{red\} This is a red sentence. \textbackslash end\{red\}
  }
}
\caption{A fictive example of procedural markup.}
\label{fig:procedural-markup-red-sentence}
\end{figure}


\begin{figure}[h]
\centering
\fbox{
\texttt{
  .sk 3 a;.in +10 -10;.ls 0;.cp 2 Multiple instructions.
  }
}
\caption{An example of procedural markup, as given by \citet{coombs}.}
\label{fig:procedural-markup-coombs}
\end{figure}



But depending on the syntax of the language, procedural markup might also turn out like a mess of symbols where the compiler instructions are hard to tell from the actual plain-text -- for the untrained professional of course. \citet{coombs} gives an example of procedural markup, as can be seen in \ref{fig:procedural-markup-coombs}, and suggest that the instructions should be interpreted as follows:

\begin{enumerate}
\item Skip three lines -- the equivalent of double-spacing twice.
\item Indent ten columns from the left and ten columns from the right.
\item Change to single-spacing.
\item Start a new page if fewer than two lines remain on the current page.
\end{enumerate}

We will discuss syntax in greater length further on. However, please consider the mental distance between syntax and the actual effect, for a minute. Consider how easy (or hard) it would be for a semi-technical author to remember and properly interpolate these commands into a document. I would argue that it is all but trivial.


\subsubsection{Mutation}
An interesting problem that reasonably may cause mental overhead for a document author working with procedural markup, is one of side-effects. Arguably, the notion of ``mutation'' or ``side-effects'' is a great source of power in programming, but also a great source of frustration. Programs that frequently mutate are generally tricky to debug.

The same is true for procedural markup. With the syntax of Figure \ref{fig:procedural-markup-red-sentence} the text that differs from the ``default'' (i.e. the red text) is enclosed between two language constructs that mark its beginning and end. Admittedly, the behavior of the particular code snippet would be reasonably easy to predict, probably even for a semi-technical author. In the case of Figure \ref{fig:procedural-markup-coombs} however, the language construct is an instruction that mutate the state of the current document from that point and onwards. In a 200 page document with this syntax, it would suddenly be non-trivial to tell whether a particular instruction have cascaded down to a particular point or not.
%TODO: Should I rather use the term control statement

Consider the simple case where a number of words are marked up such that their font color will be red. A metaphor for the syntax of Figure \ref{fig:procedural-markup-red-sentence} would be equivalent to ``change to the red pen, and then change back to whatever pen you had before'', whereas the syntax of Figure \ref{fig:procedural-markup-coombs} merely would say ``change to the red pen''. Obviously this may cause confusion for a document author.




\subsection{Descriptive}
Unfortunately there seems to be no unanimous line between procedural and descriptive markup. \citet{coombs} argue that languages such as \TeX{} and \LaTeX{} are descriptive languages, whereas \citet{bray} argue they are procedural. If I am to speculate, I would assume this particular difference in opinion stems from the differences in state-of-the-art practice at the time in which these two publications were authored.

% In order to gain some understanding in why ambiguity might arise -- ask yourself the following question. If a language L, has a declarative syntax (meaning it cannot possibly be procedural), yet the declarative-style syntax clearly describes presentational... TODO: This is not a good example.

In the time of \citet{coombs}, languages like SGML were still fresh out of the oven (the ISO standard for SGML was published in 1986\footnote{http://www.iso.org/iso/catalogue\_detail.htm?csnumber=16387}). So the mere fact that the language provided a way of expressing parts of the document through description rather than through procedure, seemingly was enough for the language to be called descriptive. In other words, I speculate that at that time it was sensible to consider a markup language to be descriptive, so long as it showed significant descriptive capabilities. Regardless of whether it also provided ways of expressing procedural markup. In the case of \LaTeX{}, consider for example language constructs such as \texttt{\textbackslash vspace} (i.e. a command that inserts vertical space). Or consider language constructs such \texttt{\textbackslash color\{red\}} which essentially say -- ``change to the red pen, from now and onwards''. Both examples are clearly procedural.

Put this in contrast to the time of \citet{bray}, where languages like XML had succeeded SGML and provided almost purely descriptive facilities. In other words, I argue that markup languages at this point in time, could only be considered descriptive, if they \emph{only} allowed for descriptive (and never procedural) expressions.

Understand, that attributing this difference of opinion to the different contexts at those points in time, is of course completely speculative from my part. Reasonably so I would argue however.

Following in this thesis I will use a notion of descriptive markup, more similar to that of \citet{bray}. Following the definition both stated by \citet{coombs} as well as \citet{bray} -- that descriptive markup denotes what a given part of the document \emph{is} rather that what it \emph{does}. I.e. denoting what parts of the text belong to which class. However, realizing that the mistake I argue \citet{coombs} made, was allowing languages with both procedural and descriptive capabilities to be called descriptive. In this thesis a markup language exhibiting any kind of procedural element will be considered a procedural markup language, regardless of its descriptive capabilities.


\subsection{Referential}
\label{sec:referential-markup}
Another category of markup described by \citet{coombs} is referential markup. The idea of referential markup essentially encompass the idea that we at some point (P1) in our document want to be able to include another part of a document or some string of characters (P2). Regardless of whether P1 and P2 are specified in different documents or even different machines.

Returning to the previously mentioned example with the character entity \texttt{\&mdash;}. If treated as referential markup, we might want every occurrence of that character entity (i.e. that piece of referential markup) to be replaced by the actual character ``\texttt{--}''.

\citet{coombs} argue that referential markup can exist in both procedural markup languages and descriptive. In the first it may take the form of user-defined variables. Whereas in the second, the case of character entity replacement outlined above is a prime example.


\subsection{Metamarkup}
Metamarkup is the term \citet{coombs} use to refer to markup languages that specify, extend, and/or constraint markup languages. While not explicitly mentioned by \cite{coombs}, languages such as DTD (Document Type Definition), XML Schema, and RelaxNG are all metamarkup languages.

%TODO: Maybe not mentioned because they were invented later?

From a linguistic point of view, a metamarkup language can be seen as a metalanguage\footnote{http://en.wikipedia.org/wiki/Metalanguage} used to formally discuss an object language. Where in a concrete example the first could be RelaxNG and the second XML.




\section{A hierarchy of abstraction}
\label{sec:abstraction-hierarchy}
It is important to understand that \citet{coombs} employ the wording ``types of markup'' when discussing the categories explained in section \ref{sec:taxonomy}, whereas \citet{bray} discuss ``a taxonomy of markup''. While similarly sounding indeed, nuances emerge when considering the fact that \citet{bray} only discuss three of the total six categories outlined by \citet{coombs}. The three included in the, so called, taxonomy are (1) presentational, (2) procedural, and (3) descriptive. In fact, also \citeauthor{coombs} specifically highlight these three, and say that they are ``in direct competition''.

If you would allow me to speculate, then I would argue this stems from the way these three categories nicely describe three different levels of abstraction given an instance of a document. Or to look at it from the other end, consider the following. Punctuational markup can very well be expressed using presentational markup, as well as procedural, as well as descriptive. Referential markup can be expressed using the two latter, and the same goes for metamarkup.

In essence -- punctuational, referential and metamarkup are essentially concepts that \emph{can be expressed}. Whereas presentational, procedural and descriptive markup are \emph{ways to express} these concepts (amongst others). Making an analogy to programming languages. Descriptive, procedural and presentational would represent the programming paradigms, and the other markup categories would represent concepts exhibited in these programming paradigms.

Allow me to stress again, that this is my subjective interpretation of the situation. But consider it for a moment and I assume you also will find it reasonable. Consider for example how machine code can be abstracted into procedural code, and how procedural code can be abstracted into declarative. Then consider how this analogy conveniently align with the types of markup.


\begin{figure}[h]
\centering

  \begin{tikzpicture}
  \coordinate (Descriptive) at (-5,0) {};
  \coordinate (Procedural) at ( 5,0) {};
  \coordinate (Presentational) at ( 0,4.5) {};
  \draw (Descriptive) -- (Presentational);
  \draw (Procedural) -- (Presentational);
  \foreach \y/\Descriptive in {0/Presentational, 1/Procedural, 2/Descriptive} {
      \draw ($(Descriptive)!\y/3!(Presentational)$) -- ($(Procedural)!\y/3!(Presentational)$) node[midway,above] {\Descriptive};
  }
  \end{tikzpicture}

\caption{A hierarchy of markup language abstraction.}
\label{fig:markup-types-hierarchy}
\end{figure}



Consequently I argue that these three ``categories'' of the taxonomy can be viewed as paradigms of markup abstraction. Much like we have invented facilities to abstract away machine level instructions, we have invented facilities to abstract low level instructions for document content transformation. This hierarchy is visualized in Figure \ref{fig:markup-types-hierarchy}.  


% We are talking about _instances_ of documents, which is why we are not regarding abstract markup languages.









\section{Generic Identifiers and Validity}
\label{sec:theory:generic-identifiers-and-validity}
\DeclareFixedFootnote{\refXMLspec}{http://www.w3.org/TR/WD-xml-961114}

In XML, the author of a document is given the freedom to mark up her document with whatever combination of element names the author see fit\refXMLspec. As XML is a subset of SGML\refXMLspec, and SGML ascended from XML\footnote{http://www.sgmlsource.com/history/roots.htm}, it is only reasonable to assume that this concept of arbitrary element names was already present in the earlier language GML. In GML, this concept was actually referred to as Generic Identifiers (GI's) \citet{goldfarb}.

According to \citet{goldfarb}, GML was designed upon two ``novel postulates''. Firstly that (1) markup should describe a document's structure rather than the processing to be performed. A document once expressed descriptively should suffice for all future processing. Secondly that (2) markup should be rigorous. So that programs can be used for processing documents.

The ingenious move of GML was to only let the markup language cover the first point of the two above \citet{goldfarb}. In other words, GML allow an author describe a document's structure in any way she see fit, using the mnemonic tag -- i.e. a GI.

In order to achieve the second goal, \citet{goldfarb} explained that GML documents ought to be coupled with GML Models. Where the GML Model notation essentially is a notation derived from the BNF-notation, but where the intent is to define the set of all valid elements. Perhaps this is the forefather of the Document Type Definition (DTD) together with the Document type Declaration that were introduced into SGML and followed into the subset XML\footnote{http://www.w3.org/TR/WD-xml-961114\#dt-dtd}.

\begin{figure}[h]
\centering
\begin{lstlisting}
:p.This added information, called :q.markup::q., serves two purposes: 
:ol.
:li.it separates the logical elements of the document; and 
:li.it specifies the processing functions to be performed on those elements. 
::ol.
\end{lstlisting}
\caption{An example of GML syntax by \citet{goldfarb}.}
\end{figure}




\subsection{Tag-validity and type-validity}
%TODO: Remember to use this piece of information!!
\label{sec:tag-validity-type-validity}
\DeclareFixedFootnote{\refSGMLspec}{http://xml.coverpages.org/wg8-n1929-g.html}
The observant reader may have noticed that a parallel was drawn between GML Models and DTD's in combination with Document Type Declarations (commonly referred to as DocTypes). In SGML an explicit distinction was made between, what was called, tag-validity and type-validity\refSGMLspec. GML Models made no such distinction and both concepts were thus instead encompassed into the idea of GML Models.

The SGML specification\refSGMLspec defined a type-valid SGML document as a document conforming to it's encompassing DTD. Subsequently defined a tag-valid SGML document to be a document that is ``fully tagged''. A document were to be considered ``fully tagged'' if every element in the document is composed of a start-tag with a generic identifier and a corresponding end-tag.

Informally put, the difference between tag-validity and type-validity can be explained as such. Tag-validity describes whether a particular document is a syntactically valid document within the language in question (e.g. XML or SGML). Whereas type-validity describes whether that particular document is semantically valid within a given domain(usually specified via a DTD).

Obviously, both are important for, as \citet{goldfarb} puts it, rigor.


\subsection{Arguing the case of Generic Identifiers}
\label{sec:arguing-the-case-of-generic-identifiers}
\citet{goldfarb} claimed that the classic markup language Scribe (constructed by \citet{reid}) manage to avoid procedural markup completely, much because of the effectivity of Generic Identifiers (GI's).

It has already been established, in Section \ref{sec:abstraction-hierarchy}, that in the hierarchy of Markup types, it is desirable to reach a state where all manually authored markup is descriptive. Consider however, how the domain of descriptive markup languages can be divided up into two sub-domains. One in which GI's exist. Where element names may be made up on the fly, by the document author. Then another, in which GI's do not exist. This second language is thus delimited by a fixed number of Identifiers.

While the expressiveness of both languages can be argued to be infinite for both languages if they exhibit the nesting property, there is still reason to argue their relative expressiveness. For one, the first language exhibits infinite expressiveness both through nesting and through an infinite number of element names. The second language however, only exhibits infinite expressiveness through nesting. Thus it is reasonable to argue that the infinite expressiveness of language one is greater then the infinite expressiveness of the second language. In the sense that there are some strings that can be encoded in first language which cannot ever possibly be encoded in the second. Consider a GI expressed in the first language that the second language does not allow. In other words, the cardinality of the set of infinite languages expressible in language one is greater than that of in language two.

Another property closely tied to that of expressiveness is the possibility for an author to express some piece of information at the highest possible abstract level. As there exist no limit on the number of elements an author can create, she can describe every object, in an infinitely diverse set of objects, using a single unique GI per object. The author never runs the risk of ``running out of distinctions''.

This last argument has the splendid benefit that an author can allow herself to push decisions into the future. \citet{coombs} argue similarly in regards to Referential markup. An argument which has been accounted for in Section \ref{sec:referential-markup}. However, the idea is that with some forms of markup, authors can enjoy the luxury of simply ignoring how (e.g.) a character such as ``--'' should be formatted. Instead the author simply employ some referential markup such as \texttt{\&mdash;}. Then, later in a stage or document transformation some decision is made on how to format the entity. GI's can be used in a very similar way. With GI's an author does not merely not have to decide upon formatting, even semantics can be delayed. With GI's an author can choose to distinguish between strings that she suspect are different through simply marking them with syntactically different GI's. There is no forced need to consider the semantics, other than the realization that two things seemingly belong to two different domains. Semantics and presentation can then be added later.

Another subtle point related to the one above, is also that GI's allow authors to mark up their documents with names that cognitively make sense to the individual.

The author of this thesis argue that languages with GI features are, in the ways outlined above, superior to languages without them. The author argue that languages that exhibit GI features are more prepared for handling unforeseen change.



\paragraph{}In the XML specification\footnote{http://www.w3.org/TR/REC-xml/\#sec-logical-struct} Generic Identifiers are defined using EBNF notation as follows\footnote{Some parts of the EBNF example snippet are omitted (such as element attributes), and some altered (such as the letter range) to favor readability.}:

\begin{lstlisting}
element  ::=  EmptyElemTag | STag content ETag
STag     ::=  "<" Name ">"
ETag     ::=  "</" Name ">"
NameChar ::=  NameStartChar | "-" | "." | [0-9] | [A-Za-z] | ...
Name     ::=  NameStartChar (NameChar)*
\end{lstlisting}

The EBNF snippet can informally be described as follows. An element can either produce an empty element tag, or the set of a start tag some content and then an ending tag. A start tag must produce a set of balanced chevrons (i.e. $<>$) in which between a name must be produced. A name can produce a non-whitespace-containing string where some characters are not allowed as the first character. And so forth for the closing of the element.






















% % % % % % % % % % % % % % % % % % % % % % 
%
%
%
%   Theory: Document processing
%
%
%
% % % % % % % % % % % % % % % % % % % % % %


\chapter{Theory part II -- Document processing}
\label{sec:theory-of-document-processing}



\label{sec:microframework}
In the spirit of \citet{reid} -- author of the paper that introduced Scribe, one of the earlier document preparation systems that argued for the ``separation of form from content'' -- we will now take a look at some variations of workflows of document authoring. Further I will suggest an ideal alternative, based on the theoretical knowledge from previous chapters.

The process that we are going to analyze is that of file format transformation. The idea of transforming a file expressed in some format $I$ into some other format $O$.

Given the creativity of humans, I would argue that, the number of workflows used in the real world today is likely to be equivalent to, or higher, than the number of people actually employing a workflow. Thus, in order to be able to discuss workflows, we need some way to generalize these workflows, such that most workflows are captured. Thus, Figure \ref{fig:workflows-framework} suggests a micro-framework that will be used as a basis for this argument. Given its utterly high level of abstraction and simplicity there is little reason to argue its representativity.


\begin{figure}[h]
  \centering

  \begin{tikzpicture}[node distance=2.5cm]
    \node (input)  [io] {$In$};
    \node (engine) [process, right of=input] {$P$};
    \node (output) [io,      right of=engine] {$Out$};

    % arrows
    \draw [arrow] (input)  -- (engine);
    \draw [arrow] (engine) -- (output);

    \begin{pgfonlayer}{background}
      % \node [draw, fill=cyan!25, rectangle, rounded corners, fit={(input) (engine) (output)}] { Foobar };
    \end{pgfonlayer}

  \end{tikzpicture}

  \caption{A naive representation document transformation.}
  \label{fig:workflows-framework}
\end{figure}


\paragraph{The micro-framework stem from the following argument: } To perform the process, previously described as transforming from $I$ to $O$, one can trivially distinguish the three parts of the statement. The pre-state ($I$), the post-state ($O$), and the program ($P$). This is illustrated in Equation \ref{eq:microframework}.

\begin{equation}
  P : \mathbb{I} \rightarrow \mathbb{O}
  \label{eq:microframework}
\end{equation}

Again, as this model simply depicts the transformation as a single joint function there is really no need for arguing its validity. The model simply depicts a function where the domain of all possible inputs is given as input, and the domain of all possible outputs is produced as output. Since the very nature of a markup transformation program is to take a string of some language and produce a string in perhaps another language, then of course all markup transformation programs can be described in this model.


Obviously the transformation program in Figure \ref{fig:workflows-framework} and Equation \ref{eq:microframework} need to be refined (i.e. broken down into further detail) in order to accurately represent most actual transformation programs. This refinement will be carried out further on, and is how the ideal model will be derived.



\section{Determining generality of a transformation program.}
In order to reason about the level of generality of different transformation programs a more visual notation will be introduced. Figure \ref{fig:workflows-framework-spec-in-spec-out} introduces  a dotted line. This dotted line represents specialization (as an opposite to generalization). If a component is depicted inside the dotted area, it suggests that there exist a language within the domain of the component that cannot possibly be present in any other domain of any other component in the figure. In other words, if the input component is depicted inside the dotted line, this means there exist an input language that cannot be produced as output. In less formal terms, this mean that the transformation program is ``coupled''\footnote{Informal usage of the term} to the formats in the domain of the component within the dotted line. The program will always be depicted within the dotted line to indicate that there exist other programs outside the dotted lines. More concrete examples will be given when discussing possible workflows next.



\begin{figure}[h]
  \centering

  \begin{tikzpicture}[node distance=2.5cm]
    \node (input)  [io] {$In$};
    \node (engine) [process, right of=input] {$P$};
    \node (output) [io,      right of=engine] {$Out$};

    \draw [arrow] (input)  -- (engine);
    \draw [arrow] (engine) -- (output);

    \begin{pgfonlayer}{background}
      \node [dotbox, fit={(input) (engine) (output)}]{};
    \end{pgfonlayer}

  \end{tikzpicture}

  \caption{A transformation program that takes specific input formats and produce specific output formats.}
  \label{fig:workflows-framework-spec-in-spec-out}
\end{figure}




\section{Possible workflows}
The intent of this section is to depict all workflows that can be expressed using the simple micro-framework outlined in Section \ref{sec:microframework}. In order to ensure coverage of all possibilities, consider the quadrant diagram depicted in Figure \ref{fig:microframework-quadrant}.

Remember that the terms ``generalized'' and ``specialized'' are used to denote the lack of, or existence (in that order) of items not present in any other domain. To oversimplify -- generalization is considered good and specialization bad. To concretize -- generalization in one end mean that any format can be used in that end. Whereas specialization in one end mean that only one particular set of formats can be used in that end. In the case of the latter, the consequence is that the program $P$ must be rewritten if one desires to make use of a format outside of that set in the specialized end. Consequently this is considered a bad feature because one must write more programs.

\begin{figure}[h]
  \centering

  \def\arraystretch{2.5}
  \begin{tabular}{|r|c|c|}
    \hline

    & \textbf{Generalized out} & \textbf{Specialized out} \\ \hline

    \textbf{Generalized in} &
    \cellcolor[HTML]{33FF99} A
    &
    \cellcolor[HTML]{FFFFCC} C

    \\ \hline

    \textbf{Specialized in} &
    \cellcolor[HTML]{CCFF99} B
    &
    \cellcolor[HTML]{FFFFFF} D

    \\ \hline
  \end{tabular}

  \caption{All possible combinations of specialization/generalization possible in the micro-framework depicted by Figure \ref{fig:workflows-framework}}
  \label{fig:microframework-quadrant}
\end{figure}


In a perfect world, transformation programs would painlessly transform any format to any format and thus exist in  Quadrant A. However, as implied in the beginning of this thesis -- writing a Quadrant A program that is both pragmatically useful and yet flexible is all but trivial \footnote{Since Quadrant A is an interesting quadrant I do of course hope exploration of it will continue.}

Instead, this thesis focus on Quadrant B programs. However, reaching for the highest level of flexibility within the quadrant. With the intent of minimizing the risk of coupling the program to a language on the brink of becoming obsolete.

Before exploring Quadrant B further, examples and explanations of each quadrant will be given below, so as to facilitate a deeper understanding of their meanings. The Quadrants will be discussed in reverse order, for the irrelevant sake of increased suspense.





\subsection{Specialized in, Specialized out (Quadrant D)}
Programs in Quadrant D (of Figure \ref{fig:microframework-quadrant}) represent the category of really specialized transformation programs. They convert formats from one specific domain, to formats from another specific domain. Whether or not these domains are the same is irrelevant. In practice, this essentially mean that if one need to facilitate some format from a different domain, the program need to be rewritten. Visually, these programs are described by Figure \ref{fig:workflows-framework-spec-in-spec-out}. Examples of such a program may for example be a \LaTeX{}-to-PDF converter, or a Markdown-to-HTML converter. 




\subsection{Generalized in, Specialized out (Quadrant C)}
Programs in Quadrant C (of Figure \ref{fig:microframework-quadrant}) represent a, quite odd, set of transformation programs. Namely programs that, in theory, take input from any domain, but always output formats of a specific domain. If one would need to output a format from a different domain, then the program need to be rewritten.

Due to the quite odd nature of these programs, it is difficult to find pramatically useful real-world examples. Perhaps this is because this Quadrant represent a set of programs very different from the intent of this thesis. Very different from the idea of ``writing once and publishing everywhere''.

Nevertheless, such a program might for example be an anything-to-XML converter, that simply slaps a root tag onto whatever string it receives and escape any XML-like syntax from the input string in order to avoid syntax errors. Consequently the program would be able to receive any format, and yet always be able to output valid XML. Pointless, but valid XML. Visually, these programs are described by Figure \ref{fig:workflows-framework-gen-in-spec-out}.

\begin{figure}[h]
  \centering

  \begin{tikzpicture}[node distance=2.5cm]
    \node (input)  [io] {$I$};
    \node (engine) [process, right of=input] {$P$};
    \node (output) [io,      right of=engine] {$O$};

    \draw [arrow] (input)  -- (engine);
    \draw [arrow] (engine) -- (output);

    \begin{pgfonlayer}{background}
      \node [dotbox, fit={(engine) (output)}]{};
    \end{pgfonlayer}

  \end{tikzpicture}

  \caption{Quadrant C programs only suffer from specialized output.}
  \label{fig:workflows-framework-gen-in-spec-out}
\end{figure}



\subsection{Specialized in, Generalized out (Quadrant B)}
Programs in Quadrant B (of Figure \ref{fig:microframework-quadrant}), can be explained through a (to many) very familiar set of tools. Namely XML combined with XSLT. One express a document in some arbitrary XML format and then use XSLT as a means of outputting multiple different other output formats. Visually, these programs are described by Figure \ref{fig:workflows-framework-spec-in-gen-out}.


\begin{figure}[h]
  \centering

  \begin{tikzpicture}[node distance=2.5cm]
    \node (input)  [io] {$I$};
    \node (engine) [process, right of=input] {$P$};
    \node (output) [io,      right of=engine] {$O$};

    \draw [arrow] (input)  -- (engine);
    \draw [arrow] (engine) -- (output);

    \begin{pgfonlayer}{background}
      \node [dotbox, fit={(input) (engine)}]{};
    \end{pgfonlayer}

  \end{tikzpicture}

  \caption{Quadrant B programs only suffer form specialized input.}
  \label{fig:workflows-framework-spec-in-gen-out}
\end{figure}



\subsection{Generalized in, Generalized out (Quadrant A)}
Programs in Quadrant A (of Figure \ref{fig:microframework-quadrant}) represent the most general kind of markup transformation programs. They take formats from the arbitrary\footnote{As you might have already understood, we are of course not talking about completely arbitrary, but rather the ability to through configuration accept (almost) arbitrary formats.} domain, and generate outputs within any domain. An example program that exhibits this behavior is the widely used software Pandoc. Visually, these programs are described by Figure \ref{fig:workflows-framework-gen-in-gen-out}.

\begin{figure}[h]
  \centering

  \begin{tikzpicture}[node distance=2.5cm]
    \node (input)  [io]                       {$In$};
    \node (engine) [process, right of=input]  {$P$};
    \node (output) [io,      right of=engine] {$Out$};

    \draw [arrow] (input)  -- (engine);
    \draw [arrow] (engine) -- (output);

    \begin{pgfonlayer}{background}
      \node [dotbox, fit={(engine)}]{};
    \end{pgfonlayer}

  \end{tikzpicture}

  \caption{Quadrant A programs take general input and produce general output.}
  \label{fig:workflows-framework-gen-in-gen-out}
\end{figure}

% TODO: Should I actually write about this?
% \section{Situational analysis}
% In this section we will criticize some of the common workflows through employing the quadrant approach already introduced and depicted by Figure \ref{fig:microframework-quadrant}. \emph{(TODO) I will probably remove this section, because I assume the reader has already appreciated the costs of working outside of the A quadrant.}
% Why are so many documents still procedural?
% Model information flows of contemporary publishing. Figures describing the current state in contrast to the thesis's subjective idea of the ideal state. Refer to Reid (1980).

% \subsection{Presentational}
% i.e. Microsoft Word etc.
% [FIG. ]
% Explicit outline of the problems so that we can attack them in the hypothetical ideal.

% \subsection{Procedural}
% i.e. \LaTeX etc.
% [FIG. ]
% Explicit outline of the problems so that we can attack them in the hypothetical ideal.

% \subsection{Descriptive}
% i.e. DocBook, EPUB, Markdown etc.
% [FIG. ]
% Explicit outline of the problems so that we can attack them in the hypothetical ideal.

% \subsection{A hypothetical ideal addressing the problems}
% [FIG.]

% \subsubsection{Composition \& Abstraction}
% The minimal denominator.









\section{A three-step model}
\label{sec:three-step-model}
\citet{goldfarb} shed some light onto the issue that descriptive markup is less trivial to parse than procedural. Informally put, to process a procedural document, one simply need to parse the document and carry out each instruction accordingly. But in the case of descriptive markup there is a disconnect between a particular marked up string and its correspondingly appropriate processing functions.

To formalize this disconnect, \citet{goldfarb} suggest a three-step model for document processing. The term document processing here refer to the idea of taking some document containing Generic Identifier's (GI's) as input, applying some set of processing functions, to produce some other document as output. The three steps of \citet{goldfarb} are (1) recognition, (2) mapping, and (3) processing. These will be explained in closer detail below.

\paragraph{Recognition} refer to the idea of recognizing that some piece of the document string should be considered an element that is marked up. While not explicitly expressed by \citet{goldfarb} it seems reasonable to informally consider this step to be the step of lexing (tokenization) and parsing, where its output would to be the parse tree. \citet{goldfarb} gives the example of an element with the generic identifier ``footnote''.

\paragraph{Mapping} refer to the idea of pairing up the correct set of processing functions with the correct document elements (or to follow the analogy of the last paragraph -- nodes in the parse tree). \citet{goldfarb} continue on the previous footnote example by saying that the element now perhaps would be paired up with a processing function that collects the content of the footnote, only to print it later at the bottom of the page.

\paragraph{Processing} trivially refer to the task of executing the processes on their respectively mapped elements.


\subsection{Arguing the case of the three-step model}
While it is splendid that \citet{goldfarb} chose to make this model explicit, it is also so trivial that it is very generally applicable. \citet{goldfarb} self give an example (similar to the following one) of low-level formatting in word-processors. The kind of low-level formatting, that \citet{coombs} would refer to as Punctuational markup. To the example. Consider how common word-processors handle this kind of low-level punctuation. Consider how when one sets the horizontal paragraph alignment option, in common word processors today, to ``justify'', space may vary between words depending on how many characters one choose to type in a row. This is an example similar to the one given by \citet{goldfarb}. Consider how the word-processors need to first (1) recognize the spaces, then (2) find the appropriate processing function for spaces, i.e. the one that will make sure the words of a line are evenly distributed over the line, and then (3) actually execute that function.

Considering tools such as XSLT it is easy to see how this model fits. XSLT is basically a language where these three steps become utterly explicit. In XSLT one uses XPath to perform step one. Then define some sort of XSLT transformation function that will be the function performed upon recognizing the defined XPath. Then finally one actually execute these processing functions, presumably via some common implementation of the standard\footnote{http://en.wikipedia.org/wiki/XSLT\#Processor\_implementations} such as Saxon XSLT\footnote{http://en.wikipedia.org/wiki/Saxon\_XSLT}.

Finally it is important to understand that while this three-step model indeed is simplistic, it is also a fundamental description of descriptive markup languages. This three-step process is part of what allows authors to express documents completely descriptively.





\section{Derived text}
\label{sec:theory:derived-text}
What \citet{goldfarb} refers to as Mapping (discussed in previous Section \ref{sec:three-step-model}) can be likened to the concept of Derived Text, that \citet{reid} discuss in the paper introducing the document processing language Scribe. \citet{reid} exhibit an understanding of the fact that much content in authored documents actually can be considered to be derivations of other parts of the same document. \citet{reid} give examples such as tables of contents, indexes, and glossaries.

Scribe actually provided a facility for dealing with derived text. \citet{reid} explain the mechanism by pointing out that any part of the text can be ``saved'' during the processing of a manuscript only to later be processed itself as regular manuscript text at certain ``collection points''.

Dealing with derived text of course become technically trivial in procedural (in terms of procedurality described in Section \ref{sec:taxonomy}) languages that exhibit language features such as variables.


\subsection{Derived text beyond a document format}
While it of course is nifty that a markup language (and it's compiler) have a built in mechanism for deriving text, it is not all positive. As highlighted in a much more recent paper by \citet{krijnen}, it becomes very hard to share derivation strategies between multiple formats. Consider for example the table of contents (TOC), apart from formatting, the underlying idea of generating a TOC is essentially the same, regardless of the markup language in question. While there may of course be variations from format to format, the general intent is very much the same.

\citet{krijnen} extend the commonly used universal document converter Pandoc\footnotePandoc, partially with the intent of making derived text\footnote{\citet{krijnen} does not refer to the concept as derived text, but refer to the same concept at heart.} sharable between readers and writers in the Pandoc system. Pandoc (as previously mentioned) is an ``n-to-n'' document converter that utilizes readers to read documents, and writers to write documents. In between these lie the universal Pandoc format. Anyone can contribute with readers or writers to the Pandoc system. \citet{krijnen} propose a solution where one writes small Haskell (the same language as Pandoc is written in) modules in order to derive text and create (e.g.) numbered headings, TOC's etc. These modules can then be reused in other writers.





















% % % % % % % % % % % % % % % % % % % % % % 
%
%
%
%   LANGUAGE REQUIREMENTS
%
%
%
% % % % % % % % % % % % % % % % % % % % % %


\chapter{Deriving markup language requirements}
\label{chpt:derived-language-requirements}
In Section \ref{sec:goals-for-the-transformation-program} requirements for a transformation program were set out. In this section, requirements for the languages supported by such a transformation program will be outlined. The requirements introduced by this chapter are strictly derived from important realizations in Section \ref{sec:theory}.


\section{Descriptive}
The hierarchy of abstraction, outlined in Section \ref{sec:abstraction-hierarchy}, shows that it is desirable (for manual work) to author documents in the ``top of the pyramid''. I.e. in descriptive languages. Thus, the requirement derived will be that all languages (assuming there might be more than one) an author ``comes in contact'' with must be descriptive.

Understand that all humans using end-tools in the eco-system should be deemed authors. In Section \ref{sec:goals-for-the-transformation-program} the idea of packages to enable reuse is discussed. Also package authors must be considered authors. Thus also packages must be expressed descriptively. Subsequently this indicate the need for a template transformation approach rather than imperative processing.



\section{Nestable}
As accounted for in Section \ref{sec:nesting}, the ability to nest elements within elements is a fundamental feature of markup languages, and increase the level of pragmatic usability drastically. By reducing the number of base tokens needed to be designed by the language designer.

Due to the lack of consensus in the community (as described in Section \ref{sec:non-hierarchical-nesting}) on sensible approaches, no attempt is made in this thesis to breach the borders of hierarchical document descriptions into the land of more general nesting. However, as nesting, as shown, is a pragmatically very useful feature of a language, the requirement for the eco-system of tools produced in this thesis is then that the input language of the eco-system be hierarchical\footnote{I encourage future research to breach this painstaking border, but also to appreciate that the measures suggested in this thesis, likely not only still apply beyond the breaching of the hierarchical border, but also are equally significant.}.



\section{Mixed content}
As accounted for in Section \ref{sec:mixed-content}, the idea of Mixed Content, is not merely a stylistic variation that might be exhibited in some documents, but missing in others. Mixed Content is a fundamental and necessary feature of language, for that language to enable marking up of documents.

The requirement is thus, that the input language of this eco-system of tools, must support mixed content.



\section{Generic Identifiers}
As accounted for in Section \ref{sec:arguing-the-case-of-generic-identifiers}, Generic Identifiers, are desirable for multiple reasons. For ``future-proofing'' a document, and a language, as well as allowing authors to think and express their content freely, without concerns of formatting or exact semantics. With GI's the author need simply to identify whether two elements are different or alike, and then make decisions on semantics and formatting later on.

While the GI's of (e.g.) XML are indeed very powerful. The author of this paper suggest that Generic Identifiers could be even more generic. If the document author is given some facility for defining her GI's then there is really no need for the mandatory chevrons. Or to put it more generally, a markup language should be able to provide the facility for defining GI's without forcing the GI's to contain any particular syntactical tokens. Call this property of a language \emph{Generic Syntax}. That is the requirement for the language in this thesis.





















% % % % % % % % % % % % % % % % % % % % % % 
%
%
%
%   TRANSFORMATION PROGRAM REQUIREMENTS
%
%
%
% % % % % % % % % % % % % % % % % % % % % %




\chapter{Deriving markup transformation requirements}
\label{chpt:derived-transformation-requirements}
In Section \ref{sec:goals-for-the-transformation-program} a number of high-level goals for ideal markup language transformation programs were outlined. This section will build upon the theory outlined in both Section \ref{sec:theory} as well as \ref{sec:theory-of-document-processing}. Using this body of knowledge as stepping stone, this chapter will formulate a set of requirements for an ideal markup transformation program. The high-level goals outlined in Section \ref{sec:goals-for-the-transformation-program} will now be discussed one by one.

\paragraph{The ability to produce any output format from any reasonable input format.} While it was never explicitly stated what a reasonable input format is, the wording makes it clear that it does not reference the domain of all possible input formats. Consequently, Section \ref{sec:microframework} determines that it is pragmatic to attempt to build Quadrant B programs rather than Quadrant A programs. According to the constructed micro-framework, Quadrant B programs fit perfectly with this high-level goal. Quadrant A programs are deemed too complex to be pragmatically useful\footnote{At least for now.}, and Quadrant C and D programs are deemed to be too useless for markup transformation.

\paragraph{Composable.} The second high-level goal was for the program to be built of composable compontents. Informally put, this refers to the idea of packages. This goal will be addressed in this chapter.

\paragraph{Extendable.} The third high-level goal referred to the idea that the ideal transformation program would not merely be composable. It would be so composable that it is possible to extend it. This goal will also be addressed in this chapter.



\section{Separating intents}
The high-level goals outlined in \ref{sec:goals-for-the-transformation-program} have been used as a guide when navigating the theory from Sections \ref{sec:theory} and \ref{sec:theory-of-document-processing}. The argument is, that if composability is to be achieved, one must identify all steps of the transformation process that can be isolated into components. Below follows such a suggestion of an ideal separation of transformation intents.


\subsection{Syntax}
As made clear in Section \ref{sec:tag-validity-type-validity}, there exist a distinction in intent between validating the syntactical correctness and the semantic correctness of a document. A compiler that lex and parse a particular input is concerned with syntactical correctness, whereas checking whether a document conforms to a (e.g.) DTD, is a concern of semantic correctness.

Thus, one intent of transformation is the first mentioned -- syntactical correctness. In terms of a compiler: lexing and parsing.

\subsection{Validations}
Building upon the ideas just mentioned in the previous section -- that syntactic validity and semantic validity, are not the same. If syntactic validity is to be considered a separate intent, then of course, the second intent of markup transformation would be semantic validity.

\subsection{Derivations}
As exhibited in Section \ref{sec:theory:derived-text}, derived text is not only a powerful part of a markup language compiler, but it is also a dangerous one. Dangerous in the sense that it may lessen re-usability if implemented at the wrong level of abstraction. This is made especially clear in the areas of critique that \citet{krijnen} deliver towards Pandoc.

The fact that derived text is a very natural part of markup languages, and have been around for a long time, is clearly exhibited by the implementation of it by \citet{reid} in the language Scribe.

Consequently, derivations are important and necessary, but must be implemented at the right level of abstraction. Subsequently, it is reasonable to assume that derivations should be considered a separate intent.

\subsection{Presentations}
The perhaps most obviously distinct intent of markup transformation is that of presentation. In Section \ref{sec:taxonomy} it was declared that even in the early days of Scribe, by \citet{reid}, it was conceived that presentation ought to be separated from content. The taxonomy introduced by \citet{coombs} clearly suggest that presentation can be separated from content. The very nature of XSLT\footnote{http://www.w3.org/TR/xslt} cries out that documents can be transformed into other documents with the intent of describing the document contents rather than its presentation. Pandoc is a case in point on how the information contained in a document can exist between the boundries of two presentational formats.

Consequently, another distinct intent is the transformation from some format to a presentational format.




\section{Using configurations to map processing functions}
\label{sec:applying-three-step-model-to-quad-b}
In order to truly adhere to the second and third high-level goals (composability, and extendability), one sensible, and obvious, approach would be to allow transformations to be configured. Consider XSLT as en example. Assume one were to construct a package system (such as the package system of \LaTeX) or (perhaps even) a public package channel\footnote{A ``package channel'' refer here to the idea of a unified publicly accessible package system, such as e.g. \texttt{apt-get}, \texttt{RubyGems} or \texttt{npm}}. Consider now that authors are free to choose element names freely in XML. This mean it would either be utterly difficult for a package creator to assume the right names for some set of elements when constructing the package, or it would spawn a situation where document authors would have to constantly adapt to the names used in packages. This of course violates the second high-level goal as it makes the package-eco-system significantly less composable.

One sensible alternative would be to apply the three-step approach to document conversion, proposed by \citet{goldfarb}. Applying this three-step process allow different intents to be left to different parts of the eco-system. Step one and two (recognition and mapping) would be carried out by the individual that aim to \emph{use} the package. Where the second step would allow the user to map the names of her elements onto the names of the elements in the ``processing function'' (PF) (i.e. the package). The package creator would thus create, only the PF. It is important of course that the PF would have to be created in a way such that it allows arguments to be passed, and thus mapped onto the variable element names within the PF.  Step three would be carried out by the transformation program itself.

For the sake of standardization it would of course, and again, proper composability, it would of course make sense to standardize the way that a PF exposes its arguments.

Figure \ref{fig:quad-b-with-three-step-process} depict the Quadrant B model refined with the property of this three-step process.

\begin{figure}[h]
  \centering

  \begin{tikzpicture}[node distance=2.5cm]
    \node (input)  [io] {$I$};
    \node (P)      [process, right of=input] {$P$};
    \node (output) [io,      right of=P] {$O$};

    \node (C)  [ds,      above of=P, xshift=-1cm, yshift=-1cm] {\(C\)};  % {Configuration};
    \node (F)  [ds,      above of=P, xshift=1cm,  yshift=-1cm] {\(PF\)}; % {Functions};

    \draw [arrow] (input)  -- (P);
    \draw [arrow] (P) -- (output);
    \draw [arrow] (C) -- (P);
    \draw [arrow] (F) -- (P);
  \end{tikzpicture}

  \caption{Quadrant B model refined with properties of the three-step model for document processing, by \citet{goldfarb}.}
  \label{fig:quad-b-with-three-step-process}
\end{figure}

































% % % % % % % % % % % % % % % % % % % % % % % % % % 
%
%
%
%   PROPOSING AN IDEAL ABSTRACT LANGUAGE
%
%
%
% % % % % % % % % % % % % % % % % % % % % % % % % % 


\chapter{Proposing a minimal abstract language}
\label{chpt:proposal-language}
This section will propose a minimal markup language that is intended to be used as input to the second deliverable of this thesis -- the transformation workflow model depicted further on in Section \ref{chpt:proposal-transformation-workflow}. The language is intended to be as minimalistic as possible, while fulfilling the requirements identified in Section \ref{chpt:derived-language-requirements}.

Any useful complexity discussed in the theory parts of this thesis will be shifted off into the transformation program, or be argued to belong in shared packages for the transformation program. Below, the requirements from Section \ref{chpt:derived-language-requirements} are repeated and explained.


\paragraph{Descriptive.} The proposed markup language(s) must be descriptive and descriptive only. Following the definition of descriptive as defined in Sections \ref{sec:taxonomy} and \ref{sec:abstraction-hierarchy}.


\paragraph{Nestable.} The proposed language(s) must support hierarchical nesting into arbitrary depth.


\paragraph{Mixed Content.} The proposed language(s) must support and be suitable for Mixed Content. Following the formal definition of suitability defined in Section \ref{sec:mixed-content}.


\paragraph{Generic Syntax.} The proposed language(s) must support the use of generic syntax and contain no forced syntactical keywords. In the case of multiple languages being suggested, only the language used as input to the transformation program must exhibit this property.




\section{A DSL for generating markup-oriented PEG}
In order to fulfill the requirement of Generic Syntax, this thesis propose the usage of Formal Grammars to express languages specific for the particular document of the moment. Asking semi-technicals (whom authors very well may be) to define languages using Formal Grammars such as a Parsing Expression Grammar (PEG) or an Extended Backus-Naur Form (EBNF) is however of course not only absurd, but also time-consuming and error prone. This thesis argue the need for significantly simpler languages that can be used to express descriptive markup languages.


Thus, the suggestion is to create a tiny Domain-Specific Language (DSL), and a tiny conversion program that converts this tiny language into actual PEG syntax. Of course, the idea is that the actual PEG should be abstracted away from the user. Thus, the ignorant user would not know there is a Parsing Expression Grammar being generated in the background. To achieve this, the minor conversion program will be included in the transformation workflow described further on in Section \ref{chpt:proposal-transformation-workflow}. 

The intent of this smaller language is of course to limit the number of expressible languages in PEG such that all valid inputs in the language will when run through the program produce a PEG that parses suitable markup formats. Formats that also adhere to the language requirements outlined previously in this chapter.

While there exist many solutions that would satisfy the above description, one will partially be supplied. This is merely in order to illustrate how the undertaking is indeed realistic.

Figure \ref{code:fupd-peg} illustrate the syntax of a very simple implementation of a DSL aimed towards producing new Parsing Expression Grammars, as discussed above.

The Parsing Expression Grammar outlined in Figure \ref{code:fupd-peg} would produce a language with a syntax in which Figure \ref{code:fupd-example-usage-syntax} would express valid code.


\begin{figure}[h]
\begin{lstlisting}
start     = (rule line?)+
rule      = name spaces ":" spaces pattern
pattern   = symbol+ "%" symbol+
name      = CHAR+
symbol    = SYMBOL / CHAR

line      = "\n" / "\r\n" / "\r"
spaces    = [ ]*

CHAR      = [A-Za-z]
SYMBOL    = [/*_\[\]]    /* and so forth... */
\end{lstlisting}
\caption{A PEG for a program intended to produce new PEG's.}
\label{code:fupd-peg}
\end{figure}


\begin{figure}[h]
\begin{lstlisting}
bold   :  **%**
quote  :  [q]%[/q]
\end{lstlisting}
\caption{Example of an author defining her own syntax, where this syntax is valid according to the grammar in Figure \ref{code:fupd-peg}}
\label{code:fupd-example-usage-syntax}
\end{figure}


Assuming that a document author would author a syntax definition as the definition depicted in Figure \ref{code:fupd-example-usage-syntax}, the actually generated Parsing Expression Grammar that would be used to parse the actual documents would look something like the PEG illustrated in Figure \ref{code:fupd-example-generated-peg}.


Assume that an author interacting with the program in question, had successfully generated the Parsing Expression Grammar depicted in Figure \ref{code:fupd-example-generated-peg}, simply by giving the code in Figure \ref{code:fupd-example-usage-syntax} as input. Then the author would have this newly generated syntax available for use. In this particular example, the syntax expressed would allow the document depicted in FIG to be a valid document. For the sake of easing readability, consider the XML interpretation of the same document depicted in FIG.

\begin{figure}[h]
\centering
\fbox{
\texttt{
He rose and spoke. [q] The only way left is **up**. [/q]
}}
\caption{Utilizing the generated grammar depicted in Figure \ref{code:fupd-example-generated-peg}}
\label{code:fup-example-markup}
\end{figure}



\begin{figure}[h]
\begin{lstlisting}
/*
 *   static, same for all input files
 */

TEXT     = [A-Za-z]

start    = (expr)*;
expr     = text / element / simple
explicit = explicit_quote
implicit = implicit_bold


/*
 *   generated, on an input per input basis
 */

T_BOLD   = "**"
S_QUOTE  = "[q]"
E_QUOTE  = "[/q]"

explicit_quote = SQUOTE (expr)* E_QUOTE
implicit_bold  = TBOLD  (expr)* T_BOLD
\end{lstlisting}
\caption{A Parsing Expression Grammar automatically generated by providing the code of Figure \ref{code:fupd-example-usage-syntax} to a program defined by the grammar depicted in \ref{code:fupd-peg}. The actual program code is omitted.}
\label{code:fupd-example-generated-peg}
\end{figure}



While this section contained a lot of code it is important to understand that the only two files that would be manually authored by a human are the two shortest ones. The first step of the two would be for the author to write her syntax definition. This code is depicted in Figure \ref{code:fupd-example-usage-syntax}. Concise, readable and declarative.

The second step would be for the author to actually type out the document in which she wants to use this syntax. This step is depicted in Figure \ref{code:fup-example-markup}. Again, concise, readable and declarative. However, since this syntax was actually defined by the author self, it will in reality be just as verbose or compact as the author wish.














% % % % % % % % % % % % % % % % % % % % % % % % % % 
%
%
%
%   PROPOSING AN IDEAL TRANSFORMATION WORKFLOW
%
%
%
% % % % % % % % % % % % % % % % % % % % % % % % % % 




\chapter{Proposing an ideal workflow}
\label{chpt:proposal-transformation-workflow}
As mentioned earlier in this chapter, the quadrant view is an oversimplification and of course there are many interesting nuances within one quadrant. This section will refine Quadrant B, using the requirements gathered in Sections \ref{chpt:derived-language-requirements} and \ref{chpt:derived-transformation-requirements}, as guidelines, and through the collected requirements reach a supported proposal for an ideal workflow.

\paragraph{The model of Quadrant B} (Figure \ref{fig:workflows-framework-spec-in-gen-out}) will be used as a starting point due to the arguments given in Section \ref{chpt:derived-transformation-requirements}. In summary, it is the one of the four quadrant models that fit the initial high-level goals outlined in Section \ref{sec:goals-for-the-transformation-program}.



\section{Applying the three-step model}
The first step will be to refine the model as depicted in Figure \ref{fig:quad-b-with-three-step-process} because of the arguments proposed in the transformation requirements Section \ref{sec:applying-three-step-model-to-quad-b}. The model will now allow a greater level of package re-use.


\section{Splitting processing according to intent}
The next step will be to refine the model according to the different intents of transformations identified in Section \ref{chpt:derived-transformation-requirements}. This refinement is depicted in Figure \ref{fig:ideal-only-different-intents}.

\begin{figure}[h]
  \centering

  \begin{tikzpicture}[node distance=.15cm]
    \node (in)  [io]                                 {\(In\)};
    \node (P1)  [process, right of=in, xshift=2.5cm] {\(P1\)};
    \node (P2)  [process, right of=P1, xshift=2.5cm] {\(P2\)};
    \node (P3)  [process, right of=P2, xshift=2.5cm] {\(P3\)};
    \node (P4)  [process, right of=P3, xshift=2.5cm] {\(P4\)};
    \node (out) [io,      right of=P4, xshift=2.5cm] {\(Out\)};

    \draw [arrow] (in) -- (P1);
    \draw [arrow] (P1) -- (P2);
    \draw [arrow] (P2) -- (P3);
    \draw [arrow] (P3) -- (P4);
    \draw [arrow] (P4) -- (out);
    \draw [arrow] (P1) edge[loop below]();
    \draw [arrow] (P2) edge[loop below]();
    \draw [arrow] (P3) edge[loop below]();
    \draw [arrow] (P4) edge[loop below]();
  \end{tikzpicture}

  \caption{Suggested refinement where each type of transformation intent is modeled as a separate process.}
  \label{fig:ideal-only-different-intents}
\end{figure}

Understand, that in Figure \ref{fig:ideal-only-different-intents}, the previously all-encompassing program $P$, has now been refined into its subparts $P1$ to $P4$. Where table in Figure \ref{tbl:ideal-only-different-intents-legend} explain the mapping of intents of the subprograms.


\begin{figure}[h]
\begin{tabular}{l l l}
Process & Intent & Description
\\ \hline

$P1$ & Syntax & Lexing and parsing of the document to create the AST.
\\

$P2$ & Validations & Validates the semantics of the document.
\\

$P3$ & Derivations & Packages that enable the document to utilize derived text.
\\

$P4$ & Presentations & Converts the document into presentation oriented formats.
\\
\end{tabular}
\caption{Legend of the subprograms with different intents depicted in Figure \ref{fig:ideal-only-different-intents}}
\label{tbl:ideal-only-different-intents-legend}
\end{figure}


\subsection{Looping the intent-delimited subprograms}
\label{sec:ideal-looping-subprograms}
Notice, that each sub-program in Figure \ref{fig:ideal-only-different-intents} has two choices of ``moving forward''. Either it can re-apply itself again, or it can move on to the next intent. This simple feature exist in order to further satisfy the second high-level goal -- composability. Since transformation at a particular level of intent can be applied any number of times, this means any number of packages can be applied to the document before moving on to the next intent. Naturally this stems from the fact that one might, e.g., not only want to inject the derived text for a table of contents, but also perhaps for a bibliography and a figure list. Since any number of intent-delimited transformations may be run, an author can decide to run any number of transformations before moving on to the next intent.



\section{Re-applying the three-step model to each step}
The observant reader may have noticed that the configuration file and the processing functions (PF) file that were depicted in Figure \ref{fig:quad-b-with-three-step-process} went missing when moving to Figure \ref{fig:ideal-only-different-intents}. This was a deliberate move, so that the decisions on how to refine the PF's would be obvious.

The question is of course whether the Configuration file and PF that previously was applied to the whole single program $P$ now should be applied to one, some or all of the sub-programs. The author of this paper argue all, and the motivation is simple. Because of the looping of intent-delimited sub-programs (as suggested in the previous Section \ref{sec:ideal-looping-subprograms}) the previously discussed (in Section \ref{sec:applying-three-step-model-to-quad-b}) problems of blindly applying packages (sub-programs) to the document, would re-appear if the document author would not have the chance to specify a mapping file. I.e. mapping in the sense of the three-step-model by \citet{goldfarb} explained in detail in Sections \ref{sec:three-step-model} and \ref{sec:applying-three-step-model-to-quad-b}. The document author that initiates the transformation must be allowed to specify a mapping file for each transformation package that will run on the document.

This refinement is depicted in Figure \ref{fig:ideal-different-intents-with-configs}. Where all of the $C$-based data stores represent mapping-files that allow the externalized processing functions (Illustrated as $F$-based data stores) to be ``mapped'' onto the current document.

\begin{figure}[h]
  \centering

  \begin{tikzpicture}[node distance=.15cm]
    \node (in)  [io]                                 {$In$};
    \node (P1)  [process, right of=in, xshift=2.5cm] {$P1$};
    \node (P2)  [process, right of=P1, xshift=2.5cm] {$P2$};
    \node (P3)  [process, right of=P2, xshift=2.5cm] {$P3$};
    \node (P4)  [process, right of=P3, xshift=2.5cm] {$P4$};
    \node (out) [io,      right of=P4, xshift=2.5cm] {$Out$};

    \node (C1)  [ds,      above of=P1, yshift=2cm, xshift=-.6cm] {$C1$};
    \node (F1)  [ds,      above of=P1, yshift=2cm, xshift= .6cm] {$F1$};

    \node (C2)  [ds,      above of=P2, yshift=2cm, xshift=-.6cm] {$C2$};
    \node (F2)  [ds,      above of=P2, yshift=2cm, xshift= .6cm] {$F2$};

    \node (C3)  [ds,      above of=P3, yshift=2cm, xshift=-.6cm] {$C3$};
    \node (F3)  [ds,      above of=P3, yshift=2cm, xshift= .6cm] {$F2$};

    \node (C4)  [ds,      above of=P4, yshift=2cm, xshift=-.6cm] {$C4$};
    \node (F4)  [ds,      above of=P4, yshift=2cm, xshift= .6cm] {$F2$};

    \draw [arrow] (in) -- (P1);
    \draw [arrow] (P1) -- (P2);
    \draw [arrow] (P2) -- (P3);
    \draw [arrow] (P3) -- (P4);
    \draw [arrow] (P4) -- (out);

    \draw [arrow] (P1) edge[loop below]();
    \draw [arrow] (P2) edge[loop below]();
    \draw [arrow] (P3) edge[loop below]();
    \draw [arrow] (P4) edge[loop below]();

    \draw [arrow] (C1) -- (P1);
    \draw [arrow] (C2) -- (P2);
    \draw [arrow] (C3) -- (P3);
    \draw [arrow] (C4) -- (P4);

    \draw [arrow] (F1) -- (P1);
    \draw [arrow] (F2) -- (P2);
    \draw [arrow] (F3) -- (P3);
    \draw [arrow] (F4) -- (P4);

  \end{tikzpicture}

  \caption{Suggested refinement where the processing functions of each intent-delimited transformation is externalized to another file (e.g. a package) and it's mapping on to the document may be controlled using a mapping file.}
  \label{fig:ideal-different-intents-with-configs}
\end{figure}






\section{Relating the proposed metalanguage}
Understand, that the first transformation ($P1$) in the models of this section, refer to the program that was briefly glanced over in Section \ref{chpt:proposal-language}. Subsequently, the minimal abstract language defined and suggested in Section \ref{chpt:proposal-language} can actually be mapped on to Figure \ref{fig:ideal-different-intents-with-configs}. The document definition (i.e. the meta-language) is depicted as $C1$. Whereas the actual document, expressed using the syntax defined by the meta-language obviously is the $Input$ file.

As one of the requirements for this transformation workflow is to endorse composability, $F1$ is a product of allowing packages even for syntax. In other words, an author may use her own custom defined syntax, but may however also or instead choose to apply the syntax of some other package. Thus, when utilizing syntax packages, $F1$ represent grammar packages and $C1$ the configuration file for customizing that grammar package to map in a way the author feel content with.





\section{An orchestrator file}
Given the current refinements of the ideal model (last version depicted in Figure \ref{fig:ideal-different-intents-with-configs}), one must realize that an execution of this model may be significantly complex. A potential ``run'' through this model might for example be explained as follows:

$$
In \to
P1\prime \to
C1\prime \to
F1\prime \to
P1\prime\prime \to
C1\prime\prime \to
F1\prime\prime \to
P1\prime\prime\prime \to
P2\prime \to
..... \to Out
$$

Thus, the document author initiating the transformation would essentially need some way of orchestrating this run. While it is of course possible to orchestrate such a thing from (e.g.) a command line using flags, the author of this paper would argue the need for an orchestration file expressed in descriptive markup.



\begin{figure}[h]
  \centering

  \begin{tikzpicture}[node distance=.1cm]
    \node (P0)  [process]                            {$P0$};

    \node (in)  [io, above of=in, yshift=2cm, xshift=-1cm]     {$In$};

    \node (P1)  [process, right of=P0, xshift=2.5cm] {$P1$};
    \node (P2)  [process, right of=P1, xshift=2.5cm] {$P2$};
    \node (P3)  [process, right of=P2, xshift=2.5cm] {$P3$};
    \node (P4)  [process, right of=P3, xshift=2.5cm] {$P4$};
    \node (out) [io,      right of=P4, xshift=2.5cm] {$Out$};

    \node (C0)  [ds,      above of=P0, yshift=2cm, xshift=.6cm] {$C0$};

    \node (C1)  [ds,      above of=P1, yshift=2cm, xshift=-.6cm] {$C1$};
    \node (F1)  [ds,      above of=P1, yshift=2cm, xshift= .6cm] {$F1$};

    \node (C2)  [ds,      above of=P2, yshift=2cm, xshift=-.6cm] {$C2$};
    \node (F2)  [ds,      above of=P2, yshift=2cm, xshift= .6cm] {$F2$};

    \node (C3)  [ds,      above of=P3, yshift=2cm, xshift=-.6cm] {$C3$};
    \node (F3)  [ds,      above of=P3, yshift=2cm, xshift= .6cm] {$F2$};

    \node (C4)  [ds,      above of=P4, yshift=2cm, xshift=-.6cm] {$C4$};
    \node (F4)  [ds,      above of=P4, yshift=2cm, xshift= .6cm] {$F2$};

    \draw [arrow] (in) -- (P0);

    \draw [arrow] (P0) -- (P1);
    \draw [arrow] (P1) -- (P2);
    \draw [arrow] (P2) -- (P3);
    \draw [arrow] (P3) -- (P4);
    \draw [arrow] (P4) -- (out);

    \draw [arrow] (P1) edge[loop below]();
    \draw [arrow] (P2) edge[loop below]();
    \draw [arrow] (P3) edge[loop below]();
    \draw [arrow] (P4) edge[loop below]();

    \draw [arrow] (C0) -- (P0);
    \draw [arrow] (C1) -- (P1);
    \draw [arrow] (C2) -- (P2);
    \draw [arrow] (C3) -- (P3);
    \draw [arrow] (C4) -- (P4);

    \draw [arrow] (F1) -- (P1);
    \draw [arrow] (F2) -- (P2);
    \draw [arrow] (F3) -- (P3);
    \draw [arrow] (F4) -- (P4);

  \end{tikzpicture}

  \caption{Suggested refinement where C0 is an orchestrating configuration file that declare all the transformations to be performed, and thus the ``path through'' the rest of the model.}
  \label{fig:ideal-with-config}
\end{figure}





\section{An ideal workflow}
Having performed all of the refinements outlined in this chapter, ending up with the model represented by Figure \ref{fig:ideal-different-intents-with-configs}, now satisfy all of the requirements derived from theory (as outlined in Section \ref{chpt:derived-transformation-requirements}). However, the intent was of course also for the model to adhere to all of the high-level goals outlined as proposed form the start -- outlined in Section \ref{sec:goals-for-the-transformation-program}. The author of this paper would argue that they are all fulfilled but in order to really achieve extendability there is one more strategy one may apply. This strategy stem from the realization that because of the high level of composability, there isn't really any need for the same program (the previously referred $P$) to run all of these sub-programs.

Instead, each sub-program could delegate to a completely different sub-program that could very well be written in a different language, and that could very well accept mapping-files and processing functions in a different language. The only really ``coupled'' parts are the inputs, outputs, and the format of the intermediate string passed between each sub-program.

While the author of this paper would suggest it sensible to initially build one eco-system of tools that share the same language, the possibility is important from a perspective of being future-proof. If all of the sub-programs may delegate, then a very high level of extendability is achieved, and the root program becomes not much more than an orchestrator of pipes through other programs.



\begin{figure}[h]
  \centering

  \begin{tikzpicture}[node distance=.1cm]
    \node (P0)  [process] {$P0$};
    \node (in)  [io,      above of=P0, yshift=2cm, xshift=-1.25cm]                                 {$In$};
    \node (P1)  [process, right of=P0, xshift=2.5cm] {$P1$};
    \node (P2)  [process, right of=P1, xshift=2.5cm] {$P2$};
    \node (P3)  [process, right of=P2, xshift=2.5cm] {$P3$};
    \node (P4)  [process, right of=P3, xshift=2.5cm] {$P4$};
    \node (out) [io,      right of=P4, xshift=2.5cm] {$Out$};

    \node (T1)  [process, above of=P1, yshift=2cm] {$T1$};
    \node (T2)  [process, above of=P2, yshift=2cm] {$T2$};
    \node (T3)  [process, above of=P3, yshift=2cm] {$T3$};
    \node (T4)  [process, above of=P4, yshift=2cm] {$T4$};

    \node (C0)  [ds,      above of=P0, yshift=2cm, xshift= .5cm] {$C0$};
    \node (C1)  [ds,      above of=T1, yshift=2cm, xshift=-.6cm] {$C1$};
    \node (C2)  [ds,      above of=T2, yshift=2cm, xshift=-.6cm] {$C2$};
    \node (C3)  [ds,      above of=T3, yshift=2cm, xshift=-.6cm] {$C3$};
    \node (C4)  [ds,      above of=T4, yshift=2cm, xshift=-.6cm] {$C4$};

    \node (F1)  [ds,      above of=T1, yshift=2cm, xshift= .6cm] {$F1$};
    \node (F2)  [ds,      above of=T2, yshift=2cm, xshift= .6cm] {$F2$};
    \node (F3)  [ds,      above of=T3, yshift=2cm, xshift= .6cm] {$F2$};
    \node (F4)  [ds,      above of=T4, yshift=2cm, xshift= .6cm] {$F2$};

    \draw [arrow] (in) -- (P0);
    \draw [arrow] (P0) -- (P1);
    \draw [arrow] (P1) -- (P2);
    \draw [arrow] (P2) -- (P3);
    \draw [arrow] (P3) -- (P4);
    \draw [arrow] (P4) -- (out);

    \draw [arrow] (C0) -- (P0);
    \draw [arrow] (C1) -- (T1);
    \draw [arrow] (C2) -- (T2);
    \draw [arrow] (C3) -- (T3);
    \draw [arrow] (C4) -- (T4);

    \draw [arrow] (F1) -- (T1);
    \draw [arrow] (F2) -- (T2);
    \draw [arrow] (F3) -- (T3);
    \draw [arrow] (F4) -- (T4);

    \draw [arrow] (P1) to[out=-225,in=225] (T1);
    \draw [arrow] (P2) to[out=-225,in=225] (T2);
    \draw [arrow] (P3) to[out=-225,in=225] (T3);
    \draw [arrow] (P4) to[out=-225,in=225] (T4);

    \draw [arrow] (T1) to[out=-45, in=45]  (P1);
    \draw [arrow] (T2) to[out=-45, in=45]  (P2);
    \draw [arrow] (T3) to[out=-45, in=45]  (P3);
    \draw [arrow] (T4) to[out=-45, in=45]  (P4);
  \end{tikzpicture}

  \caption{Suggested ideal model of markup language transformations based on all theory gathered from the bottom up in this thesis.}
  \label{fig:ideal-final}
\end{figure}
























% % % % % % % % % % % % % 
%
%
%
%   CONCLUSION
%
%
%
% % % % % % % % % % % % % 

\chapter{Conclusion}
This thesis set out with the intent of reaching two deliverables. Both of which have been produced.

\begin{enumerate}
\item A suggestion for a minimal markup language with no default syntax,
\item A model depicting an ideal workflow for markup transformations.
\end{enumerate}

\paragraph{Regarding the first deliverable,} it is indeed not a full specification for a language, but some strategic extracts depicting the lexing, parsing and usage of such a language family. The extracts underline the actual realizability of such a language. Even though much is left before such a language can be considered sound, it is apparent there exist no obvious barriers to as why such a language should not be able to be constructed.

\paragraph{Regarding the second deliverable,} the model is considered to be close to complete. The model has strong roots in the theory outlined in this thesis, and considering how significant the papers, the standards, the languages and the people explored in this thesis have been to the world of markup languages, it is safe to say that the model is based on some well-grounded knowledge.


\paragraph{The intent of the thesis}
was to show that the idea of ``n-to-n'' document conversion is so utterly complicated that it is more pragmatic to consider alternative ways of approaching the problem from an ``1-to-n'' perspective.

This intent has been met through discussing the fallacies of approaches such as Pandoc, and allowing the ideal workflow suggested in this thesis to contrast. In retrospect one could argue this thesis shows tendencies towards that ``1-to-n'' document conversions actually may be a significantly more satisfying approach to the notion of ``write once, publish everywhere''.


\paragraph{The intent of the thesis}
was to show that it is significantly easier to convert a document format with utterly few constructs to any other markup language, rather than having to ``make sense'' out of complex constructs that exist in a source language but have no really sensible counterpart in a different destination language.

Also this intent has been satisfied through showing how the transformation process easily can be cut into highly composable pieces as long as the starting point is the most abstract possible representation of a document. Again, there are tendencies pointing towards that ``1-to-n'' might not just be a more realistic approach, but actually a significantly more powerful.

















% % % % % % % % % % % % % 
%
%
%
%   ANALYSIS
%
%
%
% % % % % % % % % % % % % 


\chapter{Analysis}
The most important realization in regards to this project, is that it seem almost trivial to divide the transformation process into composable parts when employing an ``M-to-N'' approach rather than the ``N-to-N''. That producing composable package channels never proved to be cumbersome, as long as the correct packages are used at the correct level in the abstraction chain. As long as each transformation moves the document in the ``right'' way through the abstraction chain. In other words, when each transformation moves the document from a higher level of abstraction to a lower, or of course stays at the same, then packaging and composability seem not to be a major issue. It is likely that many issues in sharing transformation packages may stem from transforming documents in the ``wrong'' way through the abstraction chain. In other words from less abstract to more abstract. Consider scaling a bitmap image. Scaling down works perfectly fine, but scaling up pose highly problematic.
 
\paragraph{Attempting ``M-to-N'' instead of ``N-to-N''} seem to be another way to look at this ``walking in the right direction'' of the abstraction chain. If one starts ($M$) at the intersection of all possible languages (all $N$) one may want to output to, then it is only reasonable to assume that the level of abstraction will be utterly high from the beginning. Subsequently then decrease as transformations move it closer and closer to concrete formats.




















% % % % % % % % % % % % % 
%
%
%
%   DISCUSSION
%
%
%
% % % % % % % % % % % % % 


\chapter{Discussion}
The greatest flaw of this paper is the fact that no complete production code was produced. Fragments have been produced to ensure that all snippets in this paper are correct and functional. For example, one discussion that has been avoided is that of the need for some kind of controlled intermediate format that would be used when sending the file from one subprogram to another.

The author is currently working on an open-sourced implementation of the ideal model of markup transformation. Contacts are welcome.







\section{Future research}
For future research the author of this paper would be very interesting to see any of the below come to life.

\begin{itemize}
\item An actual implementation of the programs and languages described in this paper.
\item Approaches to handle non-hierarchical markup.
\item A deeper analysis of the different kind of syntaxes that would be possible to express through a simple Formal Grammar DSL such as the one outlined in this thesis. Consider for example the many different ways of marking up content using SGML (through e.g. omitting tags).
\item Alternative models of ideal transformation workflows.
\item Approaches on how to write the configuration files depicted in the transformation models.
\item Attempts to harmonize the use of many different programming languages within this transformation model.
\item Attempts to carry out the modeled transformation processes without compiling in each step. It seems reasonable to assume that it could be done through constant remapping, and thus only compiled in the very end.
\end{itemize}















% % % % % % % % % % % % % 
%
%
%
%   BIBLIOGRAPHY
%
%
%
% % % % % % % % % % % % % 


\bibliography{bibliography}







\end{document}
